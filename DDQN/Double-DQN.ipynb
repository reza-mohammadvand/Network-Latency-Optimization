{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from scipy.special import erfcinv\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, optimizers\n",
        "from collections import deque\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Edge Computing Environment\n",
        "class EdgeComputingEnvironment:\n",
        "    def __init__(self, M=15, area_size=100, D_m=1354, eta_m_range=(100, 300), F_max_ue=1.5, P_max=23, B=5, T_max=10, F_max_es=30, S_max_es=60, epsilon=10**-7, E_max=3, theta=10**-26, L=8, phi=0.02, N0=-174, f_es_dev=0.02, f_ue_dev=0.02):\n",
        "        \"\"\"\n",
        "        Initialize the edge computing environment with given parameters.\n",
        "        \"\"\"\n",
        "        self.M = M  # Number of users\n",
        "        self.area_size = area_size  # Size of the area in which users are distributed\n",
        "        self.D_m = D_m  # Task data size\n",
        "        self.eta_m_range = eta_m_range  # Range of Task complexity\n",
        "        self.F_max_ue = F_max_ue * 10**9  # Maximum frequency of user equipment\n",
        "        self.P_max = 10 ** ((P_max - 30) / 10)  # Convert maximum transmission power from dBm to Watts\n",
        "        self.B = B * 10**6  # Bandwidth\n",
        "        self.T_max = T_max * 10**-3  # Maximum tolerable delay\n",
        "        self.F_max_es = F_max_es * 10**9  # Maximum frequency of edge server\n",
        "        self.S_max_es = S_max_es * 10**3  # Maximum cache size of edge server\n",
        "        self.epsilon = epsilon  # Error tolerance for rate calculation\n",
        "        self.E_max = E_max * 10**-3  # Maximum energy consumption\n",
        "        self.theta = theta  # Energy coefficient\n",
        "        self.L = L  # Number of antennas\n",
        "        self.phi = phi * 10**-3  # Transmission time interval\n",
        "        self.R_min = 10**6  # Minimum data rate\n",
        "        self.N0 = N0  # Noise power in dBm\n",
        "        self.N0 = 10 ** ((N0 - 30) / 10)  # Convert noise power from dBm/Hz to Watts/Hz\n",
        "        self.PL_d = lambda d: -35.3 - (37.6 * np.log10(d))  # Path loss model\n",
        "        self.f_es_dev = f_es_dev  #The deviation between the estimated value and the actual value of the processing rate of the ES\n",
        "        self.f_ue_dev = f_ue_dev  #The deviation between the estimated value and the actual value of the processing rate of the UE\n",
        "        self.is_training = True\n",
        "        self.tasks = []\n",
        "        self.current_task = {}\n",
        "        self.Task_processed = 0\n",
        "        self.penalty = 10\n",
        "        self.penalties = [0,0,0,0,0,0]\n",
        "\n",
        "        self.user_device_params = []  # List to store parameters for each user device\n",
        "        self.initialize_user_device_params()  # Initialize user device parameters\n",
        "\n",
        "        self.cache = []  # Cache to store tasks\n",
        "        self.current_cache_size = 0  # Current size of the cache\n",
        "        self.transmitting_tasks = []  # List to store transmitting tasks\n",
        "        self.processing_tasks = []  # List to store processing tasks\n",
        "        self.current_time = 0.0  # Current simulation time\n",
        "\n",
        "        # Initialize bandwidth and computation attributes\n",
        "        self.total_bandwidth = 0 # Initialize total bandwidth\n",
        "        self.total_computation = 0 # Initialize total computation\n",
        "\n",
        "    def initialize_user_device_params(self):\n",
        "        \"\"\"\n",
        "        Initialize parameters for each user device.\n",
        "        Randomly generates user-specific parameters such as path loss.\n",
        "        \"\"\"\n",
        "        for device_id in range(self.M):\n",
        "            d = np.random.uniform(1, self.area_size / 2)  # Distance to server\n",
        "            PL_dB = self.PL_d(d)\n",
        "            g_m = 10 ** (PL_dB / 10)  # Convert path loss from dB to linear scale\n",
        "            h_bar = np.random.randn(1, self.L) + 1j * np.random.randn(1, self.L)  # Channel gain\n",
        "\n",
        "            self.user_device_params.append({\n",
        "                'device_id': device_id,  # Assign a unique ID to each device\n",
        "                'd': d,\n",
        "                'g_m': g_m,\n",
        "                'h_bar': h_bar,\n",
        "            })\n",
        "\n",
        "    def create_task(self):    \n",
        "        task_distribution = np.random.choice(range(self.M), self.M, replace=True)\n",
        "        self.tasks = []\n",
        "        for user_id in task_distribution:\n",
        "            eta_m =  np.round(np.random.choice(np.linspace(self.eta_m_range[0], self.eta_m_range[1], 50)))\n",
        "            T_max_task = self.T_max  # Static according to article\n",
        "            order = np.round(random.uniform(1,5))\n",
        "\n",
        "            task_info = {\n",
        "            'eta_m': eta_m,\n",
        "            'T_max': T_max_task,\n",
        "            'D_m': 1354,  # Task data size\n",
        "            'user_id' : user_id,\n",
        "            'order' : order\n",
        "            }\n",
        "\n",
        "            self.tasks.append(task_info)\n",
        "\n",
        "    def get_state(self):\n",
        "\n",
        "        self.tasks = sorted(self.tasks, key=lambda x: x['order'], reverse=True)\n",
        "\n",
        "        task = self.tasks.pop()\n",
        "\n",
        "        self.current_task = task\n",
        "\n",
        "        cache_hit = 1 if any(task == task_info[0] for task_info in self.cache) else 0\n",
        "\n",
        "        state = [\n",
        "            task['eta_m'],\n",
        "            self.total_bandwidth,\n",
        "            self.total_computation,\n",
        "            cache_hit,\n",
        "            self.user_device_params[task['user_id']]['d']\n",
        "        ]\n",
        "\n",
        "        return state\n",
        "\n",
        "    def calculate_gamma_m(self, b_m, p_m, user_id):\n",
        "        \"\"\"\n",
        "        Calculate the signal-to-noise ratio (SNR) for a given user.\n",
        "\n",
        "        Parameters:\n",
        "        - b_m (float): Bandwidth allocation\n",
        "        - p_m (float): Transmission power\n",
        "        - user_id (int): ID of the user\n",
        "\n",
        "        Returns:\n",
        "        - gamma_m (array): SNR values for the user's communication channel\n",
        "        \"\"\"\n",
        "        h_m = np.sqrt(self.user_device_params[user_id]['g_m']) * self.user_device_params[user_id]['h_bar']  # Channel gain\n",
        "        gamma_m = (p_m * np.linalg.norm(h_m, axis=1) ** 2) / (b_m * self.B * self.N0)  # SNR\n",
        "        \n",
        "        return gamma_m\n",
        "\n",
        "    def calculate_uplink_rate(self, b_m, p_m, user_id):\n",
        "        \"\"\"\n",
        "        Calculate the uplink data rate for a given user.\n",
        "\n",
        "        Parameters:\n",
        "        - b_m (float): Bandwidth allocation\n",
        "        - p_m (float): Transmission power\n",
        "        - user_id (int): ID of the user\n",
        "\n",
        "        Returns:\n",
        "        - R_m (float): Uplink data rate in bits/second\n",
        "        \"\"\"\n",
        "        gamma_m = self.calculate_gamma_m(b_m, p_m, user_id)  # Calculate the SINR for the m-th user\n",
        "        V_m = 1 - (1 / (1 + gamma_m) ** 2)  # Intermediate variable for rate calculation\n",
        "        Q_inv = np.sqrt(2) * erfcinv(2 * self.epsilon)  # Calculate the inverse of the Q-function for the outage probability\n",
        "        R_m = (self.B / np.log(2)) * ((b_m * np.log(1 + gamma_m)) - ((np.sqrt((b_m * V_m) / (self.phi * self.B))) * Q_inv))  # Uplink data rate\n",
        "\n",
        "        return R_m\n",
        "\n",
        "    def calculate_delay(self, alpha_m, cache_hit, R_m, D_m, f_ue_m, f_es_m, f_ue_est, f_es_est, eta_m):\n",
        "        \"\"\"\n",
        "        Calculate the end-to-end delay for a given task.\n",
        "\n",
        "        Parameters:\n",
        "        - alpha_m (float): Offloading decision\n",
        "        - cache_hit (int): Split factor (0 or 1)\n",
        "        - R_m (float): Uplink data rate in bits/second\n",
        "        - D_m (int): Data size\n",
        "        - f_ue_m (float): Computation capability of the user device\n",
        "        - f_es_m (float): Computation capability of the edge server\n",
        "        - f_ue_est (float): Estimation error for the user device's computation capability\n",
        "        - f_es_est (float): Estimation error for the edge server's computation capability\n",
        "        - eta_m (float): Computational intensity\n",
        "\n",
        "        Returns:\n",
        "        - T_e2e (float): End-to-end delay in seconds\n",
        "        \"\"\"\n",
        "        actual_f_ue_m = f_ue_m - f_ue_est  # Actual processing rate of the user device\n",
        "\n",
        "        if cache_hit == 1:\n",
        "            T_es = self.calculate_server_processing_delay(alpha_m, cache_hit, D_m, f_es_m, f_es_est, eta_m)  # Only edge server processing delay\n",
        "            T_e2e = T_es\n",
        "\n",
        "        else:\n",
        "            T_ue = (alpha_m * eta_m * D_m) / actual_f_ue_m  # User device processing delay\n",
        "            T_tr = self.calculate_transmission_delay(alpha_m, R_m, D_m)  # Transmission delay\n",
        "            T_es = self.calculate_server_processing_delay(alpha_m, cache_hit, D_m, f_es_m, f_es_est, eta_m)  # Edge server processing delay\n",
        "            T_e2e = T_ue + T_tr + T_es  # Total end-to-end delay\n",
        "\n",
        "        return T_e2e\n",
        "\n",
        "    def calculate_transmission_delay(self, alpha_m, R_m, D_m):\n",
        "        \"\"\"\n",
        "        Calculate the transmission delay for a given task.\n",
        "\n",
        "        Parameters:\n",
        "        - alpha_m (float): Offloading decision\n",
        "        - R_m (float): Uplink data rate in bits/second\n",
        "        - D_m (int): Data size\n",
        "        - user_id (int): ID of the user\n",
        "\n",
        "        Returns:\n",
        "        - T_co (float): Transmission delay in seconds\n",
        "        \"\"\"\n",
        "        T_co =  ((1 - alpha_m) * (D_m * 8)) / R_m   # Transmission delay calculation based on task size and uplink rate\n",
        "\n",
        "        return T_co\n",
        "\n",
        "    def calculate_server_processing_delay(self, alpha_m, cache_hit, D_m, f_es_m, f_es_est, eta_m):\n",
        "        \"\"\"\n",
        "        Calculate the processing delay at the edge server for a given task.\n",
        "\n",
        "        Parameters:\n",
        "        - alpha_m (float): Offloading decision\n",
        "        - D_m (int): Data size\n",
        "        - cache_hit (0,1): 1 = Exist in cache and 0 not exist in cache\n",
        "        - f_es_m (float): Computation capability of the edge server\n",
        "        - f_es_est (float): Estimation error for the edge server's computation capability\n",
        "        - eta_m (float): Computational intensity\n",
        "\n",
        "        Returns:\n",
        "        - T_es (float): Processing delay at the edge server in seconds\n",
        "        \"\"\"\n",
        "\n",
        "        actual_f_es_m = f_es_m - f_es_est  # Actual processing rate of the Edge server\n",
        "\n",
        "        if cache_hit == 0:\n",
        "            T_es = ((1 - alpha_m) * eta_m * D_m) / actual_f_es_m  # Processing delay at the edge server\n",
        "\n",
        "        else:\n",
        "            T_es = (eta_m * D_m) / actual_f_es_m\n",
        "        return T_es\n",
        "\n",
        "    def calculate_energy_consumption(self, s_m, R_m, alpha_m, p_m, D_m, f_ue_m, f_ue_est, eta_m):\n",
        "        \"\"\"\n",
        "        Calculate the energy consumption for a given task.\n",
        "\n",
        "        Parameters:\n",
        "        - alpha_m (float): Offloading decision\n",
        "        - R_m (float): Uplink data rate in bits/second\n",
        "        - s_m (int): Split factor (0 or 1)\n",
        "        - f_ue_m (float): Computation capability of the user device\n",
        "        - p_m (float): Transmission power\n",
        "        - f_ue_est (float): Estimation error for the user device's computation capability\n",
        "        - eta_m (float): Computational intensity\n",
        "\n",
        "        Returns:\n",
        "        - E_total (float): Total energy consumption in Joules\n",
        "        \"\"\"\n",
        "\n",
        "        actual_f_ue_m = f_ue_m - f_ue_est  # Calculate the actual processing rate of the UE\n",
        "\n",
        "        E_ue = alpha_m * (self.theta / 2) * eta_m * D_m * (actual_f_ue_m ** 2)  # Energy consumption at the user device\n",
        "        E_tx = ((D_m * 8) * p_m) / R_m  # Transmission energy\n",
        "\n",
        "        if s_m == 1:  # Task is in cache\n",
        "            E_total = 0  # No energy consumed when task is in cache\n",
        "        else:\n",
        "            E_total = E_ue + E_tx  # Total energy consumption\n",
        "\n",
        "        return E_total\n",
        "\n",
        "    def manage_cache(self, task_info, task_delay, cache_hit_model):\n",
        "        \"\"\"\n",
        "        Manage the cache for storing and retrieving tasks.\n",
        "\n",
        "        Parameters:\n",
        "        - task_info (tuple): Task parameters to identify the task\n",
        "        - task_delay (float): Delay of the task\n",
        "\n",
        "        Returns:\n",
        "        - bool: True if the task is found in the cache, False otherwise\n",
        "        \"\"\"\n",
        "        # Check for cache hit first\n",
        "        cache_hit = any(task_info == task[0] for task in self.cache)\n",
        "        \n",
        "        if task_delay == 0:\n",
        "            return cache_hit  # Return True if found, False otherwise\n",
        "        \n",
        "        task_size = task_info['D_m'] * 8  # Task size\n",
        "        Server_Max_Capacity = self.S_max_es  # Server maximum capacity\n",
        "        if Server_Max_Capacity == 0 :\n",
        "            return\n",
        "        \n",
        "        if self.is_training :\n",
        "            # During training\n",
        "            if cache_hit:\n",
        "                self.current_cache_size -= task_info['D_m'] * 8\n",
        "                self.cache = [task for task in self.cache if task[0] != task_info]\n",
        "\n",
        "            # During training, always update cache based on task delay\n",
        "            self.cache.append((task_info, task_delay))  # Add task to cache\n",
        "            self.current_cache_size += task_size  # Update cache size\n",
        "        \n",
        "            if self.current_cache_size >= Server_Max_Capacity:\n",
        "                sorted_cache = sorted(self.cache, key=lambda x: x[1], reverse=True)  # Sort tasks by delay in descending order\n",
        "\n",
        "                while (self.current_cache_size) > Server_Max_Capacity:\n",
        "                    if not sorted_cache:\n",
        "                        break  # Exit loop if sorted_cache is empty\n",
        "                    last_task = sorted_cache.pop()  # Remove the last task from sorted_cache\n",
        "                    self.cache.remove(last_task)  # Remove the task from the cache\n",
        "                    self.current_cache_size -= last_task[0]['D_m'] * 8  # Update current cache size\n",
        "\n",
        "        if self.is_training is False:\n",
        "            # During testing, follow the model's prediction\n",
        "            if cache_hit_model == 0 :\n",
        "                if cache_hit:\n",
        "                    self.current_cache_size -= task_info['D_m'] * 8\n",
        "                    self.cache = [task for task in self.cache if task[0] != task_info]\n",
        "            else:\n",
        "                if cache_hit:\n",
        "                    self.current_cache_size -= task_info['D_m'] * 8\n",
        "                    self.cache = [task for task in self.cache if task[0] != task_info]\n",
        "\n",
        "                if (task_size + self.current_cache_size) <= Server_Max_Capacity:\n",
        "                    self.cache.append((task_info, task_delay))  # Add task to cache\n",
        "                    self.current_cache_size += task_size  # Update cache size\n",
        "\n",
        "                else:\n",
        "                    sorted_cache = sorted(self.cache, key=lambda x: x[1], reverse=True)  # Sort tasks by delay in descending order\n",
        "\n",
        "                    while (task_size + self.current_cache_size) > Server_Max_Capacity:\n",
        "                        if not sorted_cache:\n",
        "                            break  # Exit loop if sorted_cache is empty\n",
        "                        last_task = sorted_cache.pop()  # Remove the last task from sorted_cache\n",
        "                        self.cache.remove(last_task)  # Remove the task from the cache\n",
        "                        self.current_cache_size -= last_task[0]['D_m'] * 8  # Update current cache size\n",
        "\n",
        "                    self.cache.append((task_info, task_delay))  # Add task to cache\n",
        "                    self.current_cache_size += task_size  # Update cache size\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Perform a simulation step for the given action.\n",
        "\n",
        "        Parameters:\n",
        "        - action (array): Array of action for each user\n",
        "        - tasks (array): Array of task for each user\n",
        "\n",
        "        Returns:\n",
        "        - tuple: (task_rewards, next_state, done)\n",
        "        \"\"\"\n",
        "        done = False\n",
        "\n",
        "        alpha_m = action[0]\n",
        "        b_m = action[1]\n",
        "        p_m = action[2]\n",
        "        f_ue_m = action[3]\n",
        "        f_es_m = action[4]\n",
        "        cache_hit_model = action[5]\n",
        "\n",
        "        task = self.current_task\n",
        "\n",
        "        user_id = task.pop('user_id',0)\n",
        "        task.pop('order')\n",
        "        \n",
        "        # Determine if the task is a cache hit or miss\n",
        "        cache_hit = 1 if self.manage_cache(task, 0, cache_hit_model) else 0\n",
        "\n",
        "        f_ue_est = f_ue_m * self.f_ue_dev  \n",
        "        f_es_est = f_es_m * self.f_es_dev  \n",
        "\n",
        "        # Calculate the uplink data rate for the user\n",
        "        R_m = self.calculate_uplink_rate(b_m, p_m, user_id)\n",
        "\n",
        "        # Calculate the end-to-end delay for the task\n",
        "        delay = self.calculate_delay(\n",
        "            alpha_m, cache_hit, R_m,\n",
        "            task['D_m'], f_ue_m, f_es_m, f_ue_est,\n",
        "            f_es_est, task['eta_m']\n",
        "        )\n",
        "\n",
        "        delay = np.round(delay[0],6) if isinstance(delay, np.ndarray) else np.round(delay,6)\n",
        "\n",
        "        # Calculate the energy consumption for the task\n",
        "        energy = self.calculate_energy_consumption(\n",
        "            cache_hit, R_m, alpha_m, p_m, task['D_m'], f_ue_m,\n",
        "            f_es_est, task['eta_m']\n",
        "        )\n",
        "\n",
        "        energy = np.round(energy[0],6) if isinstance(energy, np.ndarray) else np.round(energy,6)\n",
        "\n",
        "        # Manage task transmission and processing times\n",
        "        if cache_hit == 0:\n",
        "            transmission_end_time = self.current_time + self.calculate_transmission_delay(alpha_m, R_m, task['D_m'])\n",
        "            processing_end_time = transmission_end_time + self.calculate_server_processing_delay(alpha_m, cache_hit, task['D_m'], f_es_m, f_es_est, task['eta_m'])\n",
        "\n",
        "            self.transmitting_tasks.append((self.current_time, transmission_end_time, b_m))\n",
        "            process = f_es_m * (1 - alpha_m)\n",
        "            self.processing_tasks.append((transmission_end_time, processing_end_time, process))\n",
        "\n",
        "        else:\n",
        "            # For cache hit, only processing delay is considered\n",
        "            processing_end_time = self.current_time + self.calculate_server_processing_delay(alpha_m, cache_hit, task['D_m'], f_es_m, f_es_est, task['eta_m'])\n",
        "            self.processing_tasks.append((self.current_time, processing_end_time, f_es_m))\n",
        "\n",
        "        # Update cache with the task if it becomes eligible\n",
        "        self.manage_cache(task, delay, cache_hit_model)\n",
        "            \n",
        "        # Calculate total bandwidth and computation resource usage at current time\n",
        "        self.total_bandwidth = sum(b for _, end_time, b in self.transmitting_tasks if end_time > self.current_time)\n",
        "        self.total_computation = sum(f for _, end_time, f in self.processing_tasks if end_time > self.current_time)\n",
        "\n",
        "        # Free resources for tasks that have completed transmission or processing\n",
        "        self.transmitting_tasks = [(start_time, end_time, b) for start_time, end_time, b in self.transmitting_tasks if end_time > self.current_time]\n",
        "        self.processing_tasks = [(start_time, end_time, f) for start_time, end_time, f in self.processing_tasks if end_time > self.current_time]\n",
        "\n",
        "        # Check The Cache hit of model is right or not \n",
        "        cache_hit = 1 if self.manage_cache(task, 0, cache_hit_model) else 0\n",
        "        cache_hit_right = 1 if cache_hit == cache_hit_model else 0\n",
        "\n",
        "        x = (((1/self.M) - b_m) ** 2) * 10000\n",
        "\n",
        "        # Calculate reward\n",
        "        reward  = (-energy - delay)*1e4 \n",
        "        # print(reward)\n",
        "        reward -= x\n",
        "        # print(action)\n",
        "        # print(delay)\n",
        "        # print(energy)\n",
        "        \n",
        "        # Apply penalties for exceeding resource limits\n",
        "        if delay > task['T_max']:\n",
        "            reward -= self.penalty\n",
        "            done = True\n",
        "            self.penalties[0] += 1\n",
        "        if cache_hit_right == 0:\n",
        "            reward -= self.penalty\n",
        "            self.penalties[1] += 1\n",
        "        if R_m < self.R_min:\n",
        "            reward -= self.penalty\n",
        "            done = True\n",
        "            self.penalties[2] += 1\n",
        "        if energy > self.E_max:\n",
        "            reward -= self.penalty\n",
        "            done = True\n",
        "            self.penalties[3] += 1\n",
        "        if self.total_bandwidth > 1:\n",
        "            reward -= self.penalty\n",
        "            done = True\n",
        "            self.penalties[4] += 1\n",
        "        if self.total_computation > self.F_max_es:\n",
        "            reward -= self.penalty\n",
        "            done = True\n",
        "            self.penalties[5] += 1\n",
        "\n",
        "        # if b_m <= 1/self.M and R_m >= self.R_min*3:\n",
        "        #     reward += self.penalty*3\n",
        "            \n",
        "            \n",
        "        # if b_m >= 1/self.M:\n",
        "        #     reward -= self.penalty*3\n",
        "\n",
        "        # print(reward)\n",
        "        state_info = [\n",
        "            delay,\n",
        "            energy,\n",
        "            task['eta_m'],   # task complexity\n",
        "            self.total_bandwidth,\n",
        "            self.total_computation,\n",
        "            cache_hit_right,\n",
        "            self.user_device_params[user_id]['d']\n",
        "        ]\n",
        "\n",
        "        self.Task_processed += 1\n",
        "\n",
        "        return reward, state_info, done\n",
        "\n",
        "    # Increment current simulation time\n",
        "    def increase_time(self):\n",
        "        self.current_time += self.T_max\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the environment to its initial state.\n",
        "        \"\"\"\n",
        "        self.cache = [] \n",
        "        self.current_cache_size = 0\n",
        "        self.Task_processed = 0  \n",
        "        self.transmitting_tasks = [] \n",
        "        self.processing_tasks = [] \n",
        "        self.current_time = 0.0  \n",
        "        self.user_device_params = []\n",
        "        self.initialize_user_device_params()\n",
        "        self.total_bandwidth = 0  \n",
        "        self.total_computation = 0  \n",
        "\n",
        "    def render(self):\n",
        "        print(self.penalties)\n",
        "        # print(f\"Number of Users: {self.M}\")\n",
        "        # print(f\"Number of Task Processed: {self.Task_processed}\")\n",
        "        # print(f\"Total Bandwidth Used: {self.total_bandwidth}\")\n",
        "        # print(f\"Total Computation Used: {self.total_computation}\")\n",
        "        # print(f\"Current Cache Size: {np.round(self.current_cache_size/1000,2)} Kb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Double Deep Q-Network (DDQN) Agent\n",
        "class DDQNAgent:\n",
        "    def __init__(self, env, alpha=0.001, gamma=0.95, epsilon=1.0, batch_size=64, max_steps_per_episode=1, update_target_freq=25):\n",
        "        self.env = env  # Environment for the agent\n",
        "        self.num_users = env.M  # Number of users/devices in the environment\n",
        "        self.num_tasks = env.M # Number of task\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor for future rewards\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.batch_size = batch_size\n",
        "        self.max_steps_per_episode = max_steps_per_episode  # Maximum steps per episode\n",
        "        self.save_interval = 1000  # save model \n",
        "        self.num_candidates = 1000  # Define the number of candidate actions to sample\n",
        "        self.state_dim = 5  # state dimensions\n",
        "        self.action_dim = 6  # action dimensions\n",
        "        self.all_action = self.sample_all_action()  # Actions Space = 200000\n",
        "        self.is_training = True\n",
        "        self.all = 0  \n",
        "        self.update_target_freq = update_target_freq  # Frequency to update the target network\n",
        "\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.model = self.build_model()\n",
        "        self.target_model = self.build_model()\n",
        "        self.update_target_network()  # Initialize the target network\n",
        "\n",
        "    def build_model(self):\n",
        "        # Define inputs for state and action\n",
        "        input_state = layers.Input(shape=(self.state_dim,))\n",
        "        input_action = layers.Input(shape=(self.action_dim,))\n",
        "        \n",
        "        # Concatenate state and action inputs\n",
        "        concat = layers.Concatenate()([input_state, input_action])\n",
        "        \n",
        "        # Pass through dense layers\n",
        "        dense1 = layers.Dense(64, activation='relu')(concat)\n",
        "        dense2 = layers.Dense(64, activation='relu')(dense1)\n",
        "        dense3 = layers.Dense(64, activation='relu')(dense2)\n",
        "        output = layers.Dense(1, activation='linear')(dense3)  # Output the predicted reward\n",
        "        \n",
        "        # Create and compile the model\n",
        "        model = models.Model(inputs=[input_state, input_action], outputs=output)\n",
        "        model.compile(loss='mse', optimizer=optimizers.Adam(learning_rate=self.alpha, clipvalue=1.0))\n",
        "        return model\n",
        "\n",
        "    def update_target_network(self):\n",
        "        tau = 0.125\n",
        "        weights = self.model.get_weights() # give weights\n",
        "        target_weights = self.target_model.get_weights()\n",
        "        for i in range(len(target_weights)):\n",
        "            target_weights[i] = weights[i] * tau + target_weights[i] * (1 - tau) # update weights\n",
        "        self.target_model.set_weights(target_weights) # add weights that updated\n",
        "        #self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        \n",
        "    def update_epsilon(self, episode, total_episodes):\n",
        "        \"\"\"\n",
        "        Adjust the epsilon value based on the episode index.\n",
        "        The decay rate changes based on different stages of the training process.\n",
        "        \"\"\"\n",
        "        # Define three different decay rates for three stages of training\n",
        "        if episode < 0.25 * total_episodes:  # Stage 1 (0–25% of episodes)\n",
        "            decay_rate = 0.9992  # Slow decay for broad exploration\n",
        "        elif 0.25 * total_episodes <= episode < 0.75 * total_episodes:  # Stage 2 (25–75%)\n",
        "            decay_rate = 0.9980  # Moderate decay\n",
        "        else:  # Stage 3 (75–100%)\n",
        "            decay_rate = 0.9930  # Faster decay to focus on exploitation\n",
        "\n",
        "        # Update epsilon value\n",
        "        min_epsilon = 0.01  # Set a minimum epsilon value\n",
        "        self.epsilon = max(min_epsilon, self.epsilon * decay_rate)\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon and self.is_training:\n",
        "            # Choose a random action from the predefined set (exploration)\n",
        "            action = self.sample_random_action()\n",
        "\n",
        "        else:\n",
        "            if self.is_training:\n",
        "                # Sample a subset of actions from all actions for training\n",
        "                candidate_actions = np.array([self.sample_random_action() for _ in range(self.num_candidates)])\n",
        "\n",
        "            else:\n",
        "                # Use all actions during testing\n",
        "                candidate_actions = self.all_action #np.array([self.sample_random_action() for _ in range(1000)]) \n",
        "\n",
        "            # Prepare the state array for batch prediction\n",
        "            state_batch = np.tile(state, (candidate_actions.shape[0], 1))\n",
        "\n",
        "            # Normalize actions\n",
        "            normalized_action = np.array([self.normalize_action(action) for action in candidate_actions])\n",
        "\n",
        "            # Predict the reward for each candidate action\n",
        "            predicted_rewards = self.model.predict([state_batch, normalized_action])\n",
        "\n",
        "            # Select the action with the highest predicted reward\n",
        "            best_action_index = np.argmax(predicted_rewards)\n",
        "            action = candidate_actions[best_action_index]\n",
        "\n",
        "            #if self.is_training==False:                                                                                                                 #\n",
        "            print(action)\n",
        "        return action\n",
        "\n",
        "    def sample_random_action(self):\n",
        "        # Generate random values for the action parameters\n",
        "        # Generate a random value from a discrete set of 10 values ​​(for discretization). Discretization reduces the operation space, improving performance with minimal impact on results.\n",
        "        alpha = np.round(np.random.choice(np.linspace(0, 1, 10)),3)\n",
        "        b = np.round(np.random.choice(np.linspace(0.005, (2 / self.num_users), 10)),3)  # The minimum is set to 0.01 to avoid zero values, and the upper limit ensures fair distribution across users.\n",
        "        p = np.round(np.random.choice(np.linspace(0.005, self.env.P_max, 10)),3)  # The lower limit is 0.01 to avoid zero values, which are impractical and can cause calculation errors.\n",
        "        f_ue = np.round(np.random.choice(np.linspace(1e6, self.env.F_max_ue, 10))) # The minimum is set to 1 MHz to avoid unrealistic values, ensuring reasonable usage.\n",
        "        f_es = np.round(np.random.choice(np.linspace(1e6, ((2 * self.env.F_max_es) / self.num_users), 10))) # The lower limit prevents unrealistic values, while the upper limit ensures fair resource distribution.\n",
        "        cache_hit = np.random.choice([0, 1])  # Generate a random cache hit value, either 0 (no cache hit) or 1 (cache hit).\n",
        "        \n",
        "        return np.array([alpha, b, p, f_ue, f_es, cache_hit])\n",
        "    \n",
        "    def sample_all_action(self):\n",
        "        # Generate all action\n",
        "        alpha = np.round(np.linspace(0, 1, 10),3) \n",
        "        b = np.round(np.linspace(0.005, (2 / self.num_users), 10),3) \n",
        "        p = np.round(np.linspace(0.005, self.env.P_max, 10),3) \n",
        "        f_ue = np.round(np.linspace(1e6, self.env.F_max_ue, 10))\n",
        "        f_es = np.round(np.linspace(1e6, ((2 * self.env.F_max_es) / self.num_users), 10))\n",
        "        cache_hit = [0, 1]\n",
        "\n",
        "        samples = []\n",
        "\n",
        "        for i in alpha:\n",
        "            for j in b:\n",
        "                for k in p:\n",
        "                    for l in f_ue:\n",
        "                        for m in f_es:\n",
        "                            for n in cache_hit:\n",
        "                                samples.append(np.array([i, j, k, l, m, n]))\n",
        "\n",
        "        samples = np.array(samples)\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # Prioritized sampling based on rewards\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "\n",
        "        # Vectorized state, action, next_state extraction and reshaping\n",
        "        states = np.array([sample[0] for sample in minibatch])\n",
        "        actions = np.array([sample[1] for sample in minibatch])\n",
        "        rewards = np.array([sample[2] for sample in minibatch])\n",
        "        next_states = np.array([sample[3] for sample in minibatch])\n",
        "        dones = np.array([sample[4] for sample in minibatch])\n",
        "\n",
        "        states = states.reshape(self.batch_size, -1)\n",
        "        actions = actions.reshape(self.batch_size, -1)\n",
        "        next_states = next_states.reshape(self.batch_size, -1)\n",
        "\n",
        "        # Sample a subset of actions from all actions for training\n",
        "        candidate_actions = np.array([self.sample_random_action() for _ in range(self.num_candidates)])\n",
        "        normalized_candidate_actions = np.array([self.normalize_action(action) for action in candidate_actions])\n",
        "\n",
        "        # Expand dimensions to match state_batch and normalized_candidate_actions\n",
        "        state_batch = np.repeat(next_states, self.num_candidates, axis=0)\n",
        "        action_batch = np.tile(normalized_candidate_actions, (self.batch_size, 1))\n",
        "\n",
        "        # Predict Q-values ​​for each next_state-action pair\n",
        "        predicted_q_values = self.model.predict([state_batch, action_batch])\n",
        "        max_q_values = np.max(predicted_q_values.reshape(self.batch_size, self.num_candidates), axis=1)\n",
        "        print(\"predicted_q_values shape:\", predicted_q_values.shape)\n",
        "\n",
        "        # Compute target for the Q-learning update\n",
        "        targets = rewards + self.gamma * max_q_values * (1 - dones)\n",
        "\n",
        "        # Predict current Q-values ​​and update them\n",
        "        target_f = self.model.predict([states, actions])\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            target_f[i, 0] = targets[i]\n",
        "        \n",
        "        # Train the model on the updated Q-values\n",
        "        self.model.fit([states, actions], target_f, epochs=1, verbose=0)\n",
        "\n",
        "    def normalize_state(self, state):\n",
        "        # Normalize states between values ​​0 and 1\n",
        "        normalized_state = np.array([\n",
        "                state[0] / 300,  # Normalizing task complexity (eta_m)\n",
        "                state[1],  # Bandwidth (assuming it's already normalized to [0, 1])\n",
        "                state[2] / self.env.F_max_es,  # Normalizing computation\n",
        "                state[3], # cache hit\n",
        "                state[4] / (self.env.area_size/2) # distance\n",
        "        ])\n",
        "        return normalized_state\n",
        "    \n",
        "    def normalize_action(self, action):\n",
        "        # Normalize actions between values ​​0 and 1\n",
        "        normalized_action = np.array([\n",
        "                action[0] ,  # alpha\n",
        "                action[1] / (2 / self.num_users),  # bandwidth\n",
        "                action[2] / self.env.P_max,  # power transmission\n",
        "                action[3] / self.env.F_max_ue,  # Normalizing computation of user\n",
        "                action[4] / ((2 * self.env.F_max_es) / self.num_users),  # Normalizing computation of server\n",
        "                action[5] # cache hit\n",
        "        ])\n",
        "        return normalized_action\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(f'{name}model.weights.h5')\n",
        "        self.target_model.load_weights(f'{name}target_model.weights.h5')\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(f'{name}model.weights.h5')\n",
        "        self.target_model.save_weights(f'{name}target_model.weights.h5')\n",
        "\n",
        "    def train(self, num_episodes):\n",
        "        # Lists to store average delay and energy values for each episode\n",
        "        avg_delays = []\n",
        "        avg_energies = []\n",
        "        avg_rewards = []\n",
        "\n",
        "        self.env.is_training = True\n",
        "        self.is_training = True\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "\n",
        "            self.env.reset()\n",
        "\n",
        "            # Initialize total delay and energy for this episode\n",
        "            total_delay = 0\n",
        "            total_energy = 0\n",
        "            total_reward = 0\n",
        "\n",
        "            done_step = False\n",
        "\n",
        "            # Initialize the number of tasks in this episode\n",
        "            num_all_tasks = 0\n",
        "            actual_steps = 0\n",
        "\n",
        "            for step in range(self.max_steps_per_episode):\n",
        "\n",
        "                if done_step:\n",
        "                    break\n",
        "\n",
        "                self.env.create_task()\n",
        "\n",
        "                for item in range(self.num_users):\n",
        "\n",
        "                    state = np.array(self.env.get_state())\n",
        "\n",
        "                    normalized_state = np.array(self.normalize_state(state))\n",
        "\n",
        "                    action = self.act(normalized_state)\n",
        "                    \n",
        "                    # Execute the actions in the environment\n",
        "                    reward, next_state_info, done = self.env.step(action)\n",
        "\n",
        "                    # Extract delay and energy values from the next device information\n",
        "                    delay = next_state_info.pop(0)\n",
        "                    energy = next_state_info.pop(0)\n",
        "\n",
        "                    next_state = np.array(next_state_info)\n",
        "\n",
        "                    normalized_action = self.normalize_action(action)\n",
        "                    normal_nextstate = self.normalize_state(next_state)\n",
        "\n",
        "                    self.remember(normalized_state, normalized_action, reward, normal_nextstate, done)\n",
        "\n",
        "                    # Accumulate the total delay and energy for the episode\n",
        "                    total_delay += delay\n",
        "                    total_energy += energy\n",
        "                    total_reward += reward\n",
        "\n",
        "                    num_all_tasks += 1\n",
        "\n",
        "                    if done:\n",
        "                        done_step = True\n",
        "                        # Exit the loop if the episode is done\n",
        "                        break\n",
        "\n",
        "                self.env.increase_time()\n",
        "\n",
        "                actual_steps += 1\n",
        "                \n",
        "            self.env.render()\n",
        "            self.all += num_all_tasks\n",
        "\n",
        "            # Calculate and store average delay and energy for the episode\n",
        "            avg_delay = (total_delay / num_all_tasks) * 1000  # Convert to milliseconds\n",
        "            avg_energy = total_energy / num_all_tasks\n",
        "            avg_reward = total_reward / num_all_tasks\n",
        "            avg_delays.append(avg_delay)\n",
        "            avg_energies.append(avg_energy)\n",
        "            avg_rewards.append(avg_reward)\n",
        "\n",
        "            # Update epsilon for the epsilon-greedy strategy\n",
        "            self.update_epsilon(episode, num_episodes)\n",
        "\n",
        "            self.replay()\n",
        "\n",
        "            # save model\n",
        "            if (episode + 1) % self.save_interval == 0:\n",
        "                self.save(f'Model{episode+1}')\n",
        "\n",
        "            if (episode + 1) % self.update_target_freq == 0:\n",
        "                self.update_target_network()\n",
        "            \n",
        "            # Print the episode's results\n",
        "            print(f\"Train : Episode {episode + 1}/{num_episodes} - Steps Count {actual_steps} - Tasks Count {num_all_tasks} - Avg Delay: {avg_delay}, Avg Energy: {avg_energy}, Avg Reward: {avg_reward}\")\n",
        "            print(\"-\" * 100)\n",
        "\n",
        "        # Optionally plot the results\n",
        "        self.plot_results(avg_delays, avg_energies, avg_rewards)\n",
        "\n",
        "    def test(self, num_test_steps=1):\n",
        "        # Initialize total delay, alpha values, and rewards for the test\n",
        "        total_delay = 0\n",
        "        total_alpha = 0\n",
        "        total_energy = 0\n",
        "        total_reward = 0\n",
        "        \n",
        "        done_step = False\n",
        "\n",
        "        # Counter for actual steps\n",
        "        num_all_tasks = 0\n",
        "        actual_steps = 0\n",
        "        \n",
        "        # Set epsilon to 0 for testing (no exploration)\n",
        "        self.epsilon = 0\n",
        "\n",
        "        self.env.is_training = False\n",
        "        self.is_training = False\n",
        "        \n",
        "        # Reset the environment \n",
        "        self.env.reset()\n",
        "\n",
        "        for step in range(num_test_steps):\n",
        "\n",
        "            if done_step:\n",
        "                    break\n",
        "\n",
        "            self.env.create_task()\n",
        "\n",
        "            for item in range(self.num_users):\n",
        "\n",
        "                state = np.array(self.env.get_state())\n",
        "\n",
        "                normalized_state = self.normalize_state(state)\n",
        "\n",
        "                action = self.act(normalized_state)\n",
        "                \n",
        "                # Execute the actions in the environment\n",
        "                reward, next_state_info, done = self.env.step(action)\n",
        "\n",
        "                # Extract delay and energy values from the next device information\n",
        "                delay = next_state_info.pop(0)\n",
        "                energy = next_state_info.pop(0)\n",
        "\n",
        "                # Accumulate delay and alpha values\n",
        "                total_delay += delay\n",
        "                total_energy += energy\n",
        "                total_alpha += 1 - action[0]\n",
        "\n",
        "                num_all_tasks += 1\n",
        "\n",
        "                if done:\n",
        "                    done_step = True\n",
        "                    # Exit the loop if the episode is done\n",
        "                    break\n",
        "\n",
        "            self.env.increase_time()\n",
        "\n",
        "            # Increment the actual steps counter\n",
        "            actual_steps += 1\n",
        "\n",
        "        self.env.render()\n",
        "\n",
        "        # Calculate and return the average delay and alpha for the test\n",
        "        total_delay = total_delay * 1000  # Convert to milliseconds\n",
        "        avg_energy = (total_energy / num_all_tasks) * 1000  # Convert to milliJoule \n",
        "        avg_alpha = total_alpha / num_all_tasks\n",
        "        avg_reward = total_reward / num_all_tasks\n",
        "\n",
        "\n",
        "        # Print the episode's results\n",
        "        print(f\"Test : Steps Count {actual_steps} - Tasks Count {num_all_tasks} - Delay: {total_delay}, Avg Energy: {avg_energy}, Avg Reward: {avg_reward}, Avg Alpha: {avg_alpha}\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        # if num_all_tasks == 15:\n",
        "        #     return avg_delay, avg_alpha\n",
        "\n",
        "        return total_delay, avg_alpha\n",
        "\n",
        "\n",
        "    def plot_results(self, avg_delays, avg_energies, avg_rewards):\n",
        "        episodes = np.arange(1, len(avg_delays) + 1)\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(episodes, avg_delays, label='Avg Delay')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Average Delay')\n",
        "        plt.title('Average Delay per Episode')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.plot(episodes, avg_energies, label='Avg Energy')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Average Energy')\n",
        "        plt.title('Average Energy per Episode')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.plot(episodes, avg_rewards, label='Avg Reward')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Average Reward')\n",
        "        plt.title('Average Reward per Episode')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 1, 0, 0, 0]\n",
            "Train : Episode 1/2500 - Steps Count 1 - Tasks Count 3 - Avg Delay: 8.840333333333334, Avg Energy: 0.0009283333333333334, Avg Reward: -132.89777777777778\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[2, 4, 1, 0, 0, 0]\n",
            "Train : Episode 2/2500 - Steps Count 1 - Tasks Count 2 - Avg Delay: 165.96800000000002, Avg Energy: 0.001448, Avg Reward: -1696.6161111111112\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[2, 5, 2, 0, 0, 0]\n",
            "Train : Episode 3/2500 - Steps Count 1 - Tasks Count 4 - Avg Delay: 0.48149999999999993, Avg Energy: 0.0009385, Avg Reward: -32.46361111111111\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[3, 10, 2, 0, 0, 0]\n",
            "Train : Episode 4/2500 - Steps Count 1 - Tasks Count 8 - Avg Delay: 20.698, Avg Energy: 0.00041874999999999996, Avg Reward: -226.8806944444445\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[4, 10, 3, 1, 0, 0]\n",
            "Train : Episode 5/2500 - Steps Count 1 - Tasks Count 1 - Avg Delay: 16.704, Avg Energy: 0.003091, Avg Reward: -265.97777777777776\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[5, 11, 3, 1, 0, 0]\n",
            "Train : Episode 6/2500 - Steps Count 1 - Tasks Count 3 - Avg Delay: 103.01066666666665, Avg Energy: 0.0004579999999999999, Avg Reward: -1057.3544444444442\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[5, 11, 4, 1, 0, 0]\n",
            "Train : Episode 7/2500 - Steps Count 1 - Tasks Count 1 - Avg Delay: 3.82, Avg Energy: 0.000827, Avg Reward: -94.49777777777777\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[6, 13, 5, 2, 0, 0]\n",
            "Train : Episode 8/2500 - Steps Count 1 - Tasks Count 3 - Avg Delay: 8.43, Avg Energy: 0.0013930000000000001, Avg Reward: -149.81333333333333\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[7, 15, 5, 2, 0, 0]\n",
            "Train : Episode 9/2500 - Steps Count 1 - Tasks Count 2 - Avg Delay: 32.934, Avg Energy: 1.9e-05, Avg Reward: -350.3011111111111\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[8, 17, 5, 2, 0, 0]\n",
            "Train : Episode 10/2500 - Steps Count 1 - Tasks Count 5 - Avg Delay: 73.5214, Avg Energy: 0.0008324, Avg Reward: -762.6937777777778\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[9, 18, 5, 2, 0, 0]\n",
            "Train : Episode 11/2500 - Steps Count 1 - Tasks Count 2 - Avg Delay: 67.1905, Avg Energy: 0.0012079999999999999, Avg Reward: -719.0394444444446\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[10, 19, 6, 2, 0, 0]\n",
            "Train : Episode 12/2500 - Steps Count 1 - Tasks Count 2 - Avg Delay: 7.5360000000000005, Avg Energy: 0.0014925, Avg Reward: -135.65944444444443\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[11, 19, 6, 2, 0, 0]\n",
            "Train : Episode 13/2500 - Steps Count 1 - Tasks Count 2 - Avg Delay: 21.5055, Avg Energy: 0.000577, Avg Reward: -242.52111111111114\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[12, 21, 7, 2, 0, 0]\n",
            "Train : Episode 14/2500 - Steps Count 1 - Tasks Count 2 - Avg Delay: 146.43, Avg Energy: 0.000133, Avg Reward: -1497.426111111111\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[13, 22, 7, 2, 0, 0]\n",
            "Train : Episode 15/2500 - Steps Count 1 - Tasks Count 2 - Avg Delay: 55.0835, Avg Energy: 0.000599, Avg Reward: -585.5327777777777\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[14, 23, 7, 2, 0, 0]\n",
            "Train : Episode 16/2500 - Steps Count 1 - Tasks Count 3 - Avg Delay: 65.88666666666666, Avg Energy: 0.0004886666666666667, Avg Reward: -686.448888888889\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[15, 26, 8, 3, 0, 0]\n",
            "Train : Episode 17/2500 - Steps Count 1 - Tasks Count 6 - Avg Delay: 4.954166666666667, Avg Energy: 0.0016471666666666666, Avg Reward: -87.77666666666664\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[15, 26, 9, 3, 0, 0]\n",
            "Train : Episode 18/2500 - Steps Count 1 - Tasks Count 1 - Avg Delay: 9.05, Avg Energy: 0.000713, Avg Reward: -145.6577777777778\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[16, 28, 9, 3, 0, 0]\n",
            "Train : Episode 19/2500 - Steps Count 1 - Tasks Count 2 - Avg Delay: 46.14600000000001, Avg Energy: 0.0010429999999999999, Avg Reward: -501.01944444444445\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[16, 29, 10, 3, 0, 0]\n",
            "Train : Episode 20/2500 - Steps Count 1 - Tasks Count 1 - Avg Delay: 4.623, Avg Energy: 0.001672, Avg Reward: -120.97777777777779\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[17, 29, 10, 3, 0, 0]\n",
            "Train : Episode 21/2500 - Steps Count 1 - Tasks Count 1 - Avg Delay: 262.715, Avg Energy: 0.001143, Avg Reward: -2654.501111111111\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[17, 34, 11, 3, 0, 0]\n",
            "2000/2000 [==============================] - 14s 6ms/step\n",
            "predicted_q_values shape: (64000, 1)\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Train : Episode 22/2500 - Steps Count 1 - Tasks Count 8 - Avg Delay: 1.8758750000000002, Avg Energy: 0.00037799999999999997, Avg Reward: -47.43611111111112\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[17, 41, 11, 3, 0, 0]\n",
            "  87/2000 [>.............................] - ETA: 9s"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10352\\389078486.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;31m# Assuming you have your EdgeComputingEnvironment defined as per your code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEdgeComputingEnvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Initialize the DQN agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10352\\4152310575.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, num_episodes)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;31m# Update epsilon for the epsilon-greedy strategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_epsilon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m             \u001b[1;31m# save model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepisode\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_interval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10352\\4152310575.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[0mstate_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_candidates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0maction_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalized_candidate_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;31m# Predict Q-values ​​for each next_state-action pair\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mpredicted_q_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m         \u001b[0mmax_q_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_q_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"predicted_q_values shape:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_q_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\REZA\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\REZA\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2282\u001b[0m                     \u001b[1;34m\"`tf.config.run_functions_eagerly(True)` for more \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2283\u001b[0m                     \u001b[1;34m\"information of where went wrong, or file a \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2284\u001b[0m                     \u001b[1;34m\"issue/bug to `tf.keras`.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2285\u001b[0m                 )\n\u001b[1;32m-> 2286\u001b[1;33m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2287\u001b[0m         all_outputs = tf.__internal__.nest.map_structure_up_to(\n\u001b[0;32m   2288\u001b[0m             \u001b[0mbatch_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpotentially_ragged_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2289\u001b[0m         )\n",
            "\u001b[1;32mc:\\Users\\REZA\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1370\u001b[0m             \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_step\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferred_steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m         ):\n\u001b[0;32m   1372\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m             \u001b[0moriginal_spe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m             can_run_full_execution = (\n\u001b[0;32m   1376\u001b[0m                 \u001b[0moriginal_spe\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferred_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\REZA\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    635\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    638\u001b[0m     raise NotImplementedError(\n\u001b[0;32m    639\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
            "\u001b[1;32mc:\\Users\\REZA\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    724\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Read\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m       \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m     \u001b[1;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m     \u001b[1;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\REZA\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\REZA\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1180\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mOpDispatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNOT_SUPPORTED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\REZA\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    290\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"graph\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;31m# Make sure we get an input with handle data attached from resource\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m     \u001b[1;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m   \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m   \u001b[1;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_handle_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_data\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\REZA\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m   4070\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4071\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4072\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4073\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4074\u001b[1;33m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4075\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4076\u001b[0m       return identity_eager_fallback(\n\u001b[0;32m   4077\u001b[0m           input, name=name, ctx=_ctx)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Assuming you have your EdgeComputingEnvironment defined as per your code\n",
        "env = EdgeComputingEnvironment()\n",
        "\n",
        "# Initialize the DQN agent\n",
        "agent = DDQNAgent(env)\n",
        "\n",
        "\n",
        "# Load the model if you want to continue training\n",
        "#agent.load(\"Model2000\")\n",
        "\n",
        "# Train the agent\n",
        "num_episodes = 2500  # Adjust the number of episodes as needed\n",
        "agent.train(num_episodes)\n",
        "\n",
        "# Save the final model\n",
        "# agent.save(\"dqn_model.h5\")\n",
        "\n",
        "# for batch in range(3):\n",
        "\n",
        "#     agent.train(num_episodes)  # Train in batches of episodes\n",
        "#     agent.save(f'Model-{batch}.h5')\n",
        "    \n",
        "#     # Clear the session to free memory\n",
        "#     K.clear_session()\n",
        "\n",
        "#     # Reload the model for the next batch\n",
        "#     agent.load(f'Model-{batch}.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6250/6250 [==============================] - 6s 1ms/step\n",
            "[1.00000000e+00 5.00000000e-03 5.00000000e-03 5.00666667e+08\n",
            " 4.00000000e+09 1.00000000e+00]\n",
            "[822, 5170, 2613, 2063, 22, 0]\n",
            "Test : Steps Count 1 - Tasks Count 1 - Delay: 0.367, Avg Energy: 0.159, Avg Reward: 0.0, Avg Alpha: 0.0\n",
            "----------------------------------------------------------------------------------------------------\n",
            "6250/6250 [==============================] - 6s 996us/step\n",
            "[1.00000000e+00 5.00000000e-03 4.80000000e-02 5.00666667e+08\n",
            " 4.00000000e+09 1.00000000e+00]\n",
            "[822, 5170, 2614, 2063, 22, 0]\n",
            "Test : Steps Count 1 - Tasks Count 1 - Delay: 0.58, Avg Energy: 0.252, Avg Reward: 0.0, Avg Alpha: 0.0\n",
            "----------------------------------------------------------------------------------------------------\n",
            "6250/6250 [==============================] - 6s 990us/step\n",
            "[1.000e+00 4.800e-02 5.000e-03 1.500e+09 1.334e+09 1.000e+00]\n",
            "[822, 5170, 2614, 2064, 22, 0]\n",
            "Test : Steps Count 1 - Tasks Count 1 - Delay: 0.273, Avg Energy: 4.35, Avg Reward: 0.0, Avg Alpha: 0.0\n",
            "----------------------------------------------------------------------------------------------------\n",
            "6250/6250 [==============================] - 6s 997us/step\n",
            "[1.00000000e+00 5.00000000e-03 1.35000000e-01 5.00666667e+08\n",
            " 3.55566667e+09 1.00000000e+00]\n",
            "[822, 5170, 2615, 2064, 22, 0]\n",
            "Test : Steps Count 1 - Tasks Count 1 - Delay: 0.477, Avg Energy: 0.216, Avg Reward: 0.0, Avg Alpha: 0.0\n",
            "----------------------------------------------------------------------------------------------------\n",
            "6250/6250 [==============================] - 6s 973us/step\n",
            "[1.000e+00 4.800e-02 5.000e-03 1.500e+09 1.334e+09 1.000e+00]\n",
            "[822, 5170, 2615, 2065, 22, 0]\n",
            "Test : Steps Count 1 - Tasks Count 1 - Delay: 0.27599999999999997, Avg Energy: 4.409, Avg Reward: 0.0, Avg Alpha: 0.0\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "    agent.test(num_test_steps=1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
