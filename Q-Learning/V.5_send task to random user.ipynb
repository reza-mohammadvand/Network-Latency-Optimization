{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.special import erfcinv\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Edge Computing Environment\n",
        "\n",
        "class EdgeComputingEnvironment:\n",
        "    def __init__(self, M=15, area_size=100, D_m=1354, eta_m_range=(100, 300), F_max_ue=1.5, P_max=23, B=5e6, T_max=10e-3, F_max_es=30, S_max_es=60, epsilon=10e-7, E_max=3, theta=10**-26, L=8, phi=0.02e-3, N0=-174, f_es_dev=0.02, f_ue_dev=0.02):\n",
        "        \"\"\"\n",
        "        Initialize the edge computing environment with given parameters.\n",
        "        \"\"\"\n",
        "        self.M = M  # Number of users\n",
        "        self.area_size = area_size  # Size of the area in which users are distributed\n",
        "        self.D_m = D_m  # Task data size\n",
        "        self.eta_m_range = eta_m_range  # Range of computation requirements\n",
        "        self.F_max_ue = F_max_ue * 1e9  # Maximum frequency of user equipment\n",
        "        self.P_max = 10 ** (P_max / 10)  # Convert maximum transmission power from dB to Watts\n",
        "        self.B = B  # Bandwidth\n",
        "        self.T_max = T_max  # Maximum tolerable delay\n",
        "        self.F_max_es = F_max_es * 1e9  # Maximum frequency of edge server\n",
        "        self.S_max_es = S_max_es * 1e3  # Maximum storage size of edge server\n",
        "        self.epsilon = epsilon  # Error tolerance for rate calculation\n",
        "        self.E_max = E_max * 1e-3  # Maximum energy consumption\n",
        "        self.theta = theta  # Energy coefficient\n",
        "        self.L = L  # Number of antennas\n",
        "        self.phi = phi  # Transmission time interval\n",
        "        self.R_min = 1e6  # Minimum data rate\n",
        "        self.N0 = N0  # Noise power in dBm\n",
        "        self.N0 = 10 ** ((N0 - 30) / 10)  # Convert noise power from dBm/Hz to Watts/Hz\n",
        "        self.PL_d = lambda d: 10 ** ((10 ** ((-35.3 - (37.6 * np.log10(d))) / 10)) / 10)  # Path loss model\n",
        "        self.f_es_dev = f_es_dev  #The deviation between the estimated value and the actual value of the processing rate of the ES\n",
        "        self.f_ue_dev = f_ue_dev  #The deviation between the estimated value and the actual value of the processing rate of the UE\n",
        "\n",
        "        self.user_device_params = []  # List to store parameters for each user device\n",
        "        self.initialize_user_device_params()  # Initialize user device parameters\n",
        "\n",
        "        self.server_params = self.initialize_server_params()  # Initialize server parameters\n",
        "\n",
        "        self.cache = []  # Cache to store tasks\n",
        "        self.current_cache_size = 0  # Current size of the cache\n",
        "        self.transmitting_tasks = []  # List to store transmitting tasks\n",
        "        self.processing_tasks = []  # List to store processing tasks\n",
        "        self.current_time = 0  # Current simulation time\n",
        "\n",
        "        # Initialize bandwidth and computation attributes\n",
        "        self.total_bandwidth = 0 # Initialize total bandwidth\n",
        "        self.total_computation = 0 # Initialize total computation\n",
        "\n",
        "    def initialize_user_device_params(self):\n",
        "        \"\"\"\n",
        "        Initialize parameters for each user device.\n",
        "        Randomly generates user-specific parameters such as path loss.\n",
        "        \"\"\"\n",
        "        for device_id in range(self.M):\n",
        "            d = np.random.uniform(0, self.area_size / 2)  # Distance to server\n",
        "            g_m = np.array([self.PL_d(d)])  # Path loss\n",
        "            h_bar = np.random.randn(1, self.L) + 1j * np.random.randn(1, self.L)  # Channel gain\n",
        "\n",
        "            self.user_device_params.append({\n",
        "                'device_id': device_id,  # Assign a unique ID to each device\n",
        "                'd': d,\n",
        "                'g_m': g_m,\n",
        "                'h_bar': h_bar,\n",
        "            })\n",
        "\n",
        "    def initialize_server_params(self):\n",
        "        \"\"\"\n",
        "        Initialize parameters for the edge server.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'S_max_es': self.S_max_es  # Maximum storage size\n",
        "        }\n",
        "\n",
        "    def calculate_gamma_m(self, b_m, p_m, user_id):\n",
        "        \"\"\"\n",
        "        Calculate the signal-to-noise ratio (SNR) for a given user.\n",
        "\n",
        "        Parameters:\n",
        "        - b_m (float): Bandwidth allocation\n",
        "        - p_m (float): Transmission power\n",
        "        - user_id (int): ID of the user\n",
        "\n",
        "        Returns:\n",
        "        - gamma_m (array): SNR values for the user's communication channel\n",
        "        \"\"\"\n",
        "        h_m = np.sqrt(self.user_device_params[user_id]['g_m'])[:, None] * self.user_device_params[user_id]['h_bar']  # Channel gain\n",
        "        gamma_m = (p_m * np.linalg.norm(h_m, axis=1) ** 2) / (b_m * self.B * self.N0)  # SNR\n",
        "\n",
        "        return gamma_m\n",
        "\n",
        "    def calculate_uplink_rate(self, b_m, p_m, user_id):\n",
        "        \"\"\"\n",
        "        Calculate the uplink data rate for a given user.\n",
        "\n",
        "        Parameters:\n",
        "        - b_m (float): Bandwidth allocation\n",
        "        - p_m (float): Transmission power\n",
        "        - user_id (int): ID of the user\n",
        "\n",
        "        Returns:\n",
        "        - R_m (float): Uplink data rate in bits/second\n",
        "        \"\"\"\n",
        "        gamma_m = self.calculate_gamma_m(b_m, p_m, user_id)  # Calculate the SINR for the m-th user\n",
        "        V_m = 1 - (1 / (1 + gamma_m) ** 2)  # Intermediate variable for rate calculation\n",
        "        Q_inv = np.sqrt(2) * erfcinv(2 * self.epsilon)  # Calculate the inverse of the Q-function for the outage probability\n",
        "        R_m = (self.B / np.log(2)) * ((b_m * np.log(1 + gamma_m)) - ((np.sqrt((b_m * V_m) / (self.phi * self.B))) * Q_inv))  # Uplink data rate\n",
        "\n",
        "        return R_m\n",
        "\n",
        "    def calculate_delay(self, alpha_m, cache_hit, b_m, p_m, D_m, f_ue_m, f_es_m, f_ue_est, f_es_est, eta_m, user_id):\n",
        "        \"\"\"\n",
        "        Calculate the end-to-end delay for a given task.\n",
        "\n",
        "        Parameters:\n",
        "        - alpha_m (float): Offloading decision\n",
        "        - cache_hit (int): Split factor (0 or 1)\n",
        "        - b_m (float): Bandwidth allocation\n",
        "        - p_m (float): Transmission power\n",
        "        - D_m (int): Data size\n",
        "        - f_ue_m (float): Computation capability of the user device\n",
        "        - f_es_m (float): Computation capability of the edge server\n",
        "        - f_ue_est (float): Estimation error for the user device's computation capability\n",
        "        - f_es_est (float): Estimation error for the edge server's computation capability\n",
        "        - eta_m (float): Computational intensity\n",
        "        - user_id (int): ID of the user\n",
        "\n",
        "        Returns:\n",
        "        - T_e2e (float): End-to-end delay in seconds\n",
        "        \"\"\"\n",
        "        actual_f_ue_m = f_ue_m - f_ue_est  # Actual processing rate of the user device\n",
        "        actual_f_es_m = f_es_m - f_es_est  # Actual processing rate of the Edge server\n",
        "\n",
        "        if cache_hit == 1:\n",
        "            T_es = (eta_m * D_m) / actual_f_es_m  # Only edge server processing delay\n",
        "            T_e2e = T_es\n",
        "\n",
        "        else:\n",
        "            T_ue = (alpha_m * eta_m * D_m) / actual_f_ue_m  # User device processing delay\n",
        "            R_m = self.calculate_uplink_rate(b_m, p_m, user_id)  # Uplink data rate\n",
        "            T_tr = (D_m * 8) / R_m  # Transmission delay\n",
        "            T_es = ((1 - alpha_m) * eta_m * D_m) / actual_f_es_m  # Edge server processing delay\n",
        "            T_e2e = T_ue + T_tr + T_es  # Total end-to-end delay\n",
        "\n",
        "        return T_e2e\n",
        "\n",
        "    def calculate_transmission_delay(self, b_m, p_m, D_m, user_id):\n",
        "        \"\"\"\n",
        "        Calculate the transmission delay for a given task.\n",
        "\n",
        "        Parameters:\n",
        "        - b_m (float): Bandwidth allocation\n",
        "        - p_m (float): Transmission power\n",
        "        - D_m (int): Data size\n",
        "        - user_id (int): ID of the user\n",
        "\n",
        "        Returns:\n",
        "        - T_co (float): Transmission delay in seconds\n",
        "        \"\"\"\n",
        "        R_m = self.calculate_uplink_rate(b_m, p_m, user_id)  # Calculate uplink data rate\n",
        "        T_co =  (D_m * 8) / R_m   # Transmission delay calculation based on task size and uplink rate\n",
        "\n",
        "        return T_co\n",
        "\n",
        "    def calculate_server_processing_delay(self, alpha_m, cache_hit, D_m, f_es_m, f_es_est, eta_m):\n",
        "        \"\"\"\n",
        "        Calculate the processing delay at the edge server for a given task.\n",
        "\n",
        "        Parameters:\n",
        "        - alpha_m (float): Offloading decision\n",
        "        - D_m (int): Data size\n",
        "        - cache_hit (0,1): 1 = Exist in cache and 0 not exist in cache\n",
        "        - f_es_m (float): Computation capability of the edge server\n",
        "        - f_es_est (float): Estimation error for the edge server's computation capability\n",
        "        - eta_m (float): Computational intensity\n",
        "\n",
        "        Returns:\n",
        "        - T_es (float): Processing delay at the edge server in seconds\n",
        "        \"\"\"\n",
        "\n",
        "        if cache_hit == 0:\n",
        "            T_es = ((1 - alpha_m) * eta_m * D_m) / (f_es_m - f_es_est)  # Processing delay at the edge server\n",
        "\n",
        "        else:\n",
        "            T_es = (eta_m * D_m) / (f_es_m - f_es_est)\n",
        "\n",
        "        return T_es\n",
        "\n",
        "    def calculate_energy_consumption(self, s_m, b_m, alpha_m, p_m, D_m, f_ue_m, f_ue_est, eta_m, user_id):\n",
        "        \"\"\"\n",
        "        Calculate the energy consumption for a given task.\n",
        "\n",
        "        Parameters:\n",
        "        - alpha_m (float): Offloading decision\n",
        "        - s_m (int): Split factor (0 or 1)\n",
        "        - f_ue_m (float): Computation capability of the user device\n",
        "        - b_m (float): Bandwidth allocation\n",
        "        - p_m (float): Transmission power\n",
        "        - f_ue_est (float): Estimation error for the user device's computation capability\n",
        "        - eta_m (float): Computational intensity\n",
        "\n",
        "        Returns:\n",
        "        - E_total (float): Total energy consumption in Joules\n",
        "        \"\"\"\n",
        "        R_m = self.calculate_uplink_rate(b_m, p_m, user_id)  # Calculate uplink data rate\n",
        "\n",
        "        actual_f_ue_m = f_ue_m - f_ue_est  # Calculate the actual processing rate of the UE\n",
        "\n",
        "        E_ue = alpha_m * (self.theta / 2 * eta_m * D_m * (actual_f_ue_m ** 2))  # Energy consumption at the user device\n",
        "        E_tx = ((1 - alpha_m) * (D_m * 8) * p_m) / R_m  # Transmission energy\n",
        "\n",
        "        if s_m == 1:  # Task is in cache\n",
        "            E_total = 0  # No energy consumed when task is in cache\n",
        "        else:\n",
        "            E_total = E_ue + E_tx  # Total energy consumption\n",
        "\n",
        "        return E_total\n",
        "\n",
        "    def manage_cache(self, task_info, task_delay):\n",
        "        \"\"\"\n",
        "        Manage the cache for storing and retrieving tasks.\n",
        "\n",
        "        Parameters:\n",
        "        - task_info (tuple): Task parameters to identify the task\n",
        "        - task_delay (float): Delay of the task\n",
        "\n",
        "        Returns:\n",
        "        - bool: True if the task is found in the cache, False otherwise\n",
        "        \"\"\"\n",
        "        if task_delay == 0:\n",
        "            for task in self.cache:\n",
        "                if task_info == task[0]:  # Check if the task is already in cache\n",
        "                    return True\n",
        "            return False\n",
        "\n",
        "        task_size = task_info['D_m'] * 8  # Task size\n",
        "        Server_Max_Capacity = self.server_params['S_max_es']  # Server maximum capacity\n",
        "\n",
        "        if (task_size + self.current_cache_size) <= Server_Max_Capacity:\n",
        "            self.cache.append((task_info, task_delay))  # Add task to cache\n",
        "            self.current_cache_size += task_size  # Update cache size\n",
        "            return True\n",
        "        else:\n",
        "            sorted_cache = sorted(self.cache, key=lambda x: x[1], reverse=True)  # Sort tasks by delay in descending order\n",
        "\n",
        "            while (task_size + self.current_cache_size) > Server_Max_Capacity:\n",
        "                if not sorted_cache:\n",
        "                    break  # Exit loop if sorted_cache is empty\n",
        "                last_task = sorted_cache.pop()  # Remove the last task from sorted_cache\n",
        "                self.cache.remove(last_task)  # Remove the task from the cache\n",
        "                self.current_cache_size -= last_task[0]['D_m'] * 8  # Update current cache size\n",
        "\n",
        "            self.cache.append((task_info, task_delay))  # Add task to cache\n",
        "            self.current_cache_size += task_size  # Update cache size\n",
        "\n",
        "            return True\n",
        "\n",
        "    def step(self, actions, tasks, users_id):\n",
        "        \"\"\"\n",
        "        Perform a simulation step for the given action.\n",
        "\n",
        "        Parameters:\n",
        "        - action (array): Array of action for each user\n",
        "        - tasks (array): Array of task for each user\n",
        "\n",
        "        Returns:\n",
        "        - tuple: (task_rewards, next_state, done)\n",
        "        \"\"\"\n",
        "        # Initialize cumulative metrics for the step\n",
        "        task_rewards = []  # List to store reward for each task\n",
        "        state_info = []  # List to store individual task and device state information\n",
        "        done = False\n",
        "\n",
        "        for action, task, user_id in zip(actions, tasks, users_id):\n",
        "\n",
        "            # Determine if the task is a cache hit or miss\n",
        "            cache_hit = 1 if self.manage_cache(task, 0) else 0\n",
        "\n",
        "            f_es_est = action['f_es_m'] * self.f_es_dev  # initialize f_es_est\n",
        "            f_ue_est = action['f_ue_m'] * self.f_ue_dev  # initialize f_ue_est\n",
        "\n",
        "            # Calculate the end-to-end delay for the task\n",
        "            delay = self.calculate_delay(\n",
        "                action['alpha_m'], cache_hit, action['b_m'], action['p_m'],\n",
        "                task['D_m'], action['f_ue_m'], action['f_es_m'], f_ue_est,\n",
        "                f_es_est, task['eta_m'], user_id\n",
        "            )\n",
        "\n",
        "            # Calculate the energy consumption for the task\n",
        "            energy = self.calculate_energy_consumption(\n",
        "                cache_hit, action['b_m'], action['alpha_m'], action['p_m'], task['D_m'], action['f_ue_m'],\n",
        "                f_es_est, task['eta_m'], user_id\n",
        "            )\n",
        "\n",
        "            # Calculate the uplink data rate for the user\n",
        "            R_m = self.calculate_uplink_rate(action['b_m'], action['p_m'], user_id)\n",
        "\n",
        "            # Manage task transmission and processing times\n",
        "            if cache_hit == 0:\n",
        "                transmission_end_time = self.current_time + self.calculate_transmission_delay(action['b_m'], action['p_m'],task['D_m'], user_id)\n",
        "                processing_end_time = transmission_end_time + self.calculate_server_processing_delay(action['alpha_m'], cache_hit, task['D_m'], action['f_es_m'], f_es_est, task['eta_m'])\n",
        "\n",
        "                self.transmitting_tasks.append((self.current_time, transmission_end_time, action['b_m']))\n",
        "                self.processing_tasks.append((transmission_end_time, processing_end_time, action['f_es_m']))\n",
        "\n",
        "                # Update cache with the task if it becomes eligible\n",
        "                self.manage_cache(task, delay)\n",
        "            else:\n",
        "                # For cache hit, only processing delay is considered\n",
        "                processing_end_time = self.current_time + self.calculate_server_processing_delay(action['alpha_m'], cache_hit, task['D_m'], action['f_es_m'], f_es_est, task['eta_m'])\n",
        "                self.processing_tasks.append((self.current_time, processing_end_time, action['f_es_m']))\n",
        "\n",
        "            # Calculate total bandwidth and computation resource usage at current time\n",
        "            self.total_bandwidth = sum(b for _, end_time, b in self.transmitting_tasks if end_time > self.current_time)\n",
        "            self.total_computation = sum(f for _, end_time, f in self.processing_tasks if end_time > self.current_time)\n",
        "\n",
        "            # Free resources for tasks that have completed transmission or processing\n",
        "            self.transmitting_tasks = [(start_time, end_time, b) for start_time, end_time, b in self.transmitting_tasks if end_time > self.current_time]\n",
        "            self.processing_tasks = [(start_time, end_time, f) for start_time, end_time, f in self.processing_tasks if end_time > self.current_time]\n",
        "\n",
        "            # Calculate reward\n",
        "            task_reward  = -delay - energy * 1e3\n",
        "\n",
        "            # Apply penalties for exceeding resource limits\n",
        "            if delay > task['T_max']:\n",
        "                task_reward -= 1e6\n",
        "            if energy > self.E_max:\n",
        "                task_reward -= 1e6\n",
        "            if R_m < self.R_min:\n",
        "                task_reward -= 1e6\n",
        "            if self.total_bandwidth > 1:\n",
        "                task_reward -= 1e6\n",
        "            if self.total_computation > self.F_max_es:\n",
        "                task_reward -= 1e6\n",
        "\n",
        "            # Check if the cumulative reward is below a certain threshold\n",
        "            if task_reward < -1e5:\n",
        "                done = True\n",
        "\n",
        "            # Store metrics and state information for the task\n",
        "            task_rewards.append(task_reward)\n",
        "\n",
        "            state_info.append({\n",
        "                'device_id': user_id,\n",
        "                'task': task,  # Include task information directly\n",
        "                'delay': delay,\n",
        "                'energy': energy,\n",
        "                'Occupied bandwidth': self.total_bandwidth,\n",
        "                'Occupied computation': self.total_computation\n",
        "            })\n",
        "\n",
        "        # Increment current simulation time\n",
        "        self.current_time += 0.010 \n",
        "\n",
        "        # Prepare the next state\n",
        "        next_state = self.get_state(state_info)\n",
        "\n",
        "        return task_rewards, next_state, done\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the environment to its initial state.\n",
        "        \"\"\"\n",
        "        self.cache = []  # Clear cache\n",
        "        self.current_cache_size = 0  # Reset cache size\n",
        "        self.transmitting_tasks = []  # Clear transmitting tasks\n",
        "        self.processing_tasks = []  # Clear processing tasks\n",
        "        self.current_time = 0  # Reset current time\n",
        "        self.initialize_user_device_params()  # Reinitialize user device parameters\n",
        "        self.total_bandwidth = 0  # Reinitialize total bandwidth\n",
        "        self.total_computation = 0  # Reinitialize total computation\n",
        "        self.server_params = self.initialize_server_params()  # Reinitialize server parameters\n",
        "\n",
        "        state_info = [{\n",
        "            'device_id': user_id,\n",
        "            'task': None,  # Initial task is None\n",
        "            'delay': 0,\n",
        "            'energy': 0,\n",
        "            'Occupied bandwidth': self.total_bandwidth,\n",
        "            'Occupied computation': self.total_computation\n",
        "        } for user_id in range(self.M)]\n",
        "\n",
        "        initial_state = self.get_state(state_info)  # Get the initial state\n",
        "\n",
        "        return initial_state\n",
        "\n",
        "    def get_state(self, state_info):\n",
        "        \"\"\"\n",
        "        Get the current state of the environment.\n",
        "        \"\"\"\n",
        "        state = {\n",
        "            'total_bandwidth': self.total_bandwidth,\n",
        "            'total_computation': self.total_computation,\n",
        "            'current_time': self.current_time,\n",
        "            'cache_size': self.current_cache_size,\n",
        "            'state_info': state_info  # Include task and device information\n",
        "        }\n",
        "        return state\n",
        "\n",
        "    def render(self):\n",
        "        print(f\"Total Bandwidth Used: {self.total_bandwidth}\")\n",
        "        print(f\"Total Computation Used: {self.total_computation}\")\n",
        "        print(f\"Current Cache Size: {self.current_cache_size}\")\n",
        "        print(f\"Number of Transmitting Tasks: {len(self.transmitting_tasks)}\")\n",
        "        print(f\"Number of Processing Tasks (Not Exist In Cache): {len(self.processing_tasks)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q-Learning Algorithm\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, env, num_users, alpha=0.1, gamma=0.85, epsilon=1, max_steps_per_episode=20):\n",
        "        self.env = env  # Environment for the agent\n",
        "        self.num_users = num_users  # Number of users/devices in the environment\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor for future rewards\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.q_table = {}  # Q-table to store Q-values for state-action pairs\n",
        "        self.max_steps_per_episode = max_steps_per_episode  # Maximum steps per episode\n",
        "\n",
        "    def get_state(self, state_info):\n",
        "        state = self.env.get_state(state_info)  # Get the state representation from the environment\n",
        "        return state\n",
        "\n",
        "    def make_hashable(self, d):\n",
        "        # Convert a dictionary or list into a hashable type (tuple)\n",
        "        if isinstance(d, dict):\n",
        "            return tuple(sorted((k, self.make_hashable(v)) for k, v in d.items()))\n",
        "        if isinstance(d, list):\n",
        "            return tuple(self.make_hashable(e) for e in d)\n",
        "        if isinstance(d, np.ndarray):\n",
        "            return tuple(d.tolist())\n",
        "        return d\n",
        "\n",
        "    def update_epsilon(self, decay_rate=0.995, min_epsilon=0.01):\n",
        "        self.epsilon = max(min_epsilon, self.epsilon * decay_rate)  # Decay the exploration rate\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state_key = self.make_hashable(state)  # Convert state to a hashable key\n",
        "\n",
        "        if state_key not in self.q_table:\n",
        "            self.q_table[state_key] = self.initialize_q_values()  # Initialize Q-values for unseen states\n",
        "\n",
        "        if np.random.uniform(0, 1) < self.epsilon:\n",
        "            action = self.generate_random_action()  # Explore: select a random action\n",
        "            action_key = tuple(action.items())\n",
        "            if action_key not in self.q_table[state_key]:\n",
        "                self.q_table[state_key][action_key] = -100  # Initialize unseen actions with a low Q-value\n",
        "        else:\n",
        "            numerical_q_values = {k: v for k, v in self.q_table[state_key].items() if isinstance(v, (int, float))}\n",
        "            if numerical_q_values:\n",
        "                action_key = max(numerical_q_values, key=numerical_q_values.get)  # Exploit: select action with highest Q-value\n",
        "                action = dict(action_key)\n",
        "\n",
        "            else:\n",
        "                action = self.generate_random_action()\n",
        "                action_key = tuple(action.items())\n",
        "                if action_key not in self.q_table[state_key]:\n",
        "                    self.q_table[state_key][action_key] = -100\n",
        "\n",
        "        return action\n",
        "\n",
        "    def initialize_q_values(self):\n",
        "        q_values = {}\n",
        "        random_action = self.generate_random_action()  # Generate a random action\n",
        "        q_values[tuple(random_action.items())] = -100  # Initialize its Q-value with a low value\n",
        "        return q_values\n",
        "    \n",
        "    def generate_random_action(self):\n",
        "        # Generate random values for the action parameters\n",
        "        alpha = np.random.choice(np.linspace(0, 1, 10)) # or np.round(np.random.uniform(0, 1),2)\n",
        "        b = np.random.choice(np.linspace(0.01, 3/self.num_users, 10)) # or np.round(np.random.uniform(0, 1),2)\n",
        "        p = np.random.choice(np.linspace(self.env.P_max / 10, self.env.P_max, 10)) # or np.round(np.random.randint(0, self.env.P_max),2)\n",
        "        f_ue = np.random.choice(np.linspace(self.env.F_max_ue / 10, self.env.F_max_ue, 10)) # or np.round(np.random.randint(0, self.env.F_max_ue),2)\n",
        "        f_es = np.random.choice(np.linspace(self.env.F_max_es / 100, self.env.F_max_es / 10, 10)) # or np.round(np.random.randint(0, self.env.F_max_es),2)\n",
        "\n",
        "        action = {\n",
        "            'alpha_m': alpha,\n",
        "            'b_m': b,\n",
        "            'p_m': p,\n",
        "            'f_ue_m': f_ue,\n",
        "            'f_es_m': f_es\n",
        "        }\n",
        "\n",
        "        return action\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        state_key = self.make_hashable(state)  # Convert state to a hashable key\n",
        "        action_key = tuple(action.items())  # Convert action to a hashable key\n",
        "        next_state_key = self.make_hashable(next_state)  # Convert next state to a hashable key\n",
        "        if isinstance(reward,list) or isinstance(reward,np.ndarray) : \n",
        "            reward = reward[0]\n",
        "\n",
        "        if state_key not in self.q_table:\n",
        "            self.q_table[state_key] = self.initialize_q_values()  # Initialize Q-values for unseen states\n",
        "\n",
        "        if action_key not in self.q_table[state_key]:\n",
        "            self.q_table[state_key][action_key] = -100  # Initialize unseen actions with a low Q-value\n",
        "\n",
        "        if next_state_key not in self.q_table:\n",
        "            self.q_table[next_state_key] = self.initialize_q_values()  # Initialize Q-values for unseen next states\n",
        "\n",
        "        current_q = self.q_table[state_key][action_key]  # Current Q-value\n",
        "        max_next_q = max(self.q_table[next_state_key].values())  # Maximum Q-value for the next state\n",
        "\n",
        "        # Update the Q-value using the Q-learning update rule\n",
        "        self.q_table[state_key][action_key] = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)\n",
        "\n",
        "    def create_task(self):\n",
        "        # Create a new task with random parameters within specified ranges\n",
        "        eta_m =  np.round(np.random.choice(np.linspace(self.env.eta_m_range[0], self.env.eta_m_range[1], 100)))\n",
        "        T_max_task = 10e-3  # Static according to article\n",
        "        task_info = {\n",
        "            'eta_m': eta_m,\n",
        "            'T_max': T_max_task,\n",
        "            'D_m': 1354  # Task data size\n",
        "        }\n",
        "        return task_info\n",
        "\n",
        "    def train(self, num_episodes):\n",
        "        # Lists to store average delay and energy values for each episode\n",
        "        avg_delays = []\n",
        "        avg_energies = []\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            # Initialize device state information for all users at the beginning of each episode\n",
        "            device_state_info = {user_id: {\n",
        "                'total_bandwidth': 0,\n",
        "                'total_computation': 0,\n",
        "                'current_time': 0,\n",
        "                'cache_size': 0,\n",
        "                'Occupied bandwidth': 0,\n",
        "                'Occupied computation': 0\n",
        "            } for user_id in range(self.num_users)}\n",
        "\n",
        "            # Reset the environment to get the initial state\n",
        "            #state_info = self.env.reset()\n",
        "            # Get the state representation from the environment\n",
        "            #state = self.get_state(state_info)\n",
        "            # Initialize total delay and energy for this episode\n",
        "            total_delay = 0\n",
        "            total_energy = 0\n",
        "            # Initialize the number of steps taken in this episode\n",
        "            num_steps = 0\n",
        "\n",
        "            for step in range(self.max_steps_per_episode):\n",
        "                # Initialize lists for actions, tasks, and user IDs\n",
        "                actions = []\n",
        "                tasks = []\n",
        "                users_id = []\n",
        "\n",
        "                # Generate a random number of tasks and assign them to devices\n",
        "                num_tasks = np.random.randint(self.num_users / 2, self.num_users + 1)\n",
        "                # Randomly distribute tasks to users\n",
        "                task_distribution = np.random.choice(range(self.num_users), num_tasks, replace=True)\n",
        "\n",
        "                # Initialize a list to store state representations for action selection\n",
        "                states_choose_actions = []\n",
        "\n",
        "                for user_id in task_distribution:\n",
        "                    # Create a new task\n",
        "                    task = self.create_task()\n",
        "                    # Append the task to the tasks list\n",
        "                    tasks.append(task)\n",
        "                    # Append the user ID to the users_id list\n",
        "                    users_id.append(user_id)\n",
        "\n",
        "                    # Update device state information with the new task\n",
        "                    device_state_info[user_id]['task'] = task\n",
        "                    device_state_info[user_id]['device_id'] = user_id\n",
        "\n",
        "                    # Create the state representation for action selection\n",
        "                    state_choose_actions = {\n",
        "                        'total_bandwidth': device_state_info[user_id]['total_bandwidth'],\n",
        "                        'total_computation': device_state_info[user_id]['total_computation'],\n",
        "                        'current_time': device_state_info[user_id]['current_time'],\n",
        "                        'cache_size': device_state_info[user_id]['cache_size'],\n",
        "                        'device_id': user_id,\n",
        "                        'task': device_state_info[user_id]['task'],\n",
        "                        'Occupied bandwidth': device_state_info[user_id]['Occupied bandwidth'],\n",
        "                        'Occupied computation': device_state_info[user_id]['Occupied computation']\n",
        "                    }\n",
        "                    # Append the state representation to the list\n",
        "                    states_choose_actions.append(state_choose_actions)\n",
        "\n",
        "                # Select actions for each device with a task using the Q-table\n",
        "                actions = [self.get_action(state) for state in states_choose_actions]\n",
        "\n",
        "                # Execute the actions in the environment\n",
        "                rewards, next_state_info, done = self.env.step(actions, tasks, users_id)\n",
        "                # Get the next state representation from the environment\n",
        "                next_state = self.get_state(next_state_info)\n",
        "\n",
        "                # List of state representations used for action selection\n",
        "                state_info_list = states_choose_actions\n",
        "                # List of next state information\n",
        "                next_state_info_list = next_state_info['state_info']\n",
        "\n",
        "                counter_Users = 0\n",
        "                # Update Q-table and device state information for each device\n",
        "                for user_id in task_distribution:\n",
        "                    # Get the current device information\n",
        "                    device_info = state_info_list[counter_Users]\n",
        "                    # Get the next device information\n",
        "                    next_device_info = next_state_info_list[counter_Users]\n",
        "                    # Get the action taken by the user\n",
        "                    action = actions[counter_Users]\n",
        "                    # Get the reward received by the user\n",
        "                    reward = rewards[counter_Users]\n",
        "\n",
        "                    # Extract delay and energy values from the next device information\n",
        "                    delay = next_device_info.pop('delay', 0)\n",
        "                    energy = next_device_info.pop('energy', 0)\n",
        "                    # Remove delay and energy from the current device information\n",
        "                    device_info.pop('delay', 0)\n",
        "                    device_info.pop('energy', 0)\n",
        "\n",
        "                    if isinstance(device_info, dict) and isinstance(next_device_info, dict):\n",
        "                        # Combine the next state information excluding the last entry\n",
        "                        next_state_without_last = {k: next_state[k] for k in list(next_state.keys())[:-1]}\n",
        "                        combined_next_state = {**next_state_without_last, **next_device_info}\n",
        "                        # Update the Q-table with the new state-action pair\n",
        "                        self.update_q_table(device_info, action, reward, combined_next_state)\n",
        "                        # Accumulate the total delay and energy for the episode\n",
        "                        total_delay += delay\n",
        "                        total_energy += energy\n",
        "\n",
        "                    # Update device state information with the combined next state\n",
        "                    device_state_info[user_id].update(combined_next_state)\n",
        "                    # Increment the counter for the number of users\n",
        "                    counter_Users += 1\n",
        "\n",
        "                # Increment the number of steps taken in the episode\n",
        "                num_steps += 1\n",
        "                if done:\n",
        "                    # Exit the loop if the episode is done\n",
        "                    break\n",
        "\n",
        "            # Calculate and store average delay and energy for the episode\n",
        "            avg_delay = (total_delay / (num_steps * len(state_info_list))) * 1000  # Convert to milliseconds\n",
        "            avg_energy = (total_energy / (num_steps * len(state_info_list)))\n",
        "            avg_delays.append(avg_delay)\n",
        "            avg_energies.append(avg_energy)\n",
        "\n",
        "            # Print the episode's results\n",
        "            print(f\"Episode {episode + 1}/{num_episodes} - Steps Count {num_steps} - Avg Delay: {avg_delay}, Avg Energy: {avg_energy}\")\n",
        "            print(\"-\" * 100)\n",
        "            # Update epsilon for the epsilon-greedy strategy\n",
        "            self.update_epsilon()\n",
        "\n",
        "        # Optionally plot the results\n",
        "        #self.plot_results(avg_delays, avg_energies)\n",
        "\n",
        "    def test(self, num_test_steps):\n",
        "        # Reset the environment to get the initial state\n",
        "        #state_info = self.env.reset()\n",
        "        # Get the state representation from the environment\n",
        "        #state = self.get_state(state_info)\n",
        "        # Initialize total delay and alpha values for this test\n",
        "        total_delay = 0\n",
        "        total_alpha = 0\n",
        "        # Counter for actual iterations\n",
        "        actual_steps = 0\n",
        "        # Set epsilon to 0 for testing (no exploration)\n",
        "        self.epsilon = 0\n",
        "\n",
        "        # Initialize device state information for all users\n",
        "        device_state_info = {user_id: {\n",
        "            'total_bandwidth': 0,\n",
        "            'total_computation': 0,\n",
        "            'current_time': 0,\n",
        "            'cache_size': 0,\n",
        "            'Occupied bandwidth': 0,\n",
        "            'Occupied computation': 0\n",
        "        } for user_id in range(self.num_users)}\n",
        "\n",
        "        for step in range(num_test_steps):\n",
        "            # Initialize lists for actions, tasks, and user IDs\n",
        "            actions = []\n",
        "            tasks = []\n",
        "            users_id = []\n",
        "\n",
        "            # Generate a random number of tasks and assign them to devices\n",
        "            num_tasks = np.random.randint(self.num_users / 2, self.num_users + 1)\n",
        "            # Randomly distribute tasks to users\n",
        "            task_distribution = np.random.choice(range(self.num_users), num_tasks, replace=True)\n",
        "\n",
        "            # Initialize a list to store state representations for action selection\n",
        "            states_choose_actions = []\n",
        "\n",
        "            for user_id in task_distribution:\n",
        "                # Create a new task\n",
        "                task = self.create_task()\n",
        "                # Append the task to the tasks list\n",
        "                tasks.append(task)\n",
        "                # Append the user ID to the users_id list\n",
        "                users_id.append(user_id)\n",
        "\n",
        "                # Update device state information with the new task\n",
        "                device_state_info[user_id]['task'] = task\n",
        "                device_state_info[user_id]['device_id'] = user_id\n",
        "\n",
        "                # Create the state representation for action selection\n",
        "                state_choose_actions = {\n",
        "                    'total_bandwidth': device_state_info[user_id]['total_bandwidth'],\n",
        "                    'total_computation': device_state_info[user_id]['total_computation'],\n",
        "                    'current_time': device_state_info[user_id]['current_time'],\n",
        "                    'cache_size': device_state_info[user_id]['cache_size'],\n",
        "                    'device_id': user_id,\n",
        "                    'task': device_state_info[user_id]['task'],\n",
        "                    'Occupied bandwidth': device_state_info[user_id]['Occupied bandwidth'],\n",
        "                    'Occupied computation': device_state_info[user_id]['Occupied computation']\n",
        "                }\n",
        "                # Append the state representation to the list\n",
        "                states_choose_actions.append(state_choose_actions)\n",
        "\n",
        "            # Select actions for each device with a task using the Q-table\n",
        "            actions = [self.get_action(state) for state in states_choose_actions]\n",
        "\n",
        "            # Execute the actions in the environment\n",
        "            rewards, next_state_info, done = self.env.step(actions, tasks, users_id)\n",
        "            # Get the next state representation from the environment\n",
        "            next_state = self.get_state(next_state_info)\n",
        "\n",
        "            # List of state representations used for action selection\n",
        "            state_info_list = states_choose_actions\n",
        "            # List of next state information\n",
        "            next_state_info_list = next_state_info['state_info']\n",
        "\n",
        "            counter_Users = 0\n",
        "\n",
        "            for user_id in task_distribution:\n",
        "                # Get the current device information\n",
        "                device_info = state_info_list[counter_Users]\n",
        "                # Get the next device information\n",
        "                next_device_info = next_state_info_list[counter_Users]\n",
        "                # Extract delay and energy values from the next device information\n",
        "                delay = next_device_info.pop('delay', 0)\n",
        "                next_device_info.pop('energy', 0)\n",
        "                device_info.pop('delay', 0)\n",
        "                device_info.pop('energy', 0)\n",
        "\n",
        "                # Accumulate the total delay and alpha values for the test\n",
        "                total_delay += delay\n",
        "                total_alpha += actions[counter_Users]['alpha_m']\n",
        "\n",
        "                # Combine the next state information excluding the last entry\n",
        "                next_state_without_last = {k: next_state[k] for k in list(next_state.keys())[:-1]}\n",
        "                combined_next_state = {**next_state_without_last, **next_device_info}\n",
        "                # Update device state information with the combined next state\n",
        "                device_state_info[user_id].update(combined_next_state)\n",
        "\n",
        "                counter_Users += 1\n",
        "\n",
        "            # Increment the counter for actual iterations\n",
        "            actual_steps += 1\n",
        "\n",
        "            if done:\n",
        "                # Exit the loop if the test is done\n",
        "                break\n",
        "\n",
        "        # Calculate and return the average delay and alpha for the test\n",
        "        avg_delay = (total_delay / (actual_steps * len(state_info_list))) * 1000  # Convert to milliseconds\n",
        "        avg_alpha = total_alpha / (actual_steps * len(state_info_list))\n",
        "\n",
        "        return avg_delay, avg_alpha\n",
        "\n",
        "    def plot_results(self, avg_delays, avg_energies):\n",
        "        episodes = np.arange(1, len(avg_delays) + 1)\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(episodes, avg_delays, label='Avg Delay')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Average Delay')\n",
        "        plt.title('Average Delay per Episode')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(episodes, avg_energies, label='Avg Energy')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Average Energy')\n",
        "        plt.title('Average Energy per Episode')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def save_q_table(self, filename):\n",
        "        def convert_keys_to_str(d):\n",
        "            if isinstance(d, dict):\n",
        "                return {str(k): convert_keys_to_str(v) for k, v in d.items()}\n",
        "            elif isinstance(d, np.ndarray):\n",
        "                return d.tolist()\n",
        "            else:\n",
        "                return d\n",
        "\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(convert_keys_to_str(self.q_table), f)\n",
        "\n",
        "    def load_q_table(self, filename):\n",
        "        def convert_keys_to_tuple(d):\n",
        "            if isinstance(d, dict):\n",
        "                return {eval(k): convert_keys_to_tuple(v) for k, v in d.items()}\n",
        "            else:\n",
        "                return d\n",
        "\n",
        "        with open(filename, 'r') as f:\n",
        "            self.q_table = convert_keys_to_tuple(json.load(f))\n",
        "\n",
        "# Assuming you have your EdgeComputingEnvironment defined as per your code\n",
        "#env = EdgeComputingEnvironment()\n",
        "\n",
        "# Define the number of users/devices\n",
        "#num_users = env.M\n",
        "\n",
        "# Initialize the Q-learning agent\n",
        "#agent = QLearningAgent(env, num_users)\n",
        "\n",
        "# Train the agent\n",
        "#num_episodes = 5  # Adjust the number of episodes as needed\n",
        "#agent.train(num_episodes)\n",
        "\n",
        "# Show the Q-table\n",
        "#agent.save_q_table('Q_table.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# learning and test for S_max_es and E_max\n",
        "\n",
        "# Assuming you have the QLearningAgent and EdgeComputingEnvironment classes defined as before\n",
        "env = EdgeComputingEnvironment()\n",
        "\n",
        "# Define your parameter ranges\n",
        "S_max_es_values = [0, 20, 40, 60]  # in KB\n",
        "E_max_values = [1.5, 2, 2.5, 3, 3.5, 4]  # in mJ\n",
        "\n",
        "# Define the number of users/devices\n",
        "num_users = env.M\n",
        "\n",
        "# Initialize the Q-learning agent\n",
        "agent = QLearningAgent(env, num_users)\n",
        "\n",
        "# Define the number of episodes for training and testing steps\n",
        "num_train_episodes = 100  # or any suitable number for training\n",
        "num_test_steps = 20  # or any suitable number for testing\n",
        "\n",
        "# Create the folder for Q-table files if it doesn't exist\n",
        "q_table_folder = 'Q-Tables_Test1'\n",
        "os.makedirs(q_table_folder, exist_ok=True)\n",
        "\n",
        "# Storage for results\n",
        "results = []\n",
        "\n",
        "# Training phase\n",
        "for S_max_es in S_max_es_values:\n",
        "    for E_max in E_max_values:\n",
        "        # Set the environment parameters\n",
        "        env.S_max_es = S_max_es * 1e3  # Convert KB to bytes\n",
        "        env.E_max = E_max * 1e-3  # Convert mJ to J\n",
        "\n",
        "        # Train the agent\n",
        "        agent.train(num_train_episodes)\n",
        "\n",
        "        # Save the Q-table for this configuration\n",
        "        q_table_filename = os.path.join(q_table_folder, f'Q_table_S{S_max_es}_E{E_max}.json')\n",
        "        agent.save_q_table(q_table_filename)\n",
        "\n",
        "# Testing phase\n",
        "for S_max_es in S_max_es_values:\n",
        "    for E_max in E_max_values:\n",
        "        # Load the Q-table for this configuration\n",
        "        q_table_filename = os.path.join(q_table_folder, f'Q_table_S{S_max_es}_E{E_max}.json')\n",
        "        agent.load_q_table(q_table_filename)\n",
        "\n",
        "        # Set the environment parameters\n",
        "        env.S_max_es = S_max_es * 1e3  # Convert KB to bytes\n",
        "        env.E_max = E_max * 1e-3  # Convert mJ to J\n",
        "\n",
        "        total_delay = 0\n",
        "\n",
        "        # Test the agent and get the average delay\n",
        "        avg_delay, avg_alpha = agent.test(num_test_steps)\n",
        "\n",
        "        # Store the results\n",
        "        results.append({\n",
        "            'S_max_es': S_max_es,\n",
        "            'E_max': E_max,\n",
        "            'avg_delay': avg_delay,\n",
        "        })\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for S_max_es in S_max_es_values:\n",
        "    # Extract delays and inspect for any non-numerical values or sequences\n",
        "    delays = [result['avg_delay'] for result in results if result['S_max_es'] == S_max_es]\n",
        "    \n",
        "    # Ensure all delays are numerical values\n",
        "    cleaned_delays = []\n",
        "    for delay in delays:\n",
        "        if isinstance(delay, (int, float, np.number)):  # Check if delay is a numerical value\n",
        "            cleaned_delays.append(delay)\n",
        "        elif isinstance(delay, (list, np.ndarray)):  # Check if delay is a list or array\n",
        "            if len(delay) == 1:\n",
        "                cleaned_delays.append(delay[0])  # Extract single element if it's a list/array of one element\n",
        "            else:\n",
        "                print(f\"Warning: Found non-scalar delay value for S_max_es = {S_max_es}: {delay}\")\n",
        "        else:\n",
        "            print(f\"Warning: Found non-numerical delay value for S_max_es = {S_max_es}: {delay}\")\n",
        "\n",
        "    # Plot the cleaned delays\n",
        "    plt.plot(E_max_values, cleaned_delays, marker='o', label=f'S_max_es = {S_max_es} KB')\n",
        "\n",
        "plt.xlabel('Maximum energy requirement, $E_{max}$ (mJ)')\n",
        "plt.ylabel('The total e2e latency (ms)')\n",
        "plt.title('The impact of edge caching capacity ($S_{max}^{es}$) and UE\\'s energy consumption budget ($E_{max}$) on e2e latency')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# learning and test for F_max_es and deviation_values\n",
        "\n",
        "# Assuming you have your EdgeComputingEnvironment and QLearningAgent defined as per your code\n",
        "env = EdgeComputingEnvironment()\n",
        "\n",
        "# Define the number of users/devices\n",
        "num_users = env.M\n",
        "\n",
        "# Define the different F_max_es values and f_es_dev, f_ue_dev values\n",
        "F_max_es_values = [30, 32, 34, 36, 38]\n",
        "deviation_values = [0.0, 0.02]\n",
        "\n",
        "# Initialize the Q-learning agent\n",
        "agent = QLearningAgent(env, num_users)\n",
        "\n",
        "# Define the number of episodes for training and testing steps\n",
        "num_train_episodes = 100  # or any suitable number for training\n",
        "num_test_steps = 20  # or any suitable number for testing\n",
        "\n",
        "# Create the folder for Q-table files if it doesn't exist\n",
        "q_table_folder = 'Q-Tables_Tese2'\n",
        "os.makedirs(q_table_folder, exist_ok=True)\n",
        "\n",
        "# Storage for results\n",
        "results = []\n",
        "\n",
        "# Train and save Q-tables\n",
        "for deviation in deviation_values:\n",
        "    env.f_es_dev = deviation\n",
        "    env.f_ue_dev = deviation\n",
        "\n",
        "    for F_max_es in F_max_es_values:\n",
        "        env.F_max_es = F_max_es * 1e9\n",
        "\n",
        "        # Train the agent\n",
        "        agent.train(num_train_episodes)\n",
        "\n",
        "        # Save the Q-table for this configuration\n",
        "        q_table_filename = os.path.join(q_table_folder, f'Q_table_F{F_max_es}_Dev{deviation}.json')\n",
        "        agent.save_q_table(q_table_filename)\n",
        "\n",
        "# Test and collect metrics\n",
        "for deviation in deviation_values:\n",
        "    for F_max_es in F_max_es_values:\n",
        "\n",
        "        # Load the Q-table for this configuration\n",
        "        q_table_filename = os.path.join(q_table_folder, f'Q_table_F{F_max_es}_Dev{deviation}.json')\n",
        "        agent.load_q_table(q_table_filename)\n",
        "\n",
        "        env.f_es_dev = deviation\n",
        "        env.f_ue_dev = deviation\n",
        "        env.F_max_es = F_max_es * 1e9\n",
        "\n",
        "        total_delay = 0\n",
        "        total_alpha = 0\n",
        "\n",
        "        # Test the agent and get the average delay and alpha\n",
        "        avg_delay, avg_alpha = agent.test(num_test_steps)\n",
        "\n",
        "        # Store the results\n",
        "        results.append({\n",
        "            'F_max_es': F_max_es,\n",
        "            'f_dev': deviation,\n",
        "            'avg_delay': avg_delay,\n",
        "            'avg_alpha': avg_alpha\n",
        "        })\n",
        "\n",
        "# Prepare data for plotting\n",
        "delays_data = {deviation: [] for deviation in deviation_values}\n",
        "alphas_data = {deviation: [] for deviation in deviation_values}\n",
        "\n",
        "# Fill the data lists with default values if missing\n",
        "for F_max_es in F_max_es_values:\n",
        "    for deviation in deviation_values:\n",
        "        found = False\n",
        "        for result in results:\n",
        "            if result['F_max_es'] == F_max_es and result['f_dev'] == deviation:\n",
        "                delays_data[deviation].append(result['avg_delay'][0] if isinstance(result['avg_delay'], np.ndarray) else result['avg_delay'])\n",
        "                if deviation == 0:\n",
        "                    alphas_data[deviation].append(result['avg_alpha'])\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            delays_data[deviation].append(0)\n",
        "            if deviation == 0:\n",
        "                alphas_data[deviation].append(0)\n",
        "\n",
        "# Plotting the results\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Plot for delay\n",
        "bar_width = 0.35\n",
        "index = np.arange(len(F_max_es_values))\n",
        "\n",
        "for i, deviation in enumerate(deviation_values):\n",
        "    ax1.bar(index + i * bar_width, delays_data[deviation], bar_width, label=f'Latency, $\\hat{{f}} = {deviation}$', alpha=0.6)\n",
        "\n",
        "ax1.set_xlabel(\"Maximum ES's processing rate (GHz)\")\n",
        "ax1.set_ylabel(\"The total latency (ms)\")\n",
        "ax1.set_title(\"The impact of ES's processing rate, deviation values, and offloading behavior\")\n",
        "ax1.set_xticks(index + bar_width / 2)\n",
        "ax1.set_xticklabels(F_max_es_values)\n",
        "ax1.legend(loc='upper left')\n",
        "ax1.grid(True)\n",
        "\n",
        "# Plot for average alphas\n",
        "ax2 = ax1.twinx()\n",
        "alphas = alphas_data[0]\n",
        "ax2.plot(index + bar_width / 2, alphas, marker='o', color='red', label='Average offloading portions, $\\hat{f} = 0$')\n",
        "\n",
        "ax2.set_ylabel(\"Average UEs offloading portions\")\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
