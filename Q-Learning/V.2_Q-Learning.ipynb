{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "G1SRDJmQ9V83",
        "outputId": "daae3ba5-1b83-444f-cc08-e677e827dcee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1/1 - Total Reward: [-1383787.33239409], Avg Delay: [1304875.29183511], Avg Energy: [0.00061789], Tasks Created: 10\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-4de5471e5bd0>\u001b[0m in \u001b[0;36m<cell line: 150>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0mE_max_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m \u001b[0mtest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_max_es_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_max_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;31m# Plot the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-4de5471e5bd0>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, S_max_es_values, E_max_values)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_users\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0mtotal_delay\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_delay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-4de5471e5bd0>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_random_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mbest_action_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_action_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "class QLearningAgent:\n",
        "    def __init__(self, env, num_users, alpha=0.1, gamma=0.9, epsilon=0.1, max_steps_per_episode=10):\n",
        "        self.env = env\n",
        "        self.num_users = num_users\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.q_table = {}\n",
        "        self.max_steps_per_episode = max_steps_per_episode\n",
        "\n",
        "    def get_state(self):\n",
        "        state_array = self.env.get_state()\n",
        "        return tuple(state_array)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state_key = tuple(state)\n",
        "        if state_key not in self.q_table:\n",
        "            self.q_table[state_key] = self.initialize_q_values()\n",
        "\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return self.generate_random_action()\n",
        "        else:\n",
        "            best_action_key = max(self.q_table[state_key], key=self.q_table[state_key].get)\n",
        "            return dict(best_action_key)\n",
        "\n",
        "    def initialize_q_values(self):\n",
        "        alpha_values = np.linspace(0, 1, 10)\n",
        "        b_values = np.linspace(0, 0.5, 10)\n",
        "        p_values = np.linspace(1, self.env.P_max, 10)\n",
        "        f_ue_values = np.linspace(1, self.env.F_max_ue, 10)\n",
        "        f_es_values = np.linspace(1, self.env.F_max_es/10, 10)\n",
        "\n",
        "        q_values = {}\n",
        "        for alpha in alpha_values:\n",
        "            for b in b_values:\n",
        "                for p in p_values:\n",
        "                    for f_ue in f_ue_values:\n",
        "                        for f_es in f_es_values:\n",
        "                            action = {\n",
        "                                'alpha_m': alpha,\n",
        "                                'b_m': b,\n",
        "                                'p_m': p,\n",
        "                                'f_ue_m': f_ue,\n",
        "                                'f_es_m': f_es\n",
        "                            }\n",
        "                            q_values[frozenset(action.items())] = 0\n",
        "\n",
        "        return q_values\n",
        "\n",
        "    def generate_random_action(self):\n",
        "        alpha_values = np.linspace(0, 1, 10)\n",
        "        b_values = np.linspace(0, 0.5, 10)\n",
        "        p_values = np.linspace(1, self.env.P_max, 10)\n",
        "        f_ue_values = np.linspace(1, self.env.F_max_ue, 10)\n",
        "        f_es_values = np.linspace(1, self.env.F_max_es/10, 10)\n",
        "\n",
        "        return {\n",
        "            'alpha_m': np.random.choice(alpha_values),\n",
        "            'b_m': np.random.choice(b_values),\n",
        "            'p_m': np.random.choice(p_values),\n",
        "            'f_ue_m': np.random.choice(f_ue_values),\n",
        "            'f_es_m': np.random.choice(f_es_values)\n",
        "        }\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        next_state = tuple(next_state)\n",
        "        state = tuple(state)\n",
        "\n",
        "        if next_state not in self.q_table:\n",
        "            self.q_table[next_state] = self.initialize_q_values()\n",
        "\n",
        "        action_key = frozenset(action.items())\n",
        "        best_next_action = max(self.q_table[next_state], key=self.q_table[next_state].get)\n",
        "        self.q_table[state][action_key] += self.alpha * (reward + self.gamma * self.q_table[next_state][best_next_action] - self.q_table[state][action_key])\n",
        "\n",
        "    def train(self, num_episodes):\n",
        "        rewards = []  # List to store rewards for each episode\n",
        "        for episode in range(num_episodes):\n",
        "            state = self.get_state()  # Get the initial state\n",
        "            self.env.reset()  # Reset the environment\n",
        "            total_reward = 0  # Initialize total reward for this episode\n",
        "            total_delay = 0  # Initialize total delay\n",
        "            total_energy = 0  # Initialize total energy consumption\n",
        "            done = False  # Initialize the done flag\n",
        "            steps = 0  # Initialize step counter\n",
        "            task_count = 0  # Initialize task counter\n",
        "\n",
        "            while not done and steps < self.max_steps_per_episode:\n",
        "                action = self.get_action(state)  # Select an action\n",
        "                reward, next_state, done = self.env.step([action for _ in range(self.num_users)])  # Take a step in the environment\n",
        "                self.update_q_table(state, action, reward, next_state)  # Update the Q-table\n",
        "                state = next_state  # Update the state\n",
        "                total_reward += reward  # Accumulate reward\n",
        "                total_delay += next_state['total_delay']  # Accumulate delay\n",
        "                total_energy += next_state['total_energy']  # Accumulate energy consumption\n",
        "                steps += 1  # Increment step counter\n",
        "                task_count += 1  # Increment task counter\n",
        "\n",
        "            rewards.append(total_reward)  # Store the total reward for this episode\n",
        "            avg_delay = total_delay / steps  # Calculate average delay\n",
        "            avg_energy = total_energy / steps  # Calculate average energy consumption\n",
        "            print(f\"Episode {episode + 1}/{num_episodes} - Total Reward: {total_reward}, Avg Delay: {avg_delay}, Avg Energy: {avg_energy}, Tasks Created: {task_count}\")\n",
        "\n",
        "        return rewards\n",
        "\n",
        "\n",
        "    def test(self, S_max_es_values, E_max_values):\n",
        "        results = []\n",
        "        for S_max_es in S_max_es_values:\n",
        "            for E_max in E_max_values:\n",
        "                self.env.S_max_es = S_max_es\n",
        "                self.env.E_max = E_max\n",
        "\n",
        "                delays = []\n",
        "                for _ in range(10):\n",
        "                    self.env.reset()\n",
        "                    state = self.get_state()\n",
        "                    done = False\n",
        "                    total_delay = 0\n",
        "\n",
        "                    while not done:\n",
        "                        action = self.get_action(state)\n",
        "                        _, next_state, done = self.env.step([action for _ in range(self.num_users)])\n",
        "                        total_delay += next_state['total_delay']\n",
        "                        state = next_state\n",
        "\n",
        "                    delays.append(total_delay)\n",
        "\n",
        "                avg_delay = np.mean(delays)\n",
        "                results.append((S_max_es, E_max, avg_delay))\n",
        "\n",
        "        return results\n",
        "\n",
        "# Create the environment\n",
        "env = EdgeComputingEnvironment()\n",
        "\n",
        "# Initialize the Q-learning agent\n",
        "agent = QLearningAgent(env, num_users=env.M)\n",
        "\n",
        "# Train the agent\n",
        "num_episodes = 1\n",
        "training_rewards = agent.train(num_episodes)\n",
        "\n",
        "# Test the agent with different S_max_es and E_max values\n",
        "S_max_es_values = [60]\n",
        "E_max_values = [1.5]\n",
        "\n",
        "test_results = agent.test(S_max_es_values, E_max_values)\n",
        "\n",
        "# Plot the results\n",
        "x = np.arange(len(E_max_values))\n",
        "for S_max_es in S_max_es_values:\n",
        "    y = [result[2] for result in test_results if result[0] == S_max_es]\n",
        "    plt.plot(x, y, label=f'S_max_es = {S_max_es} KB')\n",
        "\n",
        "plt.xlabel('E_max (mJ)')\n",
        "plt.ylabel('Average Delay (ms)')\n",
        "plt.title('Average Delay vs. E_max for Different S_max_es Values')\n",
        "plt.xticks(x, E_max_values)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
