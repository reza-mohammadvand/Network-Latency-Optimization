{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import erfcinv\n",
    "\n",
    "class EdgeComputingEnvironment:\n",
    "    def __init__(self, M=5, area_size=100, D_m=1354, eta_m_range=(100, 300), F_max_ue=1.5e9, P_max=23, B=5e6, T_max=10e-3, F_max_es=30e9, S_max_es=60e3, epsilon=10**-7, E_max=3e-3, theta=10**-26, L=8, phi=1.0, N0_dbm=-174, simulation_max_time=100):\n",
    "        \"\"\"\n",
    "        Initialize the edge computing environment with given parameters.\n",
    "        \"\"\"\n",
    "        self.M = M  # Number of users\n",
    "        self.area_size = area_size  # Size of the area in which users are distributed\n",
    "        self.D_m = D_m  # Task data size\n",
    "        self.eta_m_range = eta_m_range  # Range of computation requirements\n",
    "        self.F_max_ue = F_max_ue  # Maximum frequency of user equipment\n",
    "        self.P_max = 10 ** (P_max / 10)  # Convert maximum transmission power from dB to Watts\n",
    "        self.B = B  # Bandwidth\n",
    "        self.T_max = T_max  # Maximum tolerable delay\n",
    "        self.F_max_es = F_max_es  # Maximum frequency of edge server\n",
    "        self.S_max_es = S_max_es  # Maximum storage size of edge server\n",
    "        self.epsilon = epsilon  # Error tolerance for rate calculation\n",
    "        self.E_max = E_max  # Maximum energy consumption\n",
    "        self.theta = theta  # Energy coefficient\n",
    "        self.L = L  # Number of antennas\n",
    "        self.phi = phi  # Transmission probability\n",
    "        self.R_min = 1e6  # Minimum data rate\n",
    "        self.N0_dbm = N0_dbm  # Noise power in dBm\n",
    "        self.N0 = 10 ** (N0_dbm / 10) / 1000  # Convert noise power from dBm/Hz to Watts/Hz\n",
    "        self.simulation_max_time = simulation_max_time  # Maximum simulation time\n",
    "        self.PL_d = lambda d: 10 ** ((-35.3 - 37.6 * np.log10(d)) / 10)  # Path loss model\n",
    "\n",
    "\n",
    "        self.user_device_params = []  # List to store parameters for each user device\n",
    "        self.initialize_user_device_params()  # Initialize user device parameters\n",
    "\n",
    "        self.server_params = self.initialize_server_params()  # Initialize server parameters\n",
    "\n",
    "        self.cache = []  # Cache to store tasks\n",
    "        self.current_cache_size = 0  # Current size of the cache\n",
    "        self.transmitting_tasks = []  # List to store transmitting tasks\n",
    "        self.processing_tasks = []  # List to store processing tasks\n",
    "        self.current_time = 0  # Current simulation time\n",
    "        self.total_delay = 0  # Initialize total delay\n",
    "        self.total_energy = 0  # Initialize total energy consumption\n",
    "\n",
    "        # Initialize bandwidth and computation attributes\n",
    "        self.total_bandwidth = 0 # Initialize total bandwidth\n",
    "        self.total_computation = 0 # Initialize total computation\n",
    "        self.avg_delay = 0\n",
    "        self.avg_energy = 0\n",
    "\n",
    "\n",
    "\n",
    "    def initialize_user_device_params(self):\n",
    "        \"\"\"\n",
    "        Initialize parameters for each user device.\n",
    "        \"\"\"\n",
    "        for device_id in range(self.M):\n",
    "            d = np.random.uniform(0, self.area_size / 2)  # Distance to server\n",
    "            g_m = np.array([self.PL_d(d)])  # Path loss\n",
    "            h_bar = np.random.randn(1, self.L) + 1j * np.random.randn(1, self.L)  # Channel gain\n",
    "\n",
    "            self.user_device_params.append({\n",
    "                'device_id': device_id,  # Assign a unique ID to each device\n",
    "                'd': d,\n",
    "                'g_m': g_m,\n",
    "                'h_bar': h_bar,\n",
    "            })\n",
    "\n",
    "    def initialize_server_params(self):\n",
    "        \"\"\"\n",
    "        Initialize parameters for the edge server.\n",
    "        \"\"\"\n",
    "        f_es_m = np.random.choice(np.linspace(1, self.F_max_es, 10, dtype=int))  # Server frequency\n",
    "        f_es_est = f_es_m * 0.02  # Estimated server frequency\n",
    "\n",
    "        return {\n",
    "            'f_es_m': f_es_m,\n",
    "            'f_es_est': f_es_est,\n",
    "            'S_max_es': self.S_max_es  # Maximum storage size\n",
    "        }\n",
    "\n",
    "    def create_task(self):\n",
    "        \"\"\"\n",
    "        Create a new task for a specific user.\n",
    "        \"\"\"\n",
    "        eta_m = np.random.choice(np.linspace(self.eta_m_range[0], self.eta_m_range[1], 10))  # Computation requirement\n",
    "        T_max_task = np.random.choice(np.linspace(self.T_max / 2, self.T_max, 10))  # Maximum tolerable delay for task\n",
    "        T_max_task = 0.001 # Static for article\n",
    "        task_info = {\n",
    "            'eta_m': eta_m,\n",
    "            'T_max': T_max_task,\n",
    "            'D_m': self.D_m  # Task data size\n",
    "        }\n",
    "        return task_info\n",
    "\n",
    "    def calculate_gamma_m(self, b_m, p_m, user_id):\n",
    "        \"\"\"\n",
    "        Calculate the signal-to-noise ratio (SNR) for a given user.\n",
    "        \"\"\"\n",
    "        h_m = np.sqrt(self.user_device_params[user_id]['g_m'])[:, None] * self.user_device_params[user_id]['h_bar']  # Channel gain\n",
    "        gamma_m = (p_m * np.linalg.norm(h_m, axis=1) ** 2) / (b_m * self.B * self.N0)  # SNR\n",
    "\n",
    "        return gamma_m\n",
    "\n",
    "    def calculate_uplink_rate(self, b_m, p_m, user_id):\n",
    "        \"\"\"\n",
    "        Calculate the uplink data rate for a given user.\n",
    "        \"\"\"\n",
    "        gamma_m = self.calculate_gamma_m(b_m, p_m, user_id)  # SNR\n",
    "        V_m = 1 - (1 / (1 + gamma_m) ** 2)  # Intermediate variable for rate calculation\n",
    "        Q_inv = np.sqrt(2) * erfcinv(2 * self.epsilon)  # Inverse Q-function\n",
    "        R_m = (self.B / np.log(2)) * (b_m * np.log(1 + gamma_m) - np.sqrt((b_m * V_m) / (self.phi * self.B)) * Q_inv)  # Uplink data rate\n",
    "\n",
    "        return R_m\n",
    "\n",
    "    def calculate_delay(self, alpha_m, cache_hit, b_m, p_m, D_m, f_ue_m, f_es_m, f_ue_est, f_es_est, eta_m, user_id):\n",
    "        \"\"\"\n",
    "        Calculate the end-to-end delay for a given task.\n",
    "        \"\"\"\n",
    "        actual_f_ue_m = f_ue_m - f_ue_est  # Actual processing rate of the user device\n",
    "\n",
    "        if cache_hit == 1:\n",
    "            T_e2e = (1 - alpha_m) * eta_m * D_m / (f_es_m - f_es_est)  # Delay if task is in cache\n",
    "        else:\n",
    "            T_ue = (alpha_m * eta_m * D_m) / actual_f_ue_m  # User device processing delay\n",
    "            R_m = self.calculate_uplink_rate(b_m, p_m, user_id)  # Uplink data rate\n",
    "            T_tr = D_m / R_m  # Transmission delay\n",
    "            T_es = (1 - alpha_m) * eta_m * D_m / (f_es_m - f_es_est)  # Edge server processing delay\n",
    "            T_e2e = T_ue + T_tr + T_es  # Total end-to-end delay\n",
    "\n",
    "        return T_e2e\n",
    "\n",
    "    def calculate_energy_consumption(self, s_m, b_m, alpha_m, p_m, D_m, f_ue_m, f_ue_est, eta_m, user_id):\n",
    "        \"\"\"\n",
    "        Calculate the energy consumption for a given task.\n",
    "        \"\"\"\n",
    "        R_m = self.calculate_uplink_rate(b_m, p_m, user_id)  # Calculate uplink data rate\n",
    "\n",
    "        actual_f_ue_m = f_ue_m - f_ue_est  # Calculate the actual processing rate of the UE\n",
    "\n",
    "        E_ue = alpha_m * (self.theta / 2 * eta_m * D_m * (actual_f_ue_m ** 2))  # Energy consumption at the user device\n",
    "        E_tx = ((1 - alpha_m) * D_m * p_m) / R_m  # Transmission energy\n",
    "\n",
    "        if s_m == 1:  # Task is in cache\n",
    "            E_total = 0  # No energy consumed when task is in cache\n",
    "        else:\n",
    "            E_total = (1 - s_m) * (E_ue + E_tx)  # Total energy consumption\n",
    "\n",
    "\n",
    "        return E_total\n",
    "\n",
    "    def manage_cache(self, task_info, task_delay):\n",
    "        \"\"\"\n",
    "        Manage the cache for storing and retrieving tasks.\n",
    "        \"\"\"\n",
    "        if task_delay == 0:\n",
    "            for task in self.cache:\n",
    "                if task_info == task[0]:  # Check if the task is already in cache\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        task_size = task_info['D_m']  # Task size\n",
    "        Server_Max_Capacity = self.server_params['S_max_es']  # Server maximum capacity\n",
    "\n",
    "        if (task_size + self.current_cache_size) <= Server_Max_Capacity:\n",
    "            self.cache.append((task_info, task_delay))  # Add task to cache\n",
    "            self.current_cache_size += task_size  # Update cache size\n",
    "            return True\n",
    "        else:\n",
    "            sorted_cache = sorted(self.cache, key=lambda x: x[1], reverse=True)  # Sort tasks by delay in descending order\n",
    "\n",
    "            while (task_size + self.current_cache_size) > Server_Max_Capacity:\n",
    "                if not sorted_cache:\n",
    "                    break  # Exit loop if sorted_cache is empty\n",
    "                last_task = sorted_cache.pop()  # Remove the last task from sorted_cache\n",
    "                self.cache.remove(last_task)  # Remove the task from the cache\n",
    "                self.current_cache_size -= last_task[0]['D_m']  # Update current cache size\n",
    "\n",
    "            self.cache.append((task_info, task_delay))  # Add task to cache\n",
    "            self.current_cache_size += task_size  # Update cache size\n",
    "\n",
    "            return True\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Execute a single simulation step.\n",
    "        \"\"\"\n",
    "        # Initialize cumulative metrics for the step\n",
    "        task_rewards = []  # List to store reward for each task\n",
    "        individual_task_params = []  # List to store individual task parameters\n",
    "        num_tasks = len(actions)  # Number of tasks\n",
    "        total_delay = 0  # Initialize total delay\n",
    "        total_energy = 0  # Initialize total energy consumption\n",
    "\n",
    "        for user_id, action in enumerate(actions):\n",
    "            # Create a new task (user_id not necessary for task creation in this case)\n",
    "            task = self.create_task()\n",
    "            # Determine if the task is a cache hit or miss\n",
    "            cache_hit = 1 if self.manage_cache(task, 0) else 0\n",
    "            f_es_est = action['f_es_m'] * 0.02\n",
    "            f_ue_est = action['f_ue_m'] * 0.02\n",
    "\n",
    "\n",
    "            # Calculate the end-to-end delay for the task\n",
    "            delay = self.calculate_delay(\n",
    "                action['alpha_m'], cache_hit, action['b_m'], action['p_m'],\n",
    "                task['D_m'], action['f_ue_m'], action['f_es_m'], f_ue_est,\n",
    "                f_es_est, task['eta_m'], user_id\n",
    "            )\n",
    "            total_delay += delay\n",
    "            self.total_delay += delay\n",
    "            # Calculate the energy consumption for the task\n",
    "            energy = self.calculate_energy_consumption(\n",
    "                cache_hit, action['b_m'], action['alpha_m'], action['p_m'], task['D_m'], action['f_ue_m'],\n",
    "                f_es_est, task['eta_m'], user_id\n",
    "            )\n",
    "            total_energy += energy\n",
    "            self.total_energy += energy\n",
    "\n",
    "            # Calculate the uplink data rate for the user\n",
    "            R_m = self.calculate_uplink_rate(action['b_m'], action['p_m'], user_id)\n",
    "            print(\"delay :\",delay, \"user_id :\",user_id, \"task: \", task, \"cache_hit :\", cache_hit, \"energy :\",energy, \"R_m :\",R_m  )\n",
    "            \n",
    "            # Manage task transmission and processing times\n",
    "            if cache_hit == 0:\n",
    "                transmission_end_time = self.current_time + task['D_m'] / R_m\n",
    "                processing_end_time = transmission_end_time + (1 - action['alpha_m']) * task['eta_m'] * task['D_m'] / (action['f_es_m'] - self.server_params['f_es_est'])\n",
    "\n",
    "                self.transmitting_tasks.append((self.current_time, transmission_end_time, action['b_m']))\n",
    "                self.processing_tasks.append((transmission_end_time, processing_end_time, ((1 - action['alpha_m']) * action['f_es_m'])))\n",
    "\n",
    "                # Update cache with the task if it becomes eligible\n",
    "                self.manage_cache(task, delay)\n",
    "            else:\n",
    "                # For cache hit, only processing delay is considered\n",
    "                processing_end_time = self.current_time + (1 - action['alpha_m']) * task['eta_m'] * task['D_m'] / (action['f_es_m'] - self.server_params['f_es_est'])\n",
    "                self.processing_tasks.append((self.current_time, processing_end_time, action['f_es_m']))\n",
    "\n",
    "            # Store individual task parameters\n",
    "            individual_task_params.append({\n",
    "                'delay': delay,\n",
    "                'alpha_m': action['alpha_m'],\n",
    "                'task_reward': None,  # Will be calculated later\n",
    "                'energy': energy,\n",
    "                'R_m': R_m,\n",
    "                'cache_hit': cache_hit,\n",
    "                'device_id': user_id,\n",
    "                'b_m': action['b_m'],\n",
    "                'p_m': action['p_m'],\n",
    "                'f_ue_m': action['f_ue_m'],\n",
    "                'f_es_m': action['f_es_m'],\n",
    "                'eta_m': task['eta_m'],\n",
    "                'T_max': task['T_max']\n",
    "            })\n",
    "\n",
    "\n",
    "        # Calculate total bandwidth and computation resource usage at current time\n",
    "        self.total_bandwidth = sum(b for _, end_time, b in self.transmitting_tasks if end_time > self.current_time)\n",
    "        self.total_computation = sum(f for _, end_time, f in self.processing_tasks if end_time > self.current_time)\n",
    "\n",
    "        # Free resources for tasks that have completed transmission or processing\n",
    "        self.transmitting_tasks = [(start_time, end_time, b) for start_time, end_time, b in self.transmitting_tasks if end_time > self.current_time]\n",
    "        self.processing_tasks = [(start_time, end_time, f) for start_time, end_time, f in self.processing_tasks if end_time > self.current_time]\n",
    "        Penalties = []\n",
    "        # Calculate reward\n",
    "        for params in individual_task_params:\n",
    "            task_reward = -params['delay'] - params['energy'] * 1e3\n",
    "\n",
    "            # Apply penalties for exceeding resource limits\n",
    "            if params['delay'] > params['T_max']:\n",
    "                task_reward -= 1e6\n",
    "                Penalties.append(\"T_max\")\n",
    "            if params['energy'] > self.E_max:\n",
    "                task_reward -= 1e6\n",
    "                Penalties.append(\"E_max\")\n",
    "            if params['R_m'] < self.R_min:\n",
    "                task_reward -= 1e6\n",
    "                Penalties.append(\"R_m\")\n",
    "\n",
    "            print(Penalties)\n",
    "\n",
    "            params['task_reward'] = task_reward\n",
    "            task_rewards.append(task_reward)\n",
    "\n",
    "\n",
    "        # Calculate average metrics per task\n",
    "        self.avg_delay = total_delay / num_tasks if num_tasks > 0 else 0\n",
    "        self.avg_energy = total_energy / num_tasks if num_tasks > 0 else 0\n",
    "        reward = sum(task_rewards) / num_tasks if num_tasks > 0 else 0\n",
    "\n",
    "        if self.total_bandwidth > 1:\n",
    "                reward -= 1e6\n",
    "                Penalties.append(\"Bandwith\")\n",
    "        if self.total_computation > self.F_max_es:\n",
    "                reward -= 1e6\n",
    "                Penalties.append(\"F_Max\")\n",
    "\n",
    "        print(Penalties)\n",
    "\n",
    "        # Increment current simulation time\n",
    "        self.current_time += 1\n",
    "\n",
    "        # Check if the simulation has reached its maximum allowed time\n",
    "        done = self.current_time >= self.simulation_max_time\n",
    "\n",
    "        # Check if the cumulative reward is below a certain threshold\n",
    "        if reward < -1e5:\n",
    "            done = True\n",
    "\n",
    "        # Prepare the next state\n",
    "        next_state = self.get_state()\n",
    "\n",
    "        return reward, next_state, done\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to its initial state.\n",
    "        \"\"\"\n",
    "        self.cache = []  # Clear cache\n",
    "        self.current_cache_size = 0  # Reset cache size\n",
    "        self.transmitting_tasks = []  # Clear transmitting tasks\n",
    "        self.processing_tasks = []  # Clear processing tasks\n",
    "        self.current_time = 0  # Reset current time\n",
    "        #self.initialize_user_device_params()  # Reinitialize user device parameters\n",
    "        self.avg_delay = 0\n",
    "        self.avg_energy = 0\n",
    "        self.server_params = self.initialize_server_params()  # Reinitialize server parameters\n",
    "\n",
    "        initial_state = self.get_state()  # Get the initial state\n",
    "        return initial_state\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Get the current state of the environment.\n",
    "        \"\"\"\n",
    "        state = {\n",
    "            'total_bandwidth': self.total_bandwidth,\n",
    "            'total_computation': self.total_computation,\n",
    "            'total_delay': self.total_delay,  # Include total delay in the state\n",
    "            'total_energy': self.total_energy,  # Include total energy consumption in the state\n",
    "            'current_time': self.current_time,\n",
    "            'cache_size': self.current_cache_size,  # Include current cache size\n",
    "            'transmitting_tasks': self.transmitting_tasks,  # transmitting tasks\n",
    "            'processing_tasks': self.processing_tasks,  # processing tasks\n",
    "        }\n",
    "        return state\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"Total Bandwidth Used: {self.total_bandwidth}\")\n",
    "        print(f\"Total Computation Used: {self.total_computation}\")\n",
    "        print(f\"Current Cache Size: {self.current_cache_size}\")\n",
    "        print(f\"Number of Transmitting Tasks: {len(self.transmitting_tasks)}\")\n",
    "        print(f\"Number of Processing Tasks (Not Exist In Cache): {len(self.processing_tasks)}\")\n",
    "        print(f\"Total Delay: {self.total_delay}\")\n",
    "        print(f\"Total Energy Consumption: {self.total_energy}\")\n",
    "        print(f\"avg_delay: {self.avg_delay}\")\n",
    "        print(f\"avg_energy: {self.avg_energy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delay : [0.00025207] user_id : 0 task:  {'eta_m': 300.0, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00221842] R_m : [13528435.7801073]\n",
      "delay : [0.00104141] user_id : 1 task:  {'eta_m': 211.11111111111111, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00197494] R_m : [12532721.39379556]\n",
      "delay : [0.00050395] user_id : 2 task:  {'eta_m': 100.0, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.0011267] R_m : [21901226.39649636]\n",
      "delay : [0.00120217] user_id : 3 task:  {'eta_m': 255.55555555555554, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00134062] R_m : [18726328.53159148]\n",
      "delay : 0.00041448979591836733 user_id : 4 task:  {'eta_m': 100.0, 'T_max': 0.001, 'D_m': 1354} cache_hit : 1 energy : 0 R_m : [13717100.93401544]\n",
      "[]\n",
      "['T_max']\n",
      "['T_max']\n",
      "['T_max', 'T_max']\n",
      "['T_max', 'T_max']\n",
      "['T_max', 'T_max']\n",
      "[-400001.33281898]\n",
      "[-400001.33281898]\n",
      "Episode 1/10 - Avg Delay: [0.00068282], Avg Energy: [0.00133214], Avg Reward: 0\n",
      "delay : [0.00021522] user_id : 0 task:  {'eta_m': 166.66666666666666, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00181912] R_m : [13528435.7801073]\n",
      "delay : [0.00017712] user_id : 1 task:  {'eta_m': 100.0, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00195317] R_m : [12532721.39379556]\n",
      "delay : [0.00016161] user_id : 2 task:  {'eta_m': 144.44444444444446, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00112911] R_m : [21901226.39649636]\n",
      "delay : 6.908163265306123e-05 user_id : 3 task:  {'eta_m': 166.66666666666666, 'T_max': 0.001, 'D_m': 1354} cache_hit : 1 energy : 0 R_m : [18726328.53159148]\n",
      "delay : 4.144897959183673e-05 user_id : 4 task:  {'eta_m': 100.0, 'T_max': 0.001, 'D_m': 1354} cache_hit : 1 energy : 0 R_m : [13717100.93401544]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[-0.98041197]\n",
      "delay : [0.00090041] user_id : 0 task:  {'eta_m': 188.88888888888889, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.0020827] R_m : [13528435.7801073]\n",
      "delay : [0.00100252] user_id : 1 task:  {'eta_m': 211.11111111111111, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00225907] R_m : [12532721.39379556]\n",
      "delay : 0.00041448979591836733 user_id : 2 task:  {'eta_m': 100.0, 'T_max': 0.001, 'D_m': 1354} cache_hit : 1 energy : 0 R_m : [21901226.39649636]\n",
      "delay : 0.0005987074829931974 user_id : 3 task:  {'eta_m': 144.44444444444446, 'T_max': 0.001, 'D_m': 1354} cache_hit : 1 energy : 0 R_m : [18726328.53159148]\n",
      "delay : [0.00020062] user_id : 4 task:  {'eta_m': 122.22222222222223, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00184689] R_m : [15378064.97790546]\n",
      "[]\n",
      "['T_max']\n",
      "['T_max']\n",
      "['T_max']\n",
      "['T_max']\n",
      "['T_max']\n",
      "[-200001.23835436]\n",
      "[-200001.23835436]\n",
      "Episode 2/10 - Avg Delay: [0.00062335], Avg Energy: [0.00123773], Avg Reward: [-200002.21876633]\n",
      "delay : [0.00112076] user_id : 0 task:  {'eta_m': 233.33333333333331, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.01604594] R_m : [15189399.822935]\n",
      "delay : [0.00083227] user_id : 1 task:  {'eta_m': 166.66666666666666, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.01715788] R_m : [14193685.42288469]\n",
      "delay : [0.00069609] user_id : 2 task:  {'eta_m': 144.44444444444446, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.01034305] R_m : [23562190.44394]\n",
      "delay : [0.00090153] user_id : 3 task:  {'eta_m': 188.88888888888889, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.01195739] R_m : [20387292.57903173]\n",
      "delay : 0.0009671428571428572 user_id : 4 task:  {'eta_m': 233.33333333333331, 'T_max': 0.001, 'D_m': 1354} cache_hit : 1 energy : 0 R_m : [15378064.97790546]\n",
      "['T_max', 'E_max']\n",
      "['T_max', 'E_max', 'E_max']\n",
      "['T_max', 'E_max', 'E_max', 'E_max']\n",
      "['T_max', 'E_max', 'E_max', 'E_max', 'E_max']\n",
      "['T_max', 'E_max', 'E_max', 'E_max', 'E_max']\n",
      "['T_max', 'E_max', 'E_max', 'E_max', 'E_max']\n",
      "[-1000011.10175603]\n",
      "[-1000011.10175603]\n",
      "Episode 3/10 - Avg Delay: [0.00090356], Avg Energy: [0.01110085], Avg Reward: 0\n",
      "delay : [0.00025033] user_id : 0 task:  {'eta_m': 233.33333333333331, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.01603797] R_m : [15189399.822935]\n",
      "delay : [0.00019518] user_id : 1 task:  {'eta_m': 144.44444444444446, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.01714927] R_m : [14193685.42288469]\n",
      "delay : [0.0001419] user_id : 2 task:  {'eta_m': 122.22222222222223, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.01033521] R_m : [23562190.44394]\n",
      "delay : 5.987074829931974e-05 user_id : 3 task:  {'eta_m': 144.44444444444446, 'T_max': 0.001, 'D_m': 1354} cache_hit : 1 energy : 0 R_m : [20387292.57903173]\n",
      "delay : [0.00018374] user_id : 4 task:  {'eta_m': 188.88888888888889, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.01607617] R_m : [15378064.97790546]\n",
      "['E_max']\n",
      "['E_max', 'E_max']\n",
      "['E_max', 'E_max', 'E_max']\n",
      "['E_max', 'E_max', 'E_max']\n",
      "['E_max', 'E_max', 'E_max', 'E_max']\n",
      "['E_max', 'E_max', 'E_max', 'E_max']\n",
      "[-800011.91989135]\n",
      "[-800011.91989135]\n",
      "Episode 4/10 - Avg Delay: [0.0001662], Avg Energy: [0.01191973], Avg Reward: 0\n",
      "delay : [0.00107778] user_id : 0 task:  {'eta_m': 233.33333333333331, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.01635998] R_m : [15189399.822935]\n",
      "delay : [0.00127234] user_id : 1 task:  {'eta_m': 277.77777777777777, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.01755009] R_m : [14193685.42288469]\n",
      "delay : [0.00076363] user_id : 2 task:  {'eta_m': 166.66666666666666, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.01057104] R_m : [23562190.44394]\n",
      "delay : [0.0011492] user_id : 3 task:  {'eta_m': 255.55555555555554, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.01231235] R_m : [20387292.57903173]\n",
      "delay : 0.001059251700680272 user_id : 4 task:  {'eta_m': 255.55555555555554, 'T_max': 0.001, 'D_m': 1354} cache_hit : 1 energy : 0 R_m : [15378064.97790546]\n",
      "['T_max', 'E_max']\n",
      "['T_max', 'E_max', 'T_max', 'E_max']\n",
      "['T_max', 'E_max', 'T_max', 'E_max', 'E_max']\n",
      "['T_max', 'E_max', 'T_max', 'E_max', 'E_max', 'T_max', 'E_max']\n",
      "['T_max', 'E_max', 'T_max', 'E_max', 'E_max', 'T_max', 'E_max', 'T_max']\n",
      "['T_max', 'E_max', 'T_max', 'E_max', 'E_max', 'T_max', 'E_max', 'T_max']\n",
      "[-1600011.3597556]\n",
      "[-1600011.3597556]\n",
      "Episode 5/10 - Avg Delay: [0.00106444], Avg Energy: [0.01135869], Avg Reward: 0\n",
      "delay : [0.00020941] user_id : 0 task:  {'eta_m': 188.88888888888889, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00265167] R_m : [38214588.88512075]\n",
      "delay : [0.00107005] user_id : 1 task:  {'eta_m': 233.33333333333331, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00072876] R_m : [35227445.81777594]\n",
      "delay : [0.00134775] user_id : 2 task:  {'eta_m': 300.0, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00043348] R_m : [63332960.70351504]\n",
      "delay : [0.00095853] user_id : 3 task:  {'eta_m': 211.11111111111111, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00048675] R_m : [53808267.10882293]\n",
      "delay : [0.00067354] user_id : 4 task:  {'eta_m': 144.44444444444446, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00065083] R_m : [38780584.33976307]\n",
      "[]\n",
      "['T_max']\n",
      "['T_max', 'T_max']\n",
      "['T_max', 'T_max']\n",
      "['T_max', 'T_max']\n",
      "['T_max', 'T_max', 'Bandwith']\n",
      "[-1400000.99114936]\n",
      "[-1400000.99114936]\n",
      "Episode 6/10 - Avg Delay: [0.00085186], Avg Energy: [0.0009903], Avg Reward: 0\n",
      "delay : [0.00073751] user_id : 0 task:  {'eta_m': 255.55555555555554, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00033495] R_m : [43197480.98590799]\n",
      "delay : [0.00012287] user_id : 1 task:  {'eta_m': 122.22222222222223, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00070623] R_m : [35227445.81777594]\n",
      "delay : [0.00018257] user_id : 2 task:  {'eta_m': 233.33333333333331, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00041449] R_m : [63332960.70351504]\n",
      "delay : 0.0 user_id : 3 task:  {'eta_m': 255.55555555555554, 'T_max': 0.001, 'D_m': 1354} cache_hit : 1 energy : 0 R_m : [18726328.53159148]\n",
      "delay : [0.000104] user_id : 4 task:  {'eta_m': 100.0, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00064008] R_m : [38780584.33976307]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['Bandwith']\n",
      "[-1000000.41937894]\n",
      "[-1000000.41937894]\n",
      "Episode 7/10 - Avg Delay: [0.00022939], Avg Energy: [0.00041915], Avg Reward: 0\n",
      "delay : [0.00130653] user_id : 0 task:  {'eta_m': 300.0, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00108958] R_m : [38214588.88512075]\n",
      "delay : [0.00121538] user_id : 1 task:  {'eta_m': 277.77777777777777, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00110995] R_m : [35227445.81777594]\n",
      "delay : [0.00091586] user_id : 2 task:  {'eta_m': 211.11111111111111, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00070292] R_m : [63332960.70351504]\n",
      "delay : 0.001243469387755102 user_id : 3 task:  {'eta_m': 300.0, 'T_max': 0.001, 'D_m': 1354} cache_hit : 1 energy : 0 R_m : [53808267.10882293]\n",
      "delay : [0.00083524] user_id : 4 task:  {'eta_m': 188.88888888888889, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.0009124] R_m : [38780584.33976307]\n",
      "['T_max']\n",
      "['T_max', 'T_max']\n",
      "['T_max', 'T_max']\n",
      "['T_max', 'T_max', 'T_max']\n",
      "['T_max', 'T_max', 'T_max']\n",
      "['T_max', 'T_max', 'T_max', 'Bandwith']\n",
      "[-1600000.76407422]\n",
      "[-1600000.76407422]\n",
      "Episode 8/10 - Avg Delay: [0.0011033], Avg Energy: [0.00076297], Avg Reward: 0\n",
      "delay : [8.60913586e-05] user_id : 0 task:  {'eta_m': 100.0, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00077664] R_m : [38214588.88512075]\n",
      "delay : [0.00017916] user_id : 1 task:  {'eta_m': 277.77777777777777, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00108016] R_m : [35227445.81777594]\n",
      "delay : [0.00017336] user_id : 2 task:  {'eta_m': 300.0, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00080506] R_m : [63332960.70351504]\n",
      "delay : [0.0001096] user_id : 3 task:  {'eta_m': 166.66666666666666, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00068584] R_m : [53808267.10882293]\n",
      "delay : 6.908163265306123e-05 user_id : 4 task:  {'eta_m': 166.66666666666666, 'T_max': 0.001, 'D_m': 1354} cache_hit : 1 energy : 0 R_m : [38780584.33976307]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['Bandwith']\n",
      "[-1000000.66966318]\n",
      "[-1000000.66966318]\n",
      "Episode 9/10 - Avg Delay: [0.00012346], Avg Energy: [0.00066954], Avg Reward: 0\n",
      "delay : [0.0007121] user_id : 0 task:  {'eta_m': 144.44444444444446, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00201554] R_m : [13528435.7801073]\n",
      "delay : [0.00136004] user_id : 1 task:  {'eta_m': 300.0, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00609633] R_m : [40210337.79491611]\n",
      "delay : 0.0005987074829931974 user_id : 2 task:  {'eta_m': 144.44444444444446, 'T_max': 0.001, 'D_m': 1354} cache_hit : 1 energy : 0 R_m : [68315852.8458457]\n",
      "delay : [0.0009564] user_id : 3 task:  {'eta_m': 211.11111111111111, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00417058] R_m : [58791159.25112314]\n",
      "delay : [0.00086606] user_id : 4 task:  {'eta_m': 188.88888888888889, 'T_max': 0.001, 'D_m': 1354} cache_hit : 0 energy : [0.00558704] R_m : [43763476.45011117]\n",
      "[]\n",
      "['T_max', 'E_max']\n",
      "['T_max', 'E_max']\n",
      "['T_max', 'E_max', 'E_max']\n",
      "['T_max', 'E_max', 'E_max', 'E_max']\n",
      "['T_max', 'E_max', 'E_max', 'E_max']\n",
      "[-800003.57479757]\n",
      "[-800003.57479757]\n",
      "Episode 10/10 - Avg Delay: [0.00089866], Avg Energy: [0.0035739], Avg Reward: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\REZA\\anaconda3\\Lib\\site-packages\\numpy\\core\\function_base.py:182: RuntimeWarning: invalid value encountered in cast\n",
      "  return y.astype(dtype, copy=False)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, num_users, alpha=0.1, gamma=0.9, epsilon=0.1, max_steps_per_episode=100):\n",
    "        self.env = env\n",
    "        self.num_users = num_users\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = {}\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "\n",
    "    def get_state(self):\n",
    "        state = self.env.get_state()\n",
    "        return tuple(state.values())  # Convert the state dictionary to a tuple of its values\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state_key = tuple(state)\n",
    "        if state_key not in self.q_table:\n",
    "            self.q_table[state_key] = self.initialize_q_values()\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.generate_random_action()\n",
    "        else:\n",
    "            best_action_key = max(self.q_table[state_key], key=self.q_table[state_key].get)\n",
    "            return dict(best_action_key)\n",
    "\n",
    "    def initialize_q_values(self):\n",
    "        alpha_values = np.linspace(0.1, 1, 2)  # 0,1\n",
    "        b_values = np.linspace(0.1, 0.3, 2) # 0,1 not neccesary\n",
    "        p_values = np.linspace(self.env.P_max/10, self.env.P_max, 2) # 0,self.env.P_max\n",
    "        f_ue_values = np.linspace(self.env.F_max_ue/3, self.env.F_max_ue, 2) # 0,self.env.F_max_ue\n",
    "        f_es_values = np.linspace(self.env.F_max_es/100, self.env.F_max_es/10, 2) # 0,self.env.F_max_es\n",
    "\n",
    "        q_values = {}\n",
    "        for alpha in alpha_values:\n",
    "            for b in b_values:\n",
    "                for p in p_values:\n",
    "                    for f_ue in f_ue_values:\n",
    "                        for f_es in f_es_values:\n",
    "                            action = {\n",
    "                                'alpha_m': alpha,\n",
    "                                'b_m': b,\n",
    "                                'p_m': p,\n",
    "                                'f_ue_m': f_ue,\n",
    "                                'f_es_m': f_es\n",
    "                            }\n",
    "                            q_values[tuple(action.items())] = 0  # Initialize Q-value for each action\n",
    "\n",
    "        return q_values\n",
    "\n",
    "    def generate_random_action(self):\n",
    "        alpha = np.random.choice(np.linspace(0.1, 1, 2))\n",
    "        b = np.random.choice(np.linspace(0.1, 0.3, 2))\n",
    "        p = np.random.choice(np.linspace(self.env.P_max/10, self.env.P_max, 2))\n",
    "        f_ue = np.random.choice(np.linspace(self.env.F_max_ue/3, self.env.F_max_ue, 2))\n",
    "        f_es = np.random.choice(np.linspace(self.env.F_max_es/100, self.env.F_max_es/10, 2))\n",
    "\n",
    "        return {\n",
    "            'alpha_m': alpha,\n",
    "            'b_m': b,\n",
    "            'p_m': p,\n",
    "            'f_ue_m': f_ue,\n",
    "            'f_es_m': f_es\n",
    "        }\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        state_key = tuple(state)\n",
    "        action_key = tuple(action.items())\n",
    "        next_state_key = tuple(next_state)\n",
    "\n",
    "        if state_key not in self.q_table:\n",
    "            self.q_table[state_key] = self.initialize_q_values()\n",
    "\n",
    "        if next_state_key not in self.q_table:\n",
    "            self.q_table[next_state_key] = self.initialize_q_values()\n",
    "\n",
    "        current_q = self.q_table[state_key][action_key]\n",
    "        max_next_q = max(self.q_table[next_state_key].values())\n",
    "\n",
    "        self.q_table[state_key][action_key] = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        avg_delays = []\n",
    "        avg_energies = []\n",
    "        avg_rewards = []\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            total_delay = 0\n",
    "            total_energy = 0\n",
    "            total_reward = 0\n",
    "            num_steps = 0\n",
    "\n",
    "            for step in range(self.max_steps_per_episode):\n",
    "                actions = []\n",
    "                for _ in range(self.num_users):\n",
    "                    action = self.get_action(state)\n",
    "                    actions.append(action)\n",
    "\n",
    "                reward, next_state, done = self.env.step(actions)\n",
    "\n",
    "                for user_id, action in enumerate(actions):\n",
    "                    self.update_q_table(state, action, reward, next_state)\n",
    "                print(reward)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                num_steps += 1\n",
    "\n",
    "            avg_delay = self.env.avg_delay\n",
    "            avg_energy = self.env.avg_energy\n",
    "            avg_rewards.append(total_reward / num_steps if num_steps > 0 else 0)\n",
    "            avg_delays.append(avg_delay)\n",
    "            avg_energies.append(avg_energy)\n",
    "            print(reward)\n",
    "            print(f'Episode {episode + 1}/{num_episodes} - Avg Delay: {avg_delay}, Avg Energy: {avg_energy}, Avg Reward: {total_reward / num_steps if num_steps > 0 else 0}')\n",
    "\n",
    "        #self.plot_results(avg_delays, avg_energies, avg_rewards)\n",
    "\n",
    "    def plot_results(self, avg_delays, avg_energies, avg_rewards):\n",
    "        episodes = np.arange(1, len(avg_delays) + 1)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(episodes, avg_delays, label='Avg Delay')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Average Delay')\n",
    "        plt.title('Average Delay per Episode')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(episodes, avg_energies, label='Avg Energy')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Average Energy')\n",
    "        plt.title('Average Energy per Episode')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(episodes, avg_rewards, label='Avg Reward')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Average Reward')\n",
    "        plt.title('Average Reward per Episode')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def show_q_table(self):\n",
    "        print()\n",
    "        for state, actions in self.q_table.items():\n",
    "            print(f\"State: {state}\")\n",
    "            for action, q_value in actions.items():\n",
    "                print(f\"  Action: {action}, Q-Value: {q_value}\")\n",
    "\n",
    "# Assuming you have your EdgeComputingEnvironment defined as per your code\n",
    "env = EdgeComputingEnvironment()\n",
    "\n",
    "# Define the number of users/devices\n",
    "num_users = env.M\n",
    "\n",
    "# Initialize the Q-learning agent\n",
    "agent = QLearningAgent(env, num_users)\n",
    "\n",
    "# Train the agent\n",
    "num_episodes = 10  # Adjust the number of episodes as needed\n",
    "agent.train(num_episodes)\n",
    "\n",
    "# Show the Q-table\n",
    "#agent.show_q_table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, num_users, alpha=0.1, gamma=0.9, epsilon=0.1, epsilon_min=0.01, epsilon_decay=0.995, max_steps_per_episode=2):\n",
    "        self.env = env\n",
    "        self.num_users = num_users\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.q_table = {}\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "\n",
    "    @staticmethod\n",
    "    def flatten_dict(d, parent_key='', sep='_'):\n",
    "        items = []\n",
    "        for k, v in d.items():\n",
    "            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "            if isinstance(v, dict):\n",
    "                items.extend(QLearningAgent.flatten_dict(v, new_key, sep=sep).items())\n",
    "            else:\n",
    "                items.append((new_key, v))\n",
    "        return dict(items)\n",
    "\n",
    "    def get_state(self, state_info):\n",
    "        state = self.env.get_state(state_info)\n",
    "        return state\n",
    "\n",
    "    def get_action(self, state, device_info):\n",
    "        def make_hashable(d):\n",
    "            if isinstance(d, dict):\n",
    "                return tuple(sorted((k, make_hashable(v)) for k, v in d.items()))\n",
    "            if isinstance(d, list):\n",
    "                return tuple(make_hashable(e) for e in d)\n",
    "            return d\n",
    "\n",
    "        flattened_state = self.flatten_dict(state)\n",
    "        flattened_device_info = self.flatten_dict(device_info)\n",
    "        combined_state = {**flattened_state, **flattened_device_info}\n",
    "        state_key = make_hashable(combined_state)\n",
    "\n",
    "        if state_key not in self.q_table:\n",
    "            self.q_table[state_key] = self.initialize_q_values()\n",
    "\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            action = self.generate_random_action()\n",
    "        else:\n",
    "            action_key = max(self.q_table[state_key], key=self.q_table[state_key].get)\n",
    "            action = dict(action_key)\n",
    "        return action\n",
    "\n",
    "    def initialize_q_values(self):\n",
    "        alpha_values = np.linspace(0.1, 1, 3)\n",
    "        b_values = np.linspace(0.1, 0.3, 3)\n",
    "        p_values = np.linspace(self.env.P_max / 10, self.env.P_max, 3)\n",
    "        f_ue_values = np.linspace(self.env.F_max_ue / 3, self.env.F_max_ue, 3)\n",
    "        f_es_values = np.linspace(self.env.F_max_es / 100, self.env.F_max_es / 10, 3)\n",
    "\n",
    "        q_values = {}\n",
    "        for alpha in alpha_values:\n",
    "            for b in b_values:\n",
    "                for p in p_values:\n",
    "                    for f_ue in f_ue_values:\n",
    "                        for f_es in f_es_values:\n",
    "                            action = {\n",
    "                                'alpha_m': alpha,\n",
    "                                'b_m': b,\n",
    "                                'p_m': p,\n",
    "                                'f_ue_m': f_ue,\n",
    "                                'f_es_m': f_es\n",
    "                            }\n",
    "                            q_values[tuple(action.items())] = 0\n",
    "\n",
    "        return q_values\n",
    "\n",
    "    def generate_random_action(self):\n",
    "        alpha = np.random.choice(np.linspace(0.1, 1, 3))\n",
    "        b = np.random.choice(np.linspace(0.1, 0.3, 3))\n",
    "        p = np.random.choice(np.linspace(self.env.P_max / 10, self.env.P_max, 3))\n",
    "        f_ue = np.random.choice(np.linspace(self.env.F_max_ue / 3, self.env.F_max_ue, 3))\n",
    "        f_es = np.random.choice(np.linspace(self.env.F_max_es / 100, self.env.F_max_es / 10, 3))\n",
    "\n",
    "        action = {\n",
    "            'alpha_m': alpha,\n",
    "            'b_m': b,\n",
    "            'p_m': p,\n",
    "            'f_ue_m': f_ue,\n",
    "            'f_es_m': f_es\n",
    "        }\n",
    "        return action\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        state_key = tuple(sorted(self.flatten_dict(state).items())) if isinstance(state, dict) else state\n",
    "        action_key = tuple(action.items())\n",
    "        next_state_key = tuple(sorted(self.flatten_dict(next_state).items())) if isinstance(next_state, dict) else next_state\n",
    "\n",
    "        if state_key not in self.q_table:\n",
    "            self.q_table[state_key] = self.initialize_q_values()\n",
    "\n",
    "        if next_state_key not in self.q_table:\n",
    "            self.q_table[next_state_key] = self.initialize_q_values()\n",
    "\n",
    "        current_q = self.q_table[state_key][action_key]\n",
    "        max_next_q = max(self.q_table[next_state_key].values())\n",
    "\n",
    "        self.q_table[state_key][action_key] = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)\n",
    "\n",
    "    def make_hashable(self, d):\n",
    "        if isinstance(d, dict):\n",
    "            return tuple(sorted((k, self.make_hashable(v)) for k, v in d.items()))\n",
    "        if isinstance(d, list):\n",
    "            return tuple(self.make_hashable(e) for e in d)\n",
    "        if isinstance(d, np.ndarray):\n",
    "            return tuple(d.tolist())\n",
    "        return d\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        avg_delays = []\n",
    "        avg_energies = []\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state_info = self.env.reset()\n",
    "            state = self.get_state(state_info)\n",
    "            total_delay = 0\n",
    "            total_energy = 0\n",
    "            num_steps = 0\n",
    "\n",
    "            for step in range(self.max_steps_per_episode):\n",
    "                actions = []\n",
    "                for user_id in range(self.num_users):\n",
    "                    device_info = state_info['state_info'][user_id]\n",
    "                    action = self.get_action(state_info, device_info)\n",
    "                    actions.append(action)\n",
    "\n",
    "                rewards, next_state_info, done = self.env.step(actions)\n",
    "                next_state = self.get_state(next_state_info)\n",
    "\n",
    "                state_info_list = state_info['state_info']\n",
    "                next_state_info_list = next_state_info['state_info']\n",
    "\n",
    "                for user_id in range(len(state_info_list)):\n",
    "                    device_info = state_info_list[user_id]\n",
    "                    next_device_info = next_state_info_list[user_id]\n",
    "                    action = actions[user_id]\n",
    "                    reward = rewards[user_id]\n",
    "\n",
    "                    if isinstance(device_info, dict) and isinstance(next_device_info, dict):\n",
    "                        combined_state = {**self.flatten_dict(state), **self.flatten_dict(device_info)}\n",
    "                        combined_next_state = {**self.flatten_dict(next_state), **self.flatten_dict(next_device_info)}\n",
    "                        self.update_q_table(combined_state, action, reward, combined_next_state)\n",
    "                        total_delay += next_device_info.get('delay', 0)\n",
    "                        total_energy += next_device_info.get('energy', 0)\n",
    "                    else:\n",
    "                        raise TypeError(\"device_info and next_device_info must be dictionaries\")\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                num_steps += 1\n",
    "            avg_delay = total_delay / num_steps if num_steps > 0 else 0\n",
    "            avg_energy = total_energy / num_steps if num_steps > 0 else 0\n",
    "            avg_delays.append(avg_delay)\n",
    "            avg_energies.append(avg_energy)\n",
    "\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "            print(f'Episode {episode + 1}/{num_episodes} - Avg Delay: {avg_delay}, Avg Energy: {avg_energy}')\n",
    "            print(\"-------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        self.plot_results(avg_delays, avg_energies)\n",
    "\n",
    "    def plot_results(self, avg_delays, avg_energies):\n",
    "        episodes = np.arange(1, len(avg_delays) + 1)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(episodes, avg_delays, label='Avg Delay')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Average Delay')\n",
    "        plt.title('Average Delay per Episode')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(episodes, avg_energies, label='Avg Energy')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Average Energy')\n",
    "        plt.title('Average Energy per Episode')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def save_q_table(self, filename):\n",
    "        q_table_str_keys = {\n",
    "            str(state): {str(action): q_value.tolist() if isinstance(q_value, np.ndarray) else q_value for action, q_value in actions.items()}\n",
    "            for state, actions in self.q_table.items()\n",
    "        }\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(q_table_str_keys, file, indent=4)\n",
    "        print(f\"Q-table saved to {filename}\")\n",
    "\n",
    "# Assuming you have your EdgeComputingEnvironment defined as per your code\n",
    "env = EdgeComputingEnvironment()\n",
    "\n",
    "# Define the number of users/devices\n",
    "num_users = env.M\n",
    "\n",
    "# Initialize the Q-learning agent\n",
    "agent = QLearningAgent(env, num_users)\n",
    "\n",
    "# Train the agent\n",
    "num_episodes = 2  # Adjust the number of episodes as needed\n",
    "agent.train(num_episodes)\n",
    "\n",
    "# Show the Q-table\n",
    "agent.save_q_table('q_table.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
