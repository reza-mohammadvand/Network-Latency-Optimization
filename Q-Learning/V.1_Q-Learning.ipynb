{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":321,"output_embedded_package_id":"17IYv1FidZ2Qj5O7T4juH2WxNiOYDVD3d"},"executionInfo":{"elapsed":1793,"status":"error","timestamp":1719139949080,"user":{"displayName":"Reza Mohammadvand","userId":"01755219696102616805"},"user_tz":-210},"id":"2hl8Q8XjfDxp","outputId":"8526f7fa-3895-494d-b75a-b2650396509e"},"outputs":[],"source":["import numpy as np\n","\n","# Initialize Q-table with appropriate dimensions\n","def initialize_q_table(state_space, action_space):\n","    return np.zeros(state_space + [action_space])\n","\n","# Choose action using epsilon-greedy strategy\n","def choose_action(state, q_table, epsilon, action_space):\n","    if np.random.rand() < epsilon:\n","        return np.random.randint(action_space)  # Exploration\n","    else:\n","        return np.argmax(q_table[tuple(state)])  # Exploitation\n","\n","# Update Q-table using Q-learning algorithm\n","def update_q_table(q_table, state, action, reward, next_state, alpha, gamma):\n","    best_next_action = np.argmax(q_table[tuple(next_state)])\n","    td_target = reward + gamma * q_table[tuple(next_state)][best_next_action]\n","    td_error = td_target - q_table[tuple(state)][action]\n","    q_table[tuple(state)][action] += alpha * td_error\n","\n","# Decay epsilon value\n","def decay_epsilon(epsilon, epsilon_decay, epsilon_min):\n","    return max(epsilon_min, epsilon * epsilon_decay)\n","\n","# Generate random actions for each user\n","def generate_random_actions(num_users, env):\n","    alpha_values = np.linspace(0, 1, 10)\n","    b_values = np.linspace(1, env.B, 10)\n","    p_values = np.linspace(1, env.P_max, 10)\n","    f_ue_values = np.linspace(1, env.F_max_ue, 10)\n","    f_es_values = np.linspace(1, env.F_max_es, 10)\n","\n","    actions = []\n","    for _ in range(num_users):\n","        action = {\n","            'alpha_m': np.random.choice(alpha_values),\n","            'b_m': np.random.choice(b_values),\n","            'p_m': np.random.choice(p_values),\n","            'f_ue_m': np.random.choice(f_ue_values),\n","            'f_es_m': np.random.choice(f_es_values)\n","        }\n","        actions.append(action)\n","    return actions\n","\n","# Run the simulation with Q-learning\n","def run_simulation(env, q_table, num_episodes, alpha, gamma, initial_epsilon, epsilon_decay, epsilon_min, action_space):\n","    epsilon = initial_epsilon\n","    for episode in range(num_episodes):\n","        state = env.reset()  # Assuming `reset` initializes the environment and returns the initial state\n","        done = False\n","        while not done:\n","            actions = generate_random_actions(env.M, env)\n","            action = choose_action(state, q_table, epsilon, action_space)\n","            reward, next_state, done = env.step(actions)  # Assuming `step` executes the action and returns next_state, reward, done, and penalties\n","            update_q_table(q_table, state, action, reward, next_state, alpha, gamma)\n","            state = next_state\n","        epsilon = decay_epsilon(epsilon, epsilon_decay, epsilon_min)\n","        print(f'Episode {episode + 1}/{num_episodes}, Epsilon: {epsilon:.4f}')\n","\n","# Define a render function for visualization\n","def render(env):\n","    print(\"Rendering the environment state:\")\n","    print(f\"Current State: {env.get_state()}\")\n","    print(f\"Total Users: {env.M}\")\n","    print(f\"Bandwidth: {env.B}\")\n","    print(f\"Maximum Power: {env.P_max}\")\n","    print(f\"Maximum UE Frequency: {env.F_max_ue}\")\n","    print(f\"Maximum ES Frequency: {env.F_max_es}\")\n","\n","# Test simulation\n","def test_simulation():\n","    # Initialize the environment\n","    env = EdgeComputingEnvironment()\n","\n","    # Set state and action spaces based on the environment\n","    state_space = [len(env.get_state())]  # Adjust state space based on the returned state\n","    action_space = 10 ** 5  # Number of possible actions\n","\n","    # Initialize Q-Learning parameters\n","    alpha = 0.1\n","    gamma = 0.9\n","    initial_epsilon = 1.0\n","    epsilon_decay = 0.99\n","    epsilon_min = 0.01\n","    num_episodes = 50\n","\n","    # Initialize Q-table\n","    q_table = initialize_q_table(state_space, action_space)\n","\n","    # Run simulation\n","    run_simulation(env, q_table, num_episodes, alpha, gamma, initial_epsilon, epsilon_decay, epsilon_min, action_space)\n","\n","    # Render the final state of the environment\n","    render(env)\n","\n","# Run the test simulation\n","test_simulation()\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOBlXNq90xXSVolUuZagPdR","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
