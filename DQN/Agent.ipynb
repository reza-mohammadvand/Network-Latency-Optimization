{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double Deep Q-Network (DDQN) Agent\n",
    "class DDQNAgent:\n",
    "    def __init__(self, env, alpha=0.001, gamma=0.95, epsilon=1.0, batch_size=64, max_steps_per_episode=1, update_target_freq=25):\n",
    "        self.env = env  # Environment for the agent\n",
    "        self.num_users = env.M  # Number of users/devices in the environment\n",
    "        self.num_tasks = env.M # Number of task\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor for future rewards\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.batch_size = batch_size\n",
    "        self.max_steps_per_episode = max_steps_per_episode  # Maximum steps per episode\n",
    "        self.save_interval = 1000  # save model \n",
    "        self.num_candidates = 1000  # Define the number of candidate actions to sample\n",
    "        self.state_dim = 5  # state dimensions\n",
    "        self.action_dim = 6  # action dimensions\n",
    "        self.all_action = self.sample_all_action()  # Actions Space = 200000\n",
    "        self.is_training = True\n",
    "        self.update_target_freq = update_target_freq  # Frequency to update the target network\n",
    "\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_network()  # Initialize the target network\n",
    "\n",
    "    def build_model(self):\n",
    "        # Define inputs for state and action\n",
    "        input_state = layers.Input(shape=(self.state_dim,))\n",
    "        input_action = layers.Input(shape=(self.action_dim,))\n",
    "        \n",
    "        # Concatenate state and action inputs\n",
    "        concat = layers.Concatenate()([input_state, input_action])\n",
    "        \n",
    "        # Pass through dense layers\n",
    "        dense1 = layers.Dense(64, activation='relu')(concat)\n",
    "        dense2 = layers.Dense(64, activation='relu')(dense1)\n",
    "        dense3 = layers.Dense(64, activation='relu')(dense2)\n",
    "        output = layers.Dense(1, activation='linear')(dense3)  # Output the predicted reward\n",
    "        \n",
    "        # Create and compile the model\n",
    "        model = models.Model(inputs=[input_state, input_action], outputs=output)\n",
    "        model.compile(loss='mse', optimizer=optimizers.Adam(learning_rate=self.alpha, clipvalue=1.0))\n",
    "        return model\n",
    "\n",
    "    def update_target_network(self):\n",
    "        tau = 0.125\n",
    "        weights = self.model.get_weights() # give weights\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i] * tau + target_weights[i] * (1 - tau) # update weights\n",
    "        \n",
    "        self.target_model.set_weights(target_weights) # add weights that updated\n",
    "        # self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def update_epsilon(self, episode, total_episodes):\n",
    "        \"\"\"\n",
    "        Adjust the epsilon value based on the episode index.\n",
    "        The decay rate changes based on different stages of the training process.\n",
    "        \"\"\"\n",
    "        # Define three different decay rates for three stages of training\n",
    "        if episode < 0.25 * total_episodes:  # Stage 1 (0–25% of episodes)\n",
    "            decay_rate = 0.9992  # Slow decay for broad exploration\n",
    "        elif 0.25 * total_episodes <= episode < 0.75 * total_episodes:  # Stage 2 (25–75%)\n",
    "            decay_rate = 0.9980  # Moderate decay\n",
    "        else:  # Stage 3 (75–100%)\n",
    "            decay_rate = 0.9930  # Faster decay to focus on exploitation\n",
    "\n",
    "        # Update epsilon value\n",
    "        min_epsilon = 0.01  # Set a minimum epsilon value\n",
    "        self.epsilon = max(min_epsilon, self.epsilon * decay_rate)\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon and self.is_training:\n",
    "            # Choose a random action from the predefined set (exploration)\n",
    "            action = self.sample_random_action()\n",
    "\n",
    "        else:\n",
    "            if self.is_training:\n",
    "                # Sample a subset of actions from all actions for training\n",
    "                candidate_actions = np.array([self.sample_random_action() for _ in range(self.num_candidates)])\n",
    "\n",
    "            else:\n",
    "                # Use all actions during testing\n",
    "                candidate_actions = self.all_action\n",
    "\n",
    "            # Prepare the state array for batch prediction\n",
    "            state_batch = np.tile(state, (candidate_actions.shape[0], 1))\n",
    "\n",
    "            # Normalize actions\n",
    "            normalized_action = np.array([self.normalize_action(action) for action in candidate_actions])\n",
    "\n",
    "            # Predict the reward for each candidate action\n",
    "            predicted_rewards = self.model.predict([state_batch, normalized_action])\n",
    "\n",
    "            # Select the action with the highest predicted reward\n",
    "            best_action_index = np.argmax(predicted_rewards)\n",
    "            action = candidate_actions[best_action_index]\n",
    "\n",
    "        return action\n",
    "\n",
    "    def sample_random_action(self):\n",
    "        # Generate random values for the action parameters\n",
    "        # Generate a random value from a discrete set of 10 values ​​(for discretization). Discretization reduces the operation space, improving performance with minimal impact on results.\n",
    "        alpha = np.round(np.random.choice(np.linspace(0, 1, 10)),3)\n",
    "        b = np.round(np.random.choice(np.linspace(0.005, (2 / self.num_users), 10)),3)  # The minimum is set to 0.01 to avoid zero values, and the upper limit ensures fair distribution across users.\n",
    "        p = np.round(np.random.choice(np.linspace(0.005, self.env.P_max, 10)),3)  # The lower limit is 0.01 to avoid zero values, which are impractical and can cause calculation errors.\n",
    "        f_ue = np.round(np.random.choice(np.linspace(1e6, self.env.F_max_ue, 10))) # The minimum is set to 1 MHz to avoid unrealistic values, ensuring reasonable usage.\n",
    "        f_es = np.round(np.random.choice(np.linspace(1e6, ((2 * self.env.F_max_es) / self.num_users), 10))) # The lower limit prevents unrealistic values, while the upper limit ensures fair resource distribution.\n",
    "        cache_hit = np.random.choice([0, 1])  # Generate a random cache hit value, either 0 (no cache hit) or 1 (cache hit).\n",
    "        \n",
    "        return np.array([alpha, b, p, f_ue, f_es, cache_hit])\n",
    "    \n",
    "    def sample_all_action(self):\n",
    "        # Generate all action\n",
    "        alpha = np.round(np.linspace(0, 1, 10),3) \n",
    "        b = np.round(np.linspace(0.005, (2 / self.num_users), 10),3) \n",
    "        p = np.round(np.linspace(0.005, self.env.P_max, 10),3) \n",
    "        f_ue = np.round(np.linspace(1e6, self.env.F_max_ue, 10))\n",
    "        f_es = np.round(np.linspace(1e6, ((2 * self.env.F_max_es) / self.num_users), 10))\n",
    "        cache_hit = [0, 1]\n",
    "\n",
    "        samples = []\n",
    "\n",
    "        for i in alpha:\n",
    "            for j in b:\n",
    "                for k in p:\n",
    "                    for l in f_ue:\n",
    "                        for m in f_es:\n",
    "                            for n in cache_hit:\n",
    "                                samples.append(np.array([i, j, k, l, m, n]))\n",
    "\n",
    "        samples = np.array(samples)\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Prioritized sampling based on rewards\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        # Vectorized state, action, next_state extraction and reshaping\n",
    "        states = np.array([sample[0] for sample in minibatch])\n",
    "        actions = np.array([sample[1] for sample in minibatch])\n",
    "        rewards = np.array([sample[2] for sample in minibatch])\n",
    "        next_states = np.array([sample[3] for sample in minibatch])\n",
    "        dones = np.array([sample[4] for sample in minibatch])\n",
    "\n",
    "        states = states.reshape(self.batch_size, -1)\n",
    "        actions = actions.reshape(self.batch_size, -1)\n",
    "        next_states = next_states.reshape(self.batch_size, -1)\n",
    "\n",
    "        # Sample a subset of actions from all actions for training\n",
    "        candidate_actions = np.array([self.sample_random_action() for _ in range(self.num_candidates)])\n",
    "        normalized_candidate_actions = np.array([self.normalize_action(action) for action in candidate_actions])\n",
    "\n",
    "        # Expand dimensions to match state_batch and normalized_candidate_actions\n",
    "        state_batch = np.repeat(next_states, self.num_candidates, axis=0)\n",
    "        action_batch = np.tile(normalized_candidate_actions, (self.batch_size, 1))\n",
    "\n",
    "        # Predict Q-values ​​for each next_state-action pair\n",
    "        predicted_q_values = self.model.predict([state_batch, action_batch])\n",
    "        max_q_values = np.max(predicted_q_values.reshape(self.batch_size, self.num_candidates), axis=1)\n",
    "\n",
    "        # Compute target for the Q-learning update\n",
    "        targets = rewards + self.gamma * max_q_values * (1 - dones)\n",
    "\n",
    "        # Predict current Q-values ​​and update them\n",
    "        target_f = self.model.predict([states, actions])\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            target_f[i, 0] = targets[i]\n",
    "        \n",
    "        # Train the model on the updated Q-values\n",
    "        self.model.fit([states, actions], target_f, epochs=1, verbose=0)\n",
    "\n",
    "    def normalize_state(self, state):\n",
    "        # Normalize states between values ​​0 and 1\n",
    "        normalized_state = np.array([\n",
    "                state[0] / 300,  # Normalizing task complexity (eta_m)\n",
    "                state[1],  # Bandwidth (assuming it's already normalized to [0, 1])\n",
    "                state[2] / self.env.F_max_es,  # Normalizing computation\n",
    "                state[3], # cache hit\n",
    "                state[4] / (self.env.area_size/2) # distance\n",
    "        ])\n",
    "        return normalized_state\n",
    "    \n",
    "    def normalize_action(self, action):\n",
    "        # Normalize actions between values ​​0 and 1\n",
    "        normalized_action = np.array([\n",
    "                action[0] ,  # alpha\n",
    "                action[1] / (2 / self.num_users),  # bandwidth\n",
    "                action[2] / self.env.P_max,  # power transmission\n",
    "                action[3] / self.env.F_max_ue,  # Normalizing computation of user\n",
    "                action[4] / ((2 * self.env.F_max_es) / self.num_users),  # Normalizing computation of server\n",
    "                action[5] # cache hit\n",
    "        ])\n",
    "        return normalized_action\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(f'{name}model.weights.h5')\n",
    "        self.target_model.load_weights(f'{name}target_model.weights.h5')\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(f'{name}model.weights.h5')\n",
    "        self.target_model.save_weights(f'{name}target_model.weights.h5')\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        # Lists to store average delay and energy values for each episode\n",
    "        avg_delays = []\n",
    "        avg_energies = []\n",
    "        avg_rewards = []\n",
    "\n",
    "        self.env.is_training = True\n",
    "        self.is_training = True\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "\n",
    "            self.env.reset()\n",
    "\n",
    "            # Initialize total delay and energy for this episode\n",
    "            total_delay = 0\n",
    "            total_energy = 0\n",
    "            total_reward = 0\n",
    "\n",
    "            done_step = False\n",
    "\n",
    "            # Initialize the number of tasks in this episode\n",
    "            num_all_tasks = 0\n",
    "            actual_steps = 0\n",
    "\n",
    "            for step in range(self.max_steps_per_episode):\n",
    "\n",
    "                if done_step:\n",
    "                    break\n",
    "\n",
    "                self.env.create_task()\n",
    "\n",
    "                for item in range(self.num_users):\n",
    "\n",
    "                    state = np.array(self.env.get_state())\n",
    "\n",
    "                    normalized_state = np.array(self.normalize_state(state))\n",
    "\n",
    "                    action = self.act(normalized_state)\n",
    "                    \n",
    "                    # Execute the actions in the environment\n",
    "                    reward, next_state_info, done = self.env.step(action)\n",
    "\n",
    "                    # Extract delay and energy values from the next device information\n",
    "                    delay = next_state_info.pop(0)\n",
    "                    energy = next_state_info.pop(0)\n",
    "\n",
    "                    next_state = np.array(next_state_info)\n",
    "\n",
    "                    normalized_action = self.normalize_action(action)\n",
    "                    normal_nextstate = self.normalize_state(next_state)\n",
    "\n",
    "                    self.remember(normalized_state, normalized_action, reward, normal_nextstate, done)\n",
    "\n",
    "                    # Accumulate the total delay and energy for the episode\n",
    "                    total_delay += delay\n",
    "                    total_energy += energy\n",
    "                    total_reward += reward\n",
    "\n",
    "                    num_all_tasks += 1\n",
    "\n",
    "                    if done:\n",
    "                        done_step = True\n",
    "                        # Exit the loop if the episode is done\n",
    "                        break\n",
    "\n",
    "                self.env.increase_time()\n",
    "\n",
    "                actual_steps += 1\n",
    "                \n",
    "            self.env.render()\n",
    "\n",
    "            # Calculate and store average delay and energy for the episode\n",
    "            avg_delay = (total_delay / num_all_tasks) * 1000  # Convert to milliseconds\n",
    "            avg_energy = total_energy / num_all_tasks\n",
    "            avg_reward = total_reward / num_all_tasks\n",
    "            avg_delays.append(avg_delay)\n",
    "            avg_energies.append(avg_energy)\n",
    "            avg_rewards.append(avg_reward)\n",
    "\n",
    "            # Update epsilon for the epsilon-greedy strategy\n",
    "            self.update_epsilon(episode, num_episodes)\n",
    "\n",
    "            self.replay()\n",
    "\n",
    "            # save model\n",
    "            if (episode + 1) % self.save_interval == 0:\n",
    "                self.save(f'Model{episode+1}')\n",
    "\n",
    "            if (episode + 1) % self.update_target_freq == 0:\n",
    "                self.update_target_network()\n",
    "            \n",
    "            # Print the episode's results\n",
    "            # print(f\"Train : Episode {episode + 1}/{num_episodes} - Steps Count {actual_steps} - Tasks Count {num_all_tasks} - Avg Delay: {avg_delay}, Avg Energy: {avg_energy}, Avg Reward: {avg_reward}\")\n",
    "            # print(\"-\" * 100)\n",
    "\n",
    "        # Optionally plot the results\n",
    "        # self.plot_results(avg_delays, avg_energies, avg_rewards)\n",
    "\n",
    "    def test(self, num_test_steps=1):\n",
    "        # Initialize total delay, alpha values, and rewards for the test\n",
    "        total_delay = 0\n",
    "        total_alpha = 0\n",
    "        total_energy = 0\n",
    "        total_reward = 0\n",
    "        \n",
    "        done_step = False\n",
    "\n",
    "        # Counter for actual steps\n",
    "        num_all_tasks = 0\n",
    "        actual_steps = 0\n",
    "        \n",
    "        # Set epsilon to 0 for testing (no exploration)\n",
    "        self.epsilon = 0\n",
    "\n",
    "        self.env.is_training = False\n",
    "        self.is_training = False\n",
    "        \n",
    "        # Reset the environment \n",
    "        self.env.reset()\n",
    "\n",
    "        for step in range(num_test_steps):\n",
    "\n",
    "            if done_step:\n",
    "                    break\n",
    "\n",
    "            self.env.create_task()\n",
    "\n",
    "            for item in range(self.num_users):\n",
    "\n",
    "                state = np.array(self.env.get_state())\n",
    "\n",
    "                normalized_state = self.normalize_state(state)\n",
    "\n",
    "                action = self.act(normalized_state)\n",
    "                \n",
    "                # Execute the actions in the environment\n",
    "                reward, next_state_info, done = self.env.step(action)\n",
    "\n",
    "                # Extract delay and energy values from the next device information\n",
    "                delay = next_state_info.pop(0)\n",
    "                energy = next_state_info.pop(0)\n",
    "\n",
    "                # Accumulate delay and alpha values\n",
    "                total_delay += delay\n",
    "                total_energy += energy\n",
    "                total_alpha += 1 - action[0]\n",
    "\n",
    "                num_all_tasks += 1\n",
    "\n",
    "                if done:\n",
    "                    done_step = True\n",
    "                    # Exit the loop if the episode is done\n",
    "                    break\n",
    "\n",
    "            self.env.increase_time()\n",
    "\n",
    "            # Increment the actual steps counter\n",
    "            actual_steps += 1\n",
    "\n",
    "        self.env.render()\n",
    "\n",
    "        # Calculate and return the average delay and alpha for the test\n",
    "        total_delay = total_delay * 1000  # Convert to milliseconds\n",
    "        avg_energy = (total_energy / num_all_tasks) * 1000  # Convert to milliJoule \n",
    "        avg_alpha = total_alpha / num_all_tasks\n",
    "        avg_reward = total_reward / num_all_tasks\n",
    "\n",
    "\n",
    "        # Print the episode's results\n",
    "        # print(f\"Test : Steps Count {actual_steps} - Tasks Count {num_all_tasks} - Delay: {total_delay}, Avg Energy: {avg_energy}, Avg Reward: {avg_reward}, Avg Alpha: {avg_alpha}\")\n",
    "        # print(\"-\" * 100)\n",
    "\n",
    "        # if num_all_tasks == 15:\n",
    "        #     return avg_delay, avg_alpha\n",
    "\n",
    "        return total_delay, avg_alpha\n",
    "\n",
    "\n",
    "    def plot_results(self, avg_delays, avg_energies, avg_rewards):\n",
    "        episodes = np.arange(1, len(avg_delays) + 1)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(episodes, avg_delays, label='Avg Delay')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Average Delay')\n",
    "        plt.title('Average Delay per Episode')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(episodes, avg_energies, label='Avg Energy')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Average Energy')\n",
    "        plt.title('Average Energy per Episode')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(episodes, avg_rewards, label='Avg Reward')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Average Reward')\n",
    "        plt.title('Average Reward per Episode')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
