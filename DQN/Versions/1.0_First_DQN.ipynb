{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.special import erfcinv\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import copy\n",
        "\n",
        "class EdgeComputingEnvironment:\n",
        "    def __init__(self, M=15, area_size=100, D_m=1354, eta_m_range=(100, 300), F_max_ue=1.5, P_max=23, B=5e6, T_max=10e-3, F_max_es=30, S_max_es=60, epsilon=10e-7, E_max=3, theta=10**-26, L=8, phi=0.02e-3, N0=-174, f_es_dev=0.02, f_ue_dev=0.02):\n",
        "\n",
        "        self.M = M  \n",
        "        self.area_size = area_size  \n",
        "        self.D_m = D_m  \n",
        "        self.eta_m_range = eta_m_range  \n",
        "        self.F_max_ue = F_max_ue * 1e9  \n",
        "        self.P_max = 10 ** (P_max / 10)  \n",
        "        self.B = B  \n",
        "        self.T_max = T_max  \n",
        "        self.F_max_es = F_max_es * 1e9  \n",
        "        self.S_max_es = S_max_es * 1e3  \n",
        "        self.epsilon = epsilon  \n",
        "        self.E_max = E_max * 1e-3 \n",
        "        self.theta = theta\n",
        "        self.L = L \n",
        "        self.phi = phi \n",
        "        self.R_min = 1e6  \n",
        "        self.N0 = N0 \n",
        "        self.N0 = 10 ** ((N0 - 30) / 10)\n",
        "        self.PL_d = lambda d: 10 ** ((10 ** ((-35.3 - (37.6 * np.log10(d))) / 10)) / 10)\n",
        "        self.f_es_dev = f_es_dev\n",
        "        self.f_ue_dev = f_ue_dev \n",
        "        self.penalty = 100\n",
        "        self.treshhold = -200\n",
        "\n",
        "        self.user_device_params = [] \n",
        "        self.initialize_user_device_params()  \n",
        "\n",
        "        self.server_params = self.initialize_server_params() \n",
        "\n",
        "        self.cache = [] \n",
        "        self.current_cache_size = 0  \n",
        "        self.transmitting_tasks = [] \n",
        "        self.processing_tasks = [] \n",
        "        self.current_time = 0\n",
        "\n",
        "        self.total_bandwidth = 0\n",
        "        self.total_computation = 0 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def initialize_user_device_params(self):\n",
        "\n",
        "        for device_id in range(self.M):\n",
        "            d = np.random.uniform(0, self.area_size / 2)  \n",
        "            g_m = np.array([self.PL_d(d)]) \n",
        "            h_bar = np.random.randn(1, self.L) + 1j * np.random.randn(1, self.L)\n",
        "\n",
        "            self.user_device_params.append({\n",
        "                'device_id': device_id, \n",
        "                'd': d,\n",
        "                'g_m': g_m,\n",
        "                'h_bar': h_bar,\n",
        "            })\n",
        "\n",
        "    def initialize_server_params(self):\n",
        "\n",
        "        return {\n",
        "            'S_max_es': self.S_max_es \n",
        "        }\n",
        "\n",
        "    def calculate_gamma_m(self, b_m, p_m, user_id):\n",
        "\n",
        "        h_m = np.sqrt(self.user_device_params[user_id]['g_m'])[:, None] * self.user_device_params[user_id]['h_bar']  \n",
        "        gamma_m = (p_m * np.linalg.norm(h_m, axis=1) ** 2) / (b_m * self.B * self.N0)  \n",
        "\n",
        "        return gamma_m\n",
        "\n",
        "    def calculate_uplink_rate(self, b_m, p_m, user_id):\n",
        "\n",
        "        gamma_m = self.calculate_gamma_m(b_m, p_m, user_id) \n",
        "        V_m = 1 - (1 / (1 + gamma_m) ** 2)  \n",
        "        Q_inv = np.sqrt(2) * erfcinv(2 * self.epsilon) \n",
        "        R_m = (self.B / np.log(2)) * ((b_m * np.log(1 + gamma_m)) - ((np.sqrt((b_m * V_m) / (self.phi * self.B))) * Q_inv)) \n",
        "\n",
        "        return R_m\n",
        "\n",
        "    def calculate_delay(self, alpha_m, cache_hit, b_m, p_m, D_m, f_ue_m, f_es_m, f_ue_est, f_es_est, eta_m, user_id):\n",
        "\n",
        "        actual_f_ue_m = f_ue_m - f_ue_est  \n",
        "        actual_f_es_m = f_es_m - f_es_est\n",
        "\n",
        "        if cache_hit == 1:\n",
        "            T_es = (eta_m * D_m) / actual_f_es_m \n",
        "            T_e2e = T_es\n",
        "\n",
        "        else:\n",
        "            T_ue = (alpha_m * eta_m * D_m) / actual_f_ue_m \n",
        "            R_m = self.calculate_uplink_rate(b_m, p_m, user_id) \n",
        "            T_tr = (D_m * 8) / R_m \n",
        "            T_es = ((1 - alpha_m) * eta_m * D_m) / actual_f_es_m  \n",
        "            T_e2e = T_ue + T_tr + T_es \n",
        "        return T_e2e\n",
        "\n",
        "    def calculate_transmission_delay(self, b_m, p_m, D_m, user_id):\n",
        "\n",
        "        R_m = self.calculate_uplink_rate(b_m, p_m, user_id)  \n",
        "        T_co =  (D_m * 8) / R_m \n",
        "\n",
        "        return T_co\n",
        "\n",
        "    def calculate_server_processing_delay(self, alpha_m, cache_hit, D_m, f_es_m, f_es_est, eta_m):\n",
        "\n",
        "        if cache_hit == 0:\n",
        "            T_es = ((1 - alpha_m) * eta_m * D_m) / (f_es_m - f_es_est)  \n",
        "\n",
        "        else:\n",
        "            T_es = (eta_m * D_m) / (f_es_m - f_es_est)\n",
        "\n",
        "        return T_es\n",
        "\n",
        "    def calculate_energy_consumption(self, s_m, b_m, alpha_m, p_m, D_m, f_ue_m, f_ue_est, eta_m, user_id):\n",
        "        R_m = self.calculate_uplink_rate(b_m, p_m, user_id) \n",
        "\n",
        "        actual_f_ue_m = f_ue_m - f_ue_est  \n",
        "        E_ue = alpha_m * (self.theta / 2 * eta_m * D_m * (actual_f_ue_m ** 2))  \n",
        "        E_tx = ((1 - alpha_m) * (D_m * 8) * p_m) / R_m \n",
        "\n",
        "        if s_m == 1:  \n",
        "            E_total = 0 \n",
        "        else:\n",
        "            E_total = E_ue + E_tx\n",
        "\n",
        "        return E_total\n",
        "\n",
        "    def manage_cache(self, task_info, task_delay):\n",
        "\n",
        "        if task_delay == 0:\n",
        "            for task in self.cache:\n",
        "                if task_info == task[0]:\n",
        "                    return True\n",
        "            return False\n",
        "\n",
        "        task_size = task_info['D_m'] * 8\n",
        "        Server_Max_Capacity = self.server_params['S_max_es']  \n",
        "\n",
        "        if (task_size + self.current_cache_size) <= Server_Max_Capacity:\n",
        "            self.cache.append((task_info, task_delay))  \n",
        "            self.current_cache_size += task_size\n",
        "            return True\n",
        "        else:\n",
        "            sorted_cache = sorted(self.cache, key=lambda x: x[1], reverse=True) \n",
        "\n",
        "            while (task_size + self.current_cache_size) > Server_Max_Capacity:\n",
        "                if not sorted_cache:\n",
        "                    break  \n",
        "                last_task = sorted_cache.pop() \n",
        "                self.cache.remove(last_task)\n",
        "                self.current_cache_size -= last_task[0]['D_m'] * 8 \n",
        "\n",
        "            self.cache.append((task_info, task_delay))\n",
        "            self.current_cache_size += task_size \n",
        "\n",
        "            return True\n",
        "\n",
        "    def step(self, actions, tasks, users_id):\n",
        "        task_rewards = []\n",
        "        state_info = []  \n",
        "        done = False\n",
        "\n",
        "        for action, task, user_id in zip(actions, tasks, users_id):\n",
        "            cache_hit = 1 if self.manage_cache(task, 0) else 0\n",
        "\n",
        "            alpha_m = action[0]\n",
        "            b_m = action[1]\n",
        "            p_m = action[2]\n",
        "            F_max_ue = action[3]\n",
        "            F_max_es = action[4]\n",
        "\n",
        "            f_ue_est = F_max_ue * self.f_ue_dev  \n",
        "            f_es_est = F_max_es * self.f_es_dev  \n",
        "\n",
        "            delay = self.calculate_delay(\n",
        "                alpha_m, cache_hit, b_m, p_m,\n",
        "                task['D_m'], F_max_ue, F_max_es, f_ue_est,\n",
        "                f_es_est, task['eta_m'], user_id\n",
        "            )\n",
        "\n",
        "            energy = self.calculate_energy_consumption(\n",
        "                cache_hit, b_m, alpha_m, p_m, task['D_m'], F_max_ue,\n",
        "                f_es_est, task['eta_m'], user_id\n",
        "            )\n",
        "\n",
        "            R_m = self.calculate_uplink_rate(b_m, p_m, user_id)\n",
        "            print(R_m)\n",
        "\n",
        "            if cache_hit == 0:\n",
        "                transmission_end_time = self.current_time + self.calculate_transmission_delay(b_m, p_m,task['D_m'], user_id)\n",
        "                processing_end_time = transmission_end_time + self.calculate_server_processing_delay(alpha_m, cache_hit, task['D_m'], F_max_es, f_es_est, task['eta_m'])\n",
        "\n",
        "                self.transmitting_tasks.append((self.current_time, transmission_end_time, b_m))\n",
        "                self.processing_tasks.append((transmission_end_time, processing_end_time, F_max_es))\n",
        "\n",
        "                self.manage_cache(task, delay)\n",
        "            else:\n",
        "                processing_end_time = self.current_time + self.calculate_server_processing_delay(alpha_m, cache_hit, task['D_m'], F_max_es, f_es_est, task['eta_m'])\n",
        "                self.processing_tasks.append((self.current_time, processing_end_time, F_max_es))\n",
        "\n",
        "            self.total_bandwidth = sum(b for _, end_time, b in self.transmitting_tasks if end_time > self.current_time)\n",
        "            self.total_computation = sum(f for _, end_time, f in self.processing_tasks if end_time > self.current_time)\n",
        "\n",
        "            self.transmitting_tasks = [(start_time, end_time, b) for start_time, end_time, b in self.transmitting_tasks if end_time > self.current_time]\n",
        "            self.processing_tasks = [(start_time, end_time, f) for start_time, end_time, f in self.processing_tasks if end_time > self.current_time]\n",
        "\n",
        "            task_reward  = -delay - energy\n",
        "\n",
        "            if delay > task['T_max']:\n",
        "                task_reward -= self.penalty\n",
        "            if energy > self.E_max:\n",
        "                task_reward -= self.penalty\n",
        "            if R_m < self.R_min:\n",
        "                task_reward -= self.penalty\n",
        "            if self.total_bandwidth > 1:\n",
        "                task_reward -= self.penalty\n",
        "            if self.total_computation > self.F_max_es:\n",
        "                task_reward -= self.penalty\n",
        "\n",
        "            if task_reward < self.treshhold:\n",
        "                done = True\n",
        "\n",
        "            task_rewards.append(task_reward)\n",
        "\n",
        "            state_info.append({\n",
        "                'cache_size': self.current_cache_size,\n",
        "                'device_id': user_id,\n",
        "                'task': task,  \n",
        "                'delay': delay,\n",
        "                'energy': energy,\n",
        "                'Occupied bandwidth': self.total_bandwidth,\n",
        "                'Occupied computation': self.total_computation\n",
        "            })\n",
        "\n",
        "        self.current_time += 0.010 \n",
        "\n",
        "        next_state = state_info\n",
        "        return task_rewards, next_state, done\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the environment to its initial state.\n",
        "        \"\"\"\n",
        "        self.cache = [] \n",
        "        self.current_cache_size = 0  \n",
        "        self.transmitting_tasks = [] \n",
        "        self.processing_tasks = [] \n",
        "        self.current_time = 0  \n",
        "        self.initialize_user_device_params()\n",
        "        self.total_bandwidth = 0  \n",
        "        self.total_computation = 0  \n",
        "        self.server_params = self.initialize_server_params() \n",
        "\n",
        "        device_state_info = {user_id: {\n",
        "            'Occupied bandwidth':  self.total_bandwidth,\n",
        "            'Occupied computation': self.total_computation,\n",
        "            'cache_size': self.current_cache_size ,\n",
        "            'device_id' : None,\n",
        "            'task' : {\n",
        "            'eta_m': 0,\n",
        "            'T_max': 0,\n",
        "            'D_m': 0\n",
        "        }\n",
        "        } for user_id in range(self.M)}\n",
        "\n",
        "        return device_state_info\n",
        "\n",
        "    def render(self):\n",
        "        print(f\"Total Bandwidth Used: {self.total_bandwidth}\")\n",
        "        print(f\"Total Computation Used: {self.total_computation}\")\n",
        "        print(f\"Current Cache Size: {self.current_cache_size}\")\n",
        "        print(f\"Number of Transmitting Tasks: {len(self.transmitting_tasks)}\")\n",
        "        print(f\"Number of Processing Tasks (Not Exist In Cache): {len(self.processing_tasks)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\REZA\\.conda\\envs\\tf\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'EdgeComputingEnvironment' object has no attribute 'action_space'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 232\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39m# Train the agent\u001b[39;00m\n\u001b[0;32m    231\u001b[0m num_episodes \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m  \u001b[39m# Adjust the number of episodes as needed\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m agent\u001b[39m.\u001b[39;49mtrain(num_episodes)\n",
            "Cell \u001b[1;32mIn[7], line 141\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[1;34m(self, num_episodes)\u001b[0m\n\u001b[0;32m    138\u001b[0m states \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(states_choose_actions)\n\u001b[0;32m    140\u001b[0m \u001b[39m# Select actions for each device with a task using the DQN model\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m actions \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(state\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39mfor\u001b[39;00m state \u001b[39min\u001b[39;00m states]\n\u001b[0;32m    143\u001b[0m \u001b[39m# Execute the actions in the environment\u001b[39;00m\n\u001b[0;32m    144\u001b[0m rewards, next_state_info, done \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(actions, tasks, users_id)\n",
            "Cell \u001b[1;32mIn[7], line 141\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    138\u001b[0m states \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(states_choose_actions)\n\u001b[0;32m    140\u001b[0m \u001b[39m# Select actions for each device with a task using the DQN model\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m actions \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(state\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)) \u001b[39mfor\u001b[39;00m state \u001b[39min\u001b[39;00m states]\n\u001b[0;32m    143\u001b[0m \u001b[39m# Execute the actions in the environment\u001b[39;00m\n\u001b[0;32m    144\u001b[0m rewards, next_state_info, done \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(actions, tasks, users_id)\n",
            "Cell \u001b[1;32mIn[7], line 43\u001b[0m, in \u001b[0;36mDQNAgent.act\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mact\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[0;32m     41\u001b[0m     \u001b[39m# Choose an action based on epsilon-greedy policy\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand() \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon:\n\u001b[1;32m---> 43\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49maction_space\u001b[39m.\u001b[39msample()  \u001b[39m# Sample random action within action_space\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     act_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpredict(state)  \u001b[39m# Predict action values using DQN model\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mclip(act_values[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'EdgeComputingEnvironment' object has no attribute 'action_space'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, optimizers\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "# Deep Q-Network (DQN) Agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, env, num_users, alpha=0.001, gamma=0.85, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, batch_size=64, max_steps_per_episode=20):\n",
        "        self.env = env  # Environment for the agent\n",
        "        self.num_users = num_users  # Number of users/devices in the environment\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor for future rewards\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.epsilon_decay = epsilon_decay  # Decay rate for epsilon\n",
        "        self.epsilon_min = epsilon_min  # Minimum value for epsilon\n",
        "        self.batch_size = batch_size  # Batch size for experience replay\n",
        "        self.max_steps_per_episode = max_steps_per_episode  # Maximum steps per episode\n",
        "\n",
        "        # Define the action space and state space dimensions\n",
        "        self.state_dim = 5  # As each state has 5 parameters\n",
        "        self.action_dim = 5  # As each action has 5 parameters\n",
        "\n",
        "        self.memory = deque(maxlen=2000)  # Experience replay memory\n",
        "        self.model = self.build_model()  # Neural network model\n",
        "\n",
        "    def build_model(self):\n",
        "        # Build a neural network to approximate the Q-values\n",
        "        model = models.Sequential()\n",
        "        model.add(layers.Dense(64, input_dim=self.state_dim, activation='relu'))\n",
        "        model.add(layers.Dense(64, activation='relu'))\n",
        "        model.add(layers.Dense(self.action_dim, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=optimizers.Adam(lr=self.alpha))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        # Store experience in memory\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        # Choose an action based on epsilon-greedy policy\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return self.env.action_space.sample()  # Sample random action within action_space\n",
        "        act_values = self.model.predict(state)  # Predict action values using DQN model\n",
        "        return np.clip(act_values[0], self.env.action_space.low, self.env.action_space.high)  # Clip actions to action_space bounds\n",
        "\n",
        "    def replay(self):\n",
        "        # Experience replay to train the network\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def load(self, name):\n",
        "        # Load model weights\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        # Save model weights\n",
        "        self.model.save_weights(name)\n",
        "\n",
        "    def create_task(self):\n",
        "        # Create a new task with random parameters within specified ranges\n",
        "        eta_m = np.round(np.random.choice(np.linspace(self.env.eta_m_range[0], self.env.eta_m_range[1], 100)))\n",
        "        T_max_task = 10e-3  # Static according to article\n",
        "        task_info = {\n",
        "            'eta_m': eta_m,\n",
        "            'T_max': T_max_task,\n",
        "            'D_m': 1354  # Task data size\n",
        "        }\n",
        "        return task_info\n",
        "\n",
        "    def train(self, num_episodes):\n",
        "        # Lists to store average delay and energy values for each episode\n",
        "        avg_delays = []\n",
        "        avg_energies = []\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            # Initialize device state information for all users at the beginning of each episode\n",
        "            device_state_info = self.env.reset()\n",
        "\n",
        "            # Initialize total delay and energy for this episode\n",
        "            total_delay = 0\n",
        "            total_energy = 0\n",
        "            total_reward = 0\n",
        "\n",
        "            # Initialize the number of steps taken in this episode\n",
        "            num_steps = 0\n",
        "\n",
        "            for step in range(self.max_steps_per_episode):\n",
        "                # Initialize lists for actions, tasks, and user IDs\n",
        "                actions = []\n",
        "                tasks = []\n",
        "                users_id = []\n",
        "\n",
        "                # Generate a random number of tasks and assign them to devices\n",
        "                num_tasks = np.random.randint(self.num_users / 2, self.num_users + 1)\n",
        "\n",
        "                # Randomly distribute tasks to users\n",
        "                task_distribution = np.random.choice(range(self.num_users), num_tasks, replace=True)\n",
        "\n",
        "                # Initialize a list to store state representations for action selection\n",
        "                states_choose_actions = []\n",
        "\n",
        "                for user_id in task_distribution:\n",
        "                    # Create a new task\n",
        "                    task = self.create_task()\n",
        "                    # Append the task to the tasks list\n",
        "                    tasks.append(task)\n",
        "                    # Append the user ID to the users_id list\n",
        "                    users_id.append(user_id)\n",
        "\n",
        "                    # Update device state information with the new task\n",
        "                    device_state_info[user_id]['task'] = task\n",
        "                    device_state_info[user_id]['device_id'] = user_id\n",
        "\n",
        "                    # Create the state representation for action selection\n",
        "                    state_choose_actions = [\n",
        "                        device_state_info[user_id]['cache_size'],\n",
        "                        user_id,\n",
        "                        device_state_info[user_id]['task']['eta_m'],\n",
        "                        device_state_info[user_id]['Occupied bandwidth'],\n",
        "                        device_state_info[user_id]['Occupied computation']\n",
        "                    ]\n",
        "\n",
        "                    # Append the state representation to the list\n",
        "                    states_choose_actions.append(state_choose_actions)\n",
        "\n",
        "                # Convert states to numpy array for DQN\n",
        "                states = np.array(states_choose_actions)\n",
        "\n",
        "                # Select actions for each device with a task using the DQN model\n",
        "                actions = [self.act(state.reshape(1, -1)) for state in states]\n",
        "\n",
        "                # Execute the actions in the environment\n",
        "                rewards, next_state_info, done = self.env.step(actions, tasks, users_id)\n",
        "                print(rewards)\n",
        "                print(next_state_info)\n",
        "\n",
        "                # Accumulate the total reward for the episode\n",
        "                total_reward += sum(rewards)\n",
        "                print(total_reward)\n",
        "                # List of state representations used for action selection\n",
        "                state_info_list = states_choose_actions\n",
        "                print(state_info_list)\n",
        "\n",
        "                counter_Users = 0\n",
        "                # Update Q-values and device state information for each device\n",
        "                for user_id in task_distribution:\n",
        "                    # Get the current device information\n",
        "                    device_info = state_info_list[counter_Users]\n",
        "                    # Get the next device information\n",
        "                    next_device_info = next_state_info[counter_Users]\n",
        "                    # Get the action taken by the user\n",
        "                    action = actions[counter_Users]\n",
        "                    # Get the reward received by the user\n",
        "                    reward = rewards[counter_Users]\n",
        "\n",
        "                    print(device_info)\n",
        "                    print(next_device_info)\n",
        "                    print(action)\n",
        "                    print(reward) \n",
        "\n",
        "                    # Extract delay and energy values from the next device information\n",
        "                    delay = next_device_info.pop('delay', 0)\n",
        "                    energy = next_device_info.pop('energy', 0)\n",
        "                    print(next_device_info)\n",
        "\n",
        "                    # Convert next state to numpy array for DQN\n",
        "                    next_state = np.array([\n",
        "                        next_device_info.get('cache_size', 0),\n",
        "                        user_id,\n",
        "                        next_device_info.get('task', {}).get('eta_m', 0),\n",
        "                        next_device_info.get('Occupied bandwidth', 0),\n",
        "                        next_device_info.get('Occupied computation', 0)\n",
        "                    ]).reshape(1, -1)\n",
        "                    print(\"ff\",next_state)\n",
        "                    # Store experience in replay memory\n",
        "                    self.remember(device_info, action, reward, next_state, done)\n",
        "\n",
        "                    # Accumulate the total delay and energy for the episode\n",
        "                    total_delay += delay\n",
        "                    total_energy += energy\n",
        "\n",
        "                    # Update device state information with the combined next state\n",
        "                    device_state_info[user_id].update(next_device_info)\n",
        "                    # Increment the counter for the number of users\n",
        "                    counter_Users += 1\n",
        "\n",
        "                # Increment the number of steps taken in the episode\n",
        "                num_steps += 1\n",
        "                if done:\n",
        "                    # Exit the loop if the episode is done\n",
        "                    break\n",
        "\n",
        "            # Calculate and store average delay and energy for the episode\n",
        "            avg_delay = (total_delay / (num_steps * len(state_info_list))) * 1000  # Convert to milliseconds\n",
        "            avg_energy = (total_energy / (num_steps * len(state_info_list)))\n",
        "            avg_reward = total_reward / num_steps\n",
        "            avg_delays.append(avg_delay)\n",
        "            avg_energies.append(avg_energy)\n",
        "\n",
        "            # Print the episode's results\n",
        "            print(f\"Train : Episode {episode + 1}/{num_episodes} - Steps Count {num_steps} - Avg Delay: {avg_delay}, Avg Energy: {avg_energy}, Avg Reward: {avg_reward}\")\n",
        "            print(\"-\" * 100)\n",
        "            # Update epsilon for the epsilon-greedy strategy\n",
        "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "            # Perform experience replay\n",
        "            self.replay()\n",
        "\n",
        "\n",
        "# Assuming you have your EdgeComputingEnvironment defined as per your code\n",
        "env = EdgeComputingEnvironment()\n",
        "\n",
        "# Define the number of users/devices\n",
        "num_users = env.M\n",
        "\n",
        "# Initialize the DQN agent\n",
        "agent = DQNAgent(env, num_users)\n",
        "\n",
        "# Train the agent\n",
        "num_episodes = 5  # Adjust the number of episodes as needed\n",
        "agent.train(num_episodes)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
