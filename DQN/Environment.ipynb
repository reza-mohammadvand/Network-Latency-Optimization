{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy.special import erfcinv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge Computing Environment\n",
    "class EdgeComputingEnvironment:\n",
    "    def __init__(self, M=15, area_size=100, D_m=1354, eta_m_range=(100, 300), F_max_ue=1.5, P_max=23, B=5, T_max=10, F_max_es=30, S_max_es=60, epsilon=10**-7, E_max=3, theta=10**-26, L=8, phi=0.02, N0=-174, f_es_dev=0.02, f_ue_dev=0.02):\n",
    "        \"\"\"\n",
    "        Initialize the edge computing environment with given parameters.\n",
    "        \"\"\"\n",
    "        self.M = M  # Number of users\n",
    "        self.area_size = area_size  # Size of the area in which users are distributed\n",
    "        self.D_m = D_m  # Task data size\n",
    "        self.eta_m_range = eta_m_range  # Range of Task complexity\n",
    "        self.F_max_ue = F_max_ue * 10**9  # Maximum frequency of user equipment\n",
    "        self.P_max = 10 ** ((P_max - 30) / 10)  # Convert maximum transmission power from dBm to Watts\n",
    "        self.B = B * 10**6  # Bandwidth\n",
    "        self.T_max = T_max * 10**-3  # Maximum tolerable delay\n",
    "        self.F_max_es = F_max_es * 10**9  # Maximum frequency of edge server\n",
    "        self.S_max_es = S_max_es * 10**3  # Maximum cache size of edge server\n",
    "        self.epsilon = epsilon  # Error tolerance for rate calculation\n",
    "        self.E_max = E_max * 10**-3  # Maximum energy consumption\n",
    "        self.theta = theta  # Energy coefficient\n",
    "        self.L = L  # Number of antennas\n",
    "        self.phi = phi * 10**-3  # Transmission time interval\n",
    "        self.R_min = 10**6  # Minimum data rate\n",
    "        self.N0 = N0  # Noise power in dBm\n",
    "        self.N0 = 10 ** ((N0 - 30) / 10)  # Convert noise power from dBm/Hz to Watts/Hz\n",
    "        self.PL_d = lambda d: -35.3 - (37.6 * np.log10(d))  # Path loss model\n",
    "        self.f_es_dev = f_es_dev  #The deviation between the estimated value and the actual value of the processing rate of the ES\n",
    "        self.f_ue_dev = f_ue_dev  #The deviation between the estimated value and the actual value of the processing rate of the UE\n",
    "        self.is_training = True\n",
    "        self.tasks = []\n",
    "        self.current_task = {}\n",
    "        self.Task_processed = 0\n",
    "        self.penalty = 10\n",
    "\n",
    "        self.user_device_params = []  # List to store parameters for each user device\n",
    "        self.initialize_user_device_params()  # Initialize user device parameters\n",
    "\n",
    "        self.cache = []  # Cache to store tasks\n",
    "        self.current_cache_size = 0  # Current size of the cache\n",
    "        self.transmitting_tasks = []  # List to store transmitting tasks\n",
    "        self.processing_tasks = []  # List to store processing tasks\n",
    "        self.current_time = 0.0  # Current simulation time\n",
    "\n",
    "        # Initialize bandwidth and computation attributes\n",
    "        self.total_bandwidth = 0 # Initialize total bandwidth\n",
    "        self.total_computation = 0 # Initialize total computation\n",
    "\n",
    "    def initialize_user_device_params(self):\n",
    "        \"\"\"\n",
    "        Initialize parameters for each user device.\n",
    "        Randomly generates user-specific parameters such as path loss.\n",
    "        \"\"\"\n",
    "        for device_id in range(self.M):\n",
    "            d = np.random.uniform(1, self.area_size / 2)  # Distance to server\n",
    "            PL_dB = self.PL_d(d)\n",
    "            g_m = 10 ** (PL_dB / 10)  # Convert path loss from dB to linear scale\n",
    "            h_bar = np.random.randn(1, self.L) + 1j * np.random.randn(1, self.L)  # Channel gain\n",
    "\n",
    "            self.user_device_params.append({\n",
    "                'device_id': device_id,  # Assign a unique ID to each device\n",
    "                'd': d,\n",
    "                'g_m': g_m,\n",
    "                'h_bar': h_bar,\n",
    "            })\n",
    "\n",
    "    def create_task(self):\n",
    "        \"\"\"\n",
    "        create task randomly\n",
    "        \"\"\"\n",
    "        task_distribution = np.random.choice(range(self.M), self.M, replace=True)\n",
    "        self.tasks = []\n",
    "        for user_id in task_distribution:\n",
    "            eta_m =  np.round(np.random.choice(np.linspace(self.eta_m_range[0], self.eta_m_range[1], 50)))  # Task complexity\n",
    "            T_max_task = self.T_max  # Static\n",
    "            order = np.round(random.uniform(1,5))\n",
    "\n",
    "            task_info = {\n",
    "            'eta_m': eta_m,\n",
    "            'T_max': T_max_task,\n",
    "            'D_m': 1354,  # Task data size\n",
    "            'user_id' : user_id,\n",
    "            'order' : order\n",
    "            }\n",
    "\n",
    "            self.tasks.append(task_info)\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        get state and task.\n",
    "        get task and check that task is in the cache or not.\n",
    "        \"\"\"\n",
    "        self.tasks = sorted(self.tasks, key=lambda x: x['order'], reverse=True)\n",
    "\n",
    "        task = self.tasks.pop()\n",
    "\n",
    "        self.current_task = task\n",
    "\n",
    "        cache_hit = 1 if any(task == task_info[0] for task_info in self.cache) else 0  # check the cache\n",
    "\n",
    "        state = [\n",
    "            task['eta_m'],\n",
    "            self.total_bandwidth,\n",
    "            self.total_computation,\n",
    "            cache_hit,\n",
    "            self.user_device_params[task['user_id']]['d']\n",
    "        ]\n",
    "\n",
    "        return state\n",
    "\n",
    "    def calculate_gamma_m(self, b_m, p_m, user_id):\n",
    "        \"\"\"\n",
    "        Calculate the signal-to-noise ratio (SNR) for a given user.\n",
    "\n",
    "        Parameters:\n",
    "        - b_m (float): Bandwidth allocation\n",
    "        - p_m (float): Transmission power\n",
    "        - user_id (int): ID of the user\n",
    "\n",
    "        Returns:\n",
    "        - gamma_m (array): SNR values for the user's communication channel\n",
    "        \"\"\"\n",
    "        h_m = np.sqrt(self.user_device_params[user_id]['g_m']) * self.user_device_params[user_id]['h_bar']  # Channel gain\n",
    "        gamma_m = (p_m * np.linalg.norm(h_m, axis=1) ** 2) / (b_m * self.B * self.N0)  # SNR\n",
    "        \n",
    "        return gamma_m\n",
    "\n",
    "    def calculate_uplink_rate(self, b_m, p_m, user_id):\n",
    "        \"\"\"\n",
    "        Calculate the uplink data rate for a given user.\n",
    "\n",
    "        Parameters:\n",
    "        - b_m (float): Bandwidth allocation\n",
    "        - p_m (float): Transmission power\n",
    "        - user_id (int): ID of the user\n",
    "\n",
    "        Returns:\n",
    "        - R_m (float): Uplink data rate in bits/second\n",
    "        \"\"\"\n",
    "        gamma_m = self.calculate_gamma_m(b_m, p_m, user_id)  # Calculate the SINR for the m-th user\n",
    "        V_m = 1 - (1 / (1 + gamma_m) ** 2)  # Intermediate variable for rate calculation\n",
    "        Q_inv = np.sqrt(2) * erfcinv(2 * self.epsilon)  # Calculate the inverse of the Q-function for the outage probability\n",
    "        R_m = (self.B / np.log(2)) * ((b_m * np.log(1 + gamma_m)) - ((np.sqrt((b_m * V_m) / (self.phi * self.B))) * Q_inv))  # Uplink data rate\n",
    "\n",
    "        return R_m\n",
    "\n",
    "    def calculate_delay(self, alpha_m, cache_hit, R_m, D_m, f_ue_m, f_es_m, f_ue_est, f_es_est, eta_m):\n",
    "        \"\"\"\n",
    "        Calculate the end-to-end delay for a given task.\n",
    "\n",
    "        Parameters:\n",
    "        - alpha_m (float): Offloading decision\n",
    "        - cache_hit (int): Split factor (0 or 1)\n",
    "        - R_m (float): Uplink data rate in bits/second\n",
    "        - D_m (int): Data size\n",
    "        - f_ue_m (float): Computation capability of the user device\n",
    "        - f_es_m (float): Computation capability of the edge server\n",
    "        - f_ue_est (float): Estimation error for the user device's computation capability\n",
    "        - f_es_est (float): Estimation error for the edge server's computation capability\n",
    "        - eta_m (float): Computational intensity\n",
    "\n",
    "        Returns:\n",
    "        - T_e2e (float): End-to-end delay in seconds\n",
    "        \"\"\"\n",
    "        actual_f_ue_m = f_ue_m - f_ue_est  # Actual processing rate of the user device\n",
    "\n",
    "        if cache_hit == 1:\n",
    "            T_es = self.calculate_server_processing_delay(alpha_m, cache_hit, D_m, f_es_m, f_es_est, eta_m)  # Only edge server processing delay\n",
    "            T_e2e = T_es\n",
    "\n",
    "        else:\n",
    "            T_ue = (alpha_m * eta_m * D_m) / actual_f_ue_m  # User device processing delay\n",
    "            T_tr = self.calculate_transmission_delay(alpha_m, R_m, D_m)  # Transmission delay\n",
    "            T_es = self.calculate_server_processing_delay(alpha_m, cache_hit, D_m, f_es_m, f_es_est, eta_m)  # Edge server processing delay\n",
    "            T_e2e = T_ue + T_tr + T_es  # Total end-to-end delay\n",
    "\n",
    "        return T_e2e\n",
    "\n",
    "    def calculate_transmission_delay(self, alpha_m, R_m, D_m):\n",
    "        \"\"\"\n",
    "        Calculate the transmission delay for a given task.\n",
    "\n",
    "        Parameters:\n",
    "        - alpha_m (float): Offloading decision\n",
    "        - R_m (float): Uplink data rate in bits/second\n",
    "        - D_m (int): Data size\n",
    "        - user_id (int): ID of the user\n",
    "\n",
    "        Returns:\n",
    "        - T_co (float): Transmission delay in seconds\n",
    "        \"\"\"\n",
    "        T_co =  ((1 - alpha_m) * (D_m * 8)) / R_m   # Transmission delay calculation based on task size and uplink rate\n",
    "\n",
    "        return T_co\n",
    "\n",
    "    def calculate_server_processing_delay(self, alpha_m, cache_hit, D_m, f_es_m, f_es_est, eta_m):\n",
    "        \"\"\"\n",
    "        Calculate the processing delay at the edge server for a given task.\n",
    "\n",
    "        Parameters:\n",
    "        - alpha_m (float): Offloading decision\n",
    "        - D_m (int): Data size\n",
    "        - cache_hit (0,1): 1 = Exist in cache and 0 not exist in cache\n",
    "        - f_es_m (float): Computation capability of the edge server\n",
    "        - f_es_est (float): Estimation error for the edge server's computation capability\n",
    "        - eta_m (float): Computational intensity\n",
    "\n",
    "        Returns:\n",
    "        - T_es (float): Processing delay at the edge server in seconds\n",
    "        \"\"\"\n",
    "\n",
    "        actual_f_es_m = f_es_m - f_es_est  # Actual processing rate of the Edge server\n",
    "\n",
    "        if cache_hit == 0:\n",
    "            T_es = ((1 - alpha_m) * eta_m * D_m) / actual_f_es_m  # Processing delay at the edge server\n",
    "\n",
    "        else:\n",
    "            T_es = (eta_m * D_m) / actual_f_es_m\n",
    "        return T_es\n",
    "\n",
    "    def calculate_energy_consumption(self, s_m, R_m, alpha_m, p_m, D_m, f_ue_m, f_ue_est, eta_m):\n",
    "        \"\"\"\n",
    "        Calculate the energy consumption for a given task.\n",
    "\n",
    "        Parameters:\n",
    "        - alpha_m (float): Offloading decision\n",
    "        - R_m (float): Uplink data rate in bits/second\n",
    "        - s_m (int): Split factor (0 or 1)\n",
    "        - f_ue_m (float): Computation capability of the user device\n",
    "        - p_m (float): Transmission power\n",
    "        - f_ue_est (float): Estimation error for the user device's computation capability\n",
    "        - eta_m (float): Computational intensity\n",
    "\n",
    "        Returns:\n",
    "        - E_total (float): Total energy consumption in Joules\n",
    "        \"\"\"\n",
    "\n",
    "        actual_f_ue_m = f_ue_m - f_ue_est  # Calculate the actual processing rate of the UE\n",
    "\n",
    "        E_ue = alpha_m * (self.theta / 2) * eta_m * D_m * (actual_f_ue_m ** 2)  # Energy consumption at the user device\n",
    "        E_tx = ((D_m * 8) * p_m) / R_m  # Transmission energy\n",
    "\n",
    "        if s_m == 1:  # Task is in cache\n",
    "            E_total = 0  # No energy consumed when task is in cache\n",
    "        else:\n",
    "            E_total = E_ue + E_tx  # Total energy consumption\n",
    "\n",
    "        return E_total\n",
    "\n",
    "    def manage_cache(self, task_info, task_delay, cache_hit_model):\n",
    "        \"\"\"\n",
    "        Manage the cache for storing and retrieving tasks.\n",
    "\n",
    "        Parameters:\n",
    "        - task_info (tuple): Task parameters to identify the task\n",
    "        - task_delay (float): Delay of the task\n",
    "\n",
    "        Returns:\n",
    "        - bool: True if the task is found in the cache, False otherwise\n",
    "        \"\"\"\n",
    "        # Check for cache hit first\n",
    "        cache_hit = any(task_info == task[0] for task in self.cache)\n",
    "        \n",
    "        if task_delay == 0:\n",
    "            return cache_hit  # Return True if found, False otherwise\n",
    "        \n",
    "        task_size = task_info['D_m'] * 8  # Task size\n",
    "        Server_Max_Capacity = self.S_max_es  # Server maximum capacity\n",
    "        if Server_Max_Capacity == 0 :\n",
    "            return\n",
    "        \n",
    "        if self.is_training :\n",
    "            # During training\n",
    "            if cache_hit:\n",
    "                self.current_cache_size -= task_info['D_m'] * 8\n",
    "                self.cache = [task for task in self.cache if task[0] != task_info]\n",
    "\n",
    "            # During training, always update cache based on task delay\n",
    "            self.cache.append((task_info, task_delay))  # Add task to cache\n",
    "            self.current_cache_size += task_size  # Update cache size\n",
    "        \n",
    "            if self.current_cache_size >= Server_Max_Capacity:\n",
    "                sorted_cache = sorted(self.cache, key=lambda x: x[1], reverse=True)  # Sort tasks by delay in descending order\n",
    "\n",
    "                while (self.current_cache_size) > Server_Max_Capacity:\n",
    "                    if not sorted_cache:\n",
    "                        break  # Exit loop if sorted_cache is empty\n",
    "                    last_task = sorted_cache.pop()  # Remove the last task from sorted_cache\n",
    "                    self.cache.remove(last_task)  # Remove the task from the cache\n",
    "                    self.current_cache_size -= last_task[0]['D_m'] * 8  # Update current cache size\n",
    "\n",
    "        if self.is_training is False:\n",
    "            # During testing, follow the model's prediction\n",
    "            if cache_hit_model == 0 :\n",
    "                if cache_hit:\n",
    "                    self.current_cache_size -= task_info['D_m'] * 8\n",
    "                    self.cache = [task for task in self.cache if task[0] != task_info]\n",
    "            else:\n",
    "                if cache_hit:\n",
    "                    self.current_cache_size -= task_info['D_m'] * 8\n",
    "                    self.cache = [task for task in self.cache if task[0] != task_info]\n",
    "\n",
    "                if (task_size + self.current_cache_size) <= Server_Max_Capacity:\n",
    "                    self.cache.append((task_info, task_delay))  # Add task to cache\n",
    "                    self.current_cache_size += task_size  # Update cache size\n",
    "\n",
    "                else:\n",
    "                    sorted_cache = sorted(self.cache, key=lambda x: x[1], reverse=True)  # Sort tasks by delay in descending order\n",
    "\n",
    "                    while (task_size + self.current_cache_size) > Server_Max_Capacity:\n",
    "                        if not sorted_cache:\n",
    "                            break  # Exit loop if sorted_cache is empty\n",
    "                        last_task = sorted_cache.pop()  # Remove the last task from sorted_cache\n",
    "                        self.cache.remove(last_task)  # Remove the task from the cache\n",
    "                        self.current_cache_size -= last_task[0]['D_m'] * 8  # Update current cache size\n",
    "\n",
    "                    self.cache.append((task_info, task_delay))  # Add task to cache\n",
    "                    self.current_cache_size += task_size  # Update cache size\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Perform a simulation step for the given action.\n",
    "\n",
    "        Parameters:\n",
    "        - action (array): Array of action for each user\n",
    "        - tasks (array): Array of task for each user\n",
    "\n",
    "        Returns:\n",
    "        - tuple: (task_rewards, next_state, done)\n",
    "        \"\"\"\n",
    "        done = False\n",
    "\n",
    "        alpha_m = action[0]\n",
    "        b_m = action[1]\n",
    "        p_m = action[2]\n",
    "        f_ue_m = action[3]\n",
    "        f_es_m = action[4]\n",
    "        cache_hit_model = action[5]\n",
    "\n",
    "        task = self.current_task\n",
    "\n",
    "        user_id = task.pop('user_id',0)\n",
    "        task.pop('order')\n",
    "        \n",
    "        # Determine if the task is a cache hit or miss\n",
    "        cache_hit = 1 if self.manage_cache(task, 0, cache_hit_model) else 0\n",
    "\n",
    "        f_ue_est = f_ue_m * self.f_ue_dev  \n",
    "        f_es_est = f_es_m * self.f_es_dev  \n",
    "\n",
    "        # Calculate the uplink data rate for the user\n",
    "        R_m = self.calculate_uplink_rate(b_m, p_m, user_id)\n",
    "\n",
    "        # Calculate the end-to-end delay for the task\n",
    "        delay = self.calculate_delay(\n",
    "            alpha_m, cache_hit, R_m,\n",
    "            task['D_m'], f_ue_m, f_es_m, f_ue_est,\n",
    "            f_es_est, task['eta_m']\n",
    "        )\n",
    "\n",
    "        delay = np.round(delay[0],6) if isinstance(delay, np.ndarray) else np.round(delay,6)\n",
    "\n",
    "        # Calculate the energy consumption for the task\n",
    "        energy = self.calculate_energy_consumption(\n",
    "            cache_hit, R_m, alpha_m, p_m, task['D_m'], f_ue_m,\n",
    "            f_es_est, task['eta_m']\n",
    "        )\n",
    "\n",
    "        energy = np.round(energy[0],6) if isinstance(energy, np.ndarray) else np.round(energy,6)\n",
    "\n",
    "        # Manage task transmission and processing times\n",
    "        if cache_hit == 0:\n",
    "            transmission_end_time = self.current_time + self.calculate_transmission_delay(alpha_m, R_m, task['D_m'])\n",
    "            processing_end_time = transmission_end_time + self.calculate_server_processing_delay(alpha_m, cache_hit, task['D_m'], f_es_m, f_es_est, task['eta_m'])\n",
    "\n",
    "            self.transmitting_tasks.append((self.current_time, transmission_end_time, b_m))\n",
    "            process = f_es_m * (1 - alpha_m)\n",
    "            self.processing_tasks.append((transmission_end_time, processing_end_time, process))\n",
    "\n",
    "        else:\n",
    "            # For cache hit, only processing delay is considered\n",
    "            processing_end_time = self.current_time + self.calculate_server_processing_delay(alpha_m, cache_hit, task['D_m'], f_es_m, f_es_est, task['eta_m'])\n",
    "            self.processing_tasks.append((self.current_time, processing_end_time, f_es_m))\n",
    "\n",
    "        # Update cache with the task if it becomes eligible\n",
    "        self.manage_cache(task, delay, cache_hit_model)\n",
    "            \n",
    "        # Calculate total bandwidth and computation resource usage at current time\n",
    "        self.total_bandwidth = sum(b for _, end_time, b in self.transmitting_tasks if end_time > self.current_time)\n",
    "        self.total_computation = sum(f for _, end_time, f in self.processing_tasks if end_time > self.current_time)\n",
    "\n",
    "        # Free resources for tasks that have completed transmission or processing\n",
    "        self.transmitting_tasks = [(start_time, end_time, b) for start_time, end_time, b in self.transmitting_tasks if end_time > self.current_time]\n",
    "        self.processing_tasks = [(start_time, end_time, f) for start_time, end_time, f in self.processing_tasks if end_time > self.current_time]\n",
    "\n",
    "        # Check The Cache hit of model is right or not \n",
    "        cache_hit = 1 if self.manage_cache(task, 0, cache_hit_model) else 0\n",
    "        cache_hit_right = 1 if cache_hit == cache_hit_model else 0\n",
    "\n",
    "        # Calculate reward\n",
    "        reward  = (-energy - delay)*1e4  # *1e4 for convert to a large number \n",
    "        \n",
    "        # Apply penalties for exceeding resource limits\n",
    "        if delay > task['T_max']:\n",
    "            reward -= self.penalty\n",
    "            done = True\n",
    "        if cache_hit_right == 0:\n",
    "            reward -= self.penalty\n",
    "        if R_m < self.R_min:\n",
    "            reward -= self.penalty*2\n",
    "            done = True\n",
    "        if energy > self.E_max:\n",
    "            reward -= self.penalty\n",
    "            done = True\n",
    "        if self.total_bandwidth > 1:\n",
    "            reward -= self.penalty\n",
    "            done = True\n",
    "        if self.total_computation > self.F_max_es:\n",
    "            reward -= self.penalty\n",
    "            done = True\n",
    "\n",
    "        state_info = [\n",
    "            delay,\n",
    "            energy,\n",
    "            task['eta_m'],\n",
    "            self.total_bandwidth,\n",
    "            self.total_computation,\n",
    "            cache_hit_right,\n",
    "            self.user_device_params[user_id]['d']\n",
    "        ]\n",
    "\n",
    "        self.Task_processed += 1\n",
    "\n",
    "        return reward, state_info, done\n",
    "\n",
    "    # Increment current simulation time\n",
    "    def increase_time(self):\n",
    "        self.current_time += self.T_max\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to its initial state.\n",
    "        \"\"\"\n",
    "        self.cache = [] \n",
    "        self.current_cache_size = 0\n",
    "        self.Task_processed = 0  \n",
    "        self.transmitting_tasks = [] \n",
    "        self.processing_tasks = [] \n",
    "        self.current_time = 0.0  \n",
    "        self.user_device_params = []\n",
    "        self.initialize_user_device_params()\n",
    "        self.total_bandwidth = 0  \n",
    "        self.total_computation = 0  \n",
    "\n",
    "    def render(self):\n",
    "        print(f\"Number of Users: {self.M}\")\n",
    "        print(f\"Number of Task Processed: {self.Task_processed}\")\n",
    "        print(f\"Total Bandwidth Used: {self.total_bandwidth}\")\n",
    "        print(f\"Total Computation Used: {self.total_computation}\")\n",
    "        print(f\"Current Cache Size: {np.round(self.current_cache_size/1000,2)} Kb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
